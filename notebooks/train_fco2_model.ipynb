{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f1e5dd-99a2-4065-ae58-8b28bdaa523a",
   "metadata": {},
   "source": [
    "#### Load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27b1bb0a-93ee-43fe-a22a-26fe0c830211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0,'../../mocsy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f974531b-e417-49ff-9c59-d45f4e31a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mocsy\n",
    "from mocsy import mvars\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c27fdcfc-6816-4aa0-88f6-75e5a80d8a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2696a537-4cc5-4088-83da-e853a259c478",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Calculate fCO2 training data with mocsy routine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce5c4036-fa74-4a51-9ed4-c0f8fde59940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fCO2(alk, dic, tem, sal, sil, phos):\n",
    "    # input units\n",
    "    # alk in mol / kg\n",
    "    # dic in mol / kg\n",
    "    # tem in °C\n",
    "    # sal in PSU\n",
    "    # sil in mol / kg\n",
    "    # phos in mol / kg\n",
    "    n = len(alk)\n",
    "    return mvars(alk=alk,\n",
    "                     dic=dic,\n",
    "                     temp=tem,\n",
    "                     sal=sal,\n",
    "                     sil=sil,\n",
    "                     phos=phos,\n",
    "                     patm=tuple(1 for _ in range(n)),\n",
    "                     depth=tuple(5 for _ in range(n)),\n",
    "                     lat=tuple(np.nan for _ in range(n)),\n",
    "                     optcon='mol/kg',\n",
    "                     optt='Tpot',\n",
    "                     optp='db',\n",
    "                     optk1k2='l',\n",
    "                     optb='u74',\n",
    "                     optkf='pf',\n",
    "                     opts='Sprc')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "329f9dc6-8f17-41dc-9ba0-29cbcdd5c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "samples = []\n",
    "sample_size = 6000_000\n",
    "for i in range(sample_size):\n",
    "    # alk between 1700e-6 and 2700e-6 mol kg-1\n",
    "    alk = np.random.uniform(low=1700e-6, high=2700e-6)\n",
    "    # dic between 1700e-6 mol kg-1 and alk\n",
    "    dic = np.random.uniform(low=1700e-6, high=alk)\n",
    "    # tem between 2 and 35 °C\n",
    "    tem = np.random.uniform(low=2, high=35)\n",
    "    # sal between 19 and 43 PSU\n",
    "    sal = np.random.uniform(low=19, high=43)\n",
    "    # sil between 0 and 134 mumol kg-1\n",
    "    sil = np.random.uniform(low=0, high=134e-6)\n",
    "    # phos between 0 and 4 mumol kg-1\n",
    "    phos = np.random.uniform(low=0, high=4e-6)\n",
    "    sample = (alk, dic, tem, sal, sil, phos)\n",
    "    samples.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33a458ef-b191-4adf-908f-71409075faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_alk, sample_dic, sample_tem, sample_sal, sample_sil, sample_phos =\\\n",
    "zip(*samples)\n",
    "sample_fco2 = calc_fCO2(sample_alk, sample_dic, sample_tem, sample_sal, sample_sil, sample_phos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6553912d-693c-4564-9742-0f30e6497c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453.50214571167703\n",
      "[328.11596538]\n",
      "(6000000,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(np.median(sample_fco2))\n",
    "print(calc_fCO2(tuple([2200e-6]), tuple([1950e-6]), tuple([18.5]), tuple([31]), tuple([67e-6]), tuple([2e-6])))\n",
    "print(sample_fco2.shape)\n",
    "print(type(sample_fco2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d62cadd1-7d5b-44f9-b67b-7314ba3e783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 5000_000\n",
    "\n",
    "train_fco2, valid_fco2 = np.split(sample_fco2, [ntrain])\n",
    "train_alk, valid_alk = np.split(np.array(sample_alk), [ntrain])\n",
    "train_dic, valid_dic = np.split(np.array(sample_dic), [ntrain])\n",
    "train_tem, valid_tem = np.split(np.array(sample_tem), [ntrain])\n",
    "train_sal, valid_sal = np.split(np.array(sample_sal), [ntrain])\n",
    "train_sil, valid_sil = np.split(np.array(sample_sil), [ntrain])\n",
    "train_phos, valid_phos = np.split(np.array(sample_phos), [ntrain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9723b9f-5778-4b50-9fab-92bce16129a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some statistics:\n",
      "Mean for alk samples: 2.200027e-03, expected: 2.200000e-03\n",
      "Std for alk samples: 2.886953e-04, expected: 2.886751e-04\n",
      "-----\n",
      "Mean for dic samples: 1.950093e-03, expected: 1.950000e-03\n",
      "Std for dic samples: 2.205415e-04, expected: 2.204793e-04\n",
      "-----\n",
      "Mean for tem samples: 1.849863e+01, expected: 1.850000e+01\n",
      "Std for tem samples: 9.530802e+00, expected: 9.526279e+00\n",
      "-----\n",
      "Mean for sal samples: 3.099651e+01, expected: 3.100000e+01\n",
      "Std for sal samples: 6.927757e+00, expected: 6.928203e+00\n",
      "-----\n",
      "Mean for sil samples: 6.697189e-05, expected: 6.700000e-05\n",
      "Std for sil samples: 3.870030e-05, expected: 3.868247e-05\n",
      "-----\n",
      "Mean for phos samples: 1.999234e-06, expected: 2.000000e-06\n",
      "Std for phos samples: 1.154432e-06, expected: 1.154701e-06\n"
     ]
    }
   ],
   "source": [
    "sample_alk_mean = (1700e-6 + 2700e-6) / 2\n",
    "sample_alk_std = (2700e-6 - 1700e-6) / np.sqrt(12)\n",
    "\n",
    "sample_dic_mean = 1700e-6 + (2700e-6 - 1700e-6) / 4\n",
    "sample_dic_std = (2700e-6 - 1700e-6) * np.sqrt(7 / 144)\n",
    "\n",
    "sample_tem_mean = (2 + 35) / 2\n",
    "sample_tem_std = (35 - 2) / np.sqrt(12)\n",
    "\n",
    "sample_sal_mean = (19 + 43) / 2\n",
    "sample_sal_std = (43 - 19) / np.sqrt(12)\n",
    "\n",
    "sample_sil_mean = (134e-6 + 0e-6) / 2\n",
    "sample_sil_std = (134e-6 - 0e-6) / np.sqrt(12)\n",
    "\n",
    "sample_phos_mean = (4e-6 + 0e-6) / 2\n",
    "sample_phos_std = (4e-6 - 0e-6) / np.sqrt(12)\n",
    "\n",
    "print(\"Some statistics:\")\n",
    "print(\"Mean for alk samples: {:.6e}, expected: {:.6e}\".format(np.mean(train_alk), sample_alk_mean))\n",
    "print(\"Std for alk samples: {:.6e}, expected: {:.6e}\".format(np.std(train_alk), sample_alk_std))\n",
    "print(\"-----\")\n",
    "print(\"Mean for dic samples: {:.6e}, expected: {:.6e}\".format(np.mean(train_dic), sample_dic_mean))\n",
    "print(\"Std for dic samples: {:.6e}, expected: {:.6e}\".format(np.std(train_dic), sample_dic_std))\n",
    "print(\"-----\")\n",
    "print(\"Mean for tem samples: {:.6e}, expected: {:.6e}\".format(np.mean(train_tem), sample_tem_mean))\n",
    "print(\"Std for tem samples: {:.6e}, expected: {:.6e}\".format(np.std(train_tem), sample_tem_std))\n",
    "print(\"-----\")\n",
    "print(\"Mean for sal samples: {:.6e}, expected: {:.6e}\".format(np.mean(sample_sal), sample_sal_mean))\n",
    "print(\"Std for sal samples: {:.6e}, expected: {:.6e}\".format(np.std(sample_sal), sample_sal_std))\n",
    "print(\"-----\")\n",
    "print(\"Mean for sil samples: {:.6e}, expected: {:.6e}\".format(np.mean(train_sil), sample_sil_mean))\n",
    "print(\"Std for sil samples: {:.6e}, expected: {:.6e}\".format(np.std(train_sil), sample_sil_std))\n",
    "print(\"-----\")\n",
    "print(\"Mean for phos samples: {:.6e}, expected: {:.6e}\".format(np.mean(train_phos), sample_phos_mean))\n",
    "print(\"Std for phos samples: {:.6e}, expected: {:.6e}\".format(np.std(train_phos), sample_phos_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf96fd-d71d-4724-89e3-f50269240345",
   "metadata": {},
   "source": [
    "#### Normalize samples and train neural network with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e39058f-ec81-456c-a9ec-388dde07b785",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_means = np.array([sample_alk_mean, sample_dic_mean, sample_tem_mean,\n",
    "                         sample_sal_mean, sample_sil_mean, sample_phos_mean])\n",
    "sample_stds = np.array([sample_alk_std, sample_dic_std, sample_tem_std,\n",
    "                         sample_sal_std, sample_sil_std, sample_phos_std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed0d400c-fb81-4cb2-9453-057fc3f5268c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000000, 6)\n"
     ]
    }
   ],
   "source": [
    "train_features = np.concatenate([train_alk[:, np.newaxis], train_dic[:, np.newaxis], train_tem[:, np.newaxis],\n",
    "                               train_sal[:, np.newaxis], train_sil[:, np.newaxis], train_phos[:, np.newaxis]], axis=1)\n",
    "\n",
    "valid_features = np.concatenate([valid_alk[:, np.newaxis], valid_dic[:, np.newaxis], valid_tem[:, np.newaxis],\n",
    "                               valid_sal[:, np.newaxis], valid_sil[:, np.newaxis], valid_phos[:, np.newaxis]], axis=1)\n",
    "\n",
    "print(train_features.shape)\n",
    "train_features_normalized = (train_features - sample_means) / sample_stds\n",
    "valid_features_normalized = (valid_features - sample_means) / sample_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f36cf192-52e5-4370-beeb-dc28e51c08e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verify that data is normalized.\n",
      "[ 9.47580989e-05  4.23186847e-04 -1.44276512e-04 -4.32910793e-04\n",
      " -7.26599596e-04 -6.63078680e-04]\n",
      "[1.00006983 1.00028241 1.00047477 0.99986704 1.00046103 0.99976755]\n"
     ]
    }
   ],
   "source": [
    "print(\"Verify that data is normalized.\")\n",
    "print(np.mean(train_features_normalized, axis=0))\n",
    "print(np.std(train_features_normalized, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "933cfa31-3848-4efe-a686-e90a1bceca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size, device=self.device)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size, device=self.device)\n",
    "        self.linear3 = nn.Linear(hidden_size, hidden_size, device=self.device)\n",
    "        self.linear4 = nn.Linear(hidden_size, output_size, device=self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        x = F.elu(self.linear1(x))\n",
    "        x = F.elu(self.linear2(x))\n",
    "        x = F.elu(self.linear3(x))\n",
    "        x = F.elu(self.linear4(x))\n",
    "        return x\n",
    "\n",
    "    def save(self, file_name='model.pth'):\n",
    "        model_folder_path = './model'\n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "\n",
    "        file_name = os.path.join(model_folder_path, file_name)\n",
    "        torch.save(self.state_dict(), file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9edbbbae-1c19-42ed-86b4-a55bea37fcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear1): Linear(in_features=6, out_features=64, bias=True)\n",
      "  (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (linear3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (linear4): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Number of trainable parameters in the model: 8833\n"
     ]
    }
   ],
   "source": [
    "model = MLP(6, 64, 1)\n",
    "print(model)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of trainable parameters in the model:\", pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d143e2a-376d-4563-927c-a280ede841f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_dataloader(features, labels, batch_size):\n",
    "    ntrain = len(labels)\n",
    "    nbatch = ntrain // batch_size\n",
    "    indices = np.arange(ntrain, dtype=int)\n",
    "    random.shuffle(indices)\n",
    "    batch_indices = np.split(indices[:nbatch * batch_size], nbatch)\n",
    "    batch_data = [(torch.from_numpy(np.take(features, ind, axis=0).astype(\"float32\")),\n",
    "                   torch.from_numpy(np.take(labels, ind).astype(\"float32\"))) for ind in batch_indices]\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "166b8204-6654-4bae-8be9-18e25f949319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    running_loss = 0. # running loss over all batches in the epoch\n",
    "    \n",
    "    training_data = training_dataloader(train_features_normalized,\n",
    "                                        train_fco2, batch_size)\n",
    "    \n",
    "    for batch in training_data:\n",
    "        features, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = torch.squeeze(model(features))\n",
    "        loss = loss_function(outputs, labels.to(torch.device(\"cuda\")))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.detach().cpu().item()\n",
    "\n",
    "    ntrain = len(train_fco2)\n",
    "    nbatch = ntrain // batch_size\n",
    "\n",
    "    return running_loss / nbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09e2104f-fbc4-43e1-ad6b-456f6f882b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Loss train 28864.45091426239 valid 43.128937357100455\n",
      "Epoch 2:\n",
      "Loss train 18.55601351594925 valid 7.7519794439822824\n",
      "Epoch 3:\n",
      "Loss train 5.254506319570542 valid 3.7493398866832717\n",
      "Epoch 4:\n",
      "Loss train 2.9867678776979445 valid 2.6385124397728417\n",
      "Epoch 5:\n",
      "Loss train 2.2759529870986936 valid 1.7075538814436286\n",
      "Epoch 6:\n",
      "Loss train 1.8838338971495627 valid 1.1381169277901935\n",
      "Epoch 7:\n",
      "Loss train 1.7012697219491004 valid 1.136594912110647\n",
      "Epoch 8:\n",
      "Loss train 1.5039412677645683 valid 0.895458614610667\n",
      "Epoch 9:\n",
      "Loss train 1.380420422422886 valid 1.3525340934443202\n",
      "Epoch 10:\n",
      "Loss train 1.323362349307537 valid 0.8849860516594632\n",
      "Epoch 11:\n",
      "Loss train 1.2400273625016212 valid 0.6465340860687695\n",
      "Epoch 12:\n",
      "Loss train 1.1784311193466186 valid 1.227594006874498\n",
      "Epoch 13:\n",
      "Loss train 1.1239829023063184 valid 1.0609803077610296\n",
      "Epoch 14:\n",
      "Loss train 1.1019444766163826 valid 0.8497758823394075\n",
      "Epoch 15:\n",
      "Loss train 1.048903335148096 valid 0.950761750038526\n",
      "Epoch 16:\n",
      "Loss train 1.0574259937465191 valid 0.5225577440407224\n",
      "Epoch 17:\n",
      "Loss train 0.9908062466621399 valid 0.7670591776368622\n",
      "Epoch 18:\n",
      "Loss train 1.0190661082088948 valid 0.6587160208460815\n",
      "Epoch 19:\n",
      "Loss train 0.9427409795761108 valid 0.8245386440232108\n",
      "Epoch 20:\n",
      "Loss train 0.9855625270307065 valid 0.587197934354292\n",
      "Epoch 21:\n",
      "Loss train 0.893112448322773 valid 0.39425281274847007\n",
      "Epoch 22:\n",
      "Loss train 0.9084227944970131 valid 0.6393148624971157\n",
      "Epoch 23:\n",
      "Loss train 0.9192780799567699 valid 0.43799816442636563\n",
      "Epoch 24:\n",
      "Loss train 0.8908434274792671 valid 0.41672191841813644\n",
      "Epoch 25:\n",
      "Loss train 0.8419634033828973 valid 3.9924602517156536\n",
      "Epoch 26:\n",
      "Loss train 0.8929673182934522 valid 0.34239744397767774\n",
      "Epoch 27:\n",
      "Loss train 0.8628793999791146 valid 0.45411705711743305\n",
      "Epoch 28:\n",
      "Loss train 0.8247780751645565 valid 0.7373109105107915\n",
      "Epoch 29:\n",
      "Loss train 0.8063714539021254 valid 0.5314315213732573\n",
      "Epoch 30:\n",
      "Loss train 0.8290708865910769 valid 0.4373647800445702\n",
      "Epoch 31:\n",
      "Loss train 0.8053906405299902 valid 0.37082726728629795\n",
      "Epoch 32:\n",
      "Loss train 0.7906551250100136 valid 0.33838611561453785\n",
      "Epoch 33:\n",
      "Loss train 0.8832167794704437 valid 0.24542523778302613\n",
      "Epoch 34:\n",
      "Loss train 0.7546210051238537 valid 0.3714086359425492\n",
      "Epoch 35:\n",
      "Loss train 0.7766593624651432 valid 0.39229947473590093\n",
      "Epoch 36:\n",
      "Loss train 0.7681426560461521 valid 0.4752108176853643\n",
      "Epoch 37:\n",
      "Loss train 0.778305139914155 valid 0.3671825969913844\n",
      "Epoch 38:\n",
      "Loss train 0.7441631887972355 valid 0.2628647324148181\n",
      "Epoch 39:\n",
      "Loss train 0.7420820673316717 valid 0.2940084797979346\n",
      "Epoch 40:\n",
      "Loss train 0.7483630093663931 valid 0.3691240518066905\n",
      "Epoch 41:\n",
      "Loss train 0.7538265789836646 valid 0.31142887950725645\n",
      "Epoch 42:\n",
      "Loss train 0.7198828360378742 valid 0.4547985754777614\n",
      "Epoch 43:\n",
      "Loss train 0.736722896850109 valid 7.895510916763918\n",
      "Epoch 44:\n",
      "Loss train 0.6876316194027662 valid 0.7210958901341394\n",
      "Epoch 45:\n",
      "Loss train 0.7123326275110244 valid 0.3881572920061206\n",
      "Epoch 46:\n",
      "Loss train 0.6883053372830152 valid 0.3040276615819465\n",
      "Epoch 47:\n",
      "Loss train 0.7466783274024725 valid 0.26670745471747\n",
      "Epoch 48:\n",
      "Loss train 0.6923562159329653 valid 0.34294161107689974\n",
      "Epoch 49:\n",
      "Loss train 0.7196515589118004 valid 0.2129374819095023\n",
      "Epoch 50:\n",
      "Loss train 0.6812141857385635 valid 0.28053543245026574\n",
      "Epoch 51:\n",
      "Loss train 0.6973192566305398 valid 0.27164253992871074\n",
      "Epoch 52:\n",
      "Loss train 0.7683687580645084 valid 0.23276781716926487\n",
      "Epoch 53:\n",
      "Loss train 0.6572206220835447 valid 0.23706632747938927\n",
      "Epoch 54:\n",
      "Loss train 0.6891922513663769 valid 1.118736316072884\n",
      "Epoch 55:\n",
      "Loss train 0.6524696586817503 valid 0.19012883763328095\n",
      "Epoch 56:\n",
      "Loss train 0.6666517510712147 valid 1.102563928455632\n",
      "Epoch 57:\n",
      "Loss train 0.7186727279275655 valid 0.20988365193082947\n",
      "Epoch 58:\n",
      "Loss train 0.6516172331750393 valid 0.27431264614840334\n",
      "Epoch 59:\n",
      "Loss train 0.6329475774347783 valid 1.7679404777546157\n",
      "Epoch 60:\n",
      "Loss train 0.682136735624075 valid 0.39068631374852664\n",
      "Epoch 61:\n",
      "Loss train 0.6564082294046879 valid 3.3218289891782478\n",
      "Epoch 62:\n",
      "Loss train 0.6458747769057751 valid 1.300216516637017\n",
      "Epoch 63:\n",
      "Loss train 0.6262976589053869 valid 0.2560155129390194\n",
      "Epoch 64:\n",
      "Loss train 0.6537878454834223 valid 0.23272520735975732\n",
      "Epoch 65:\n",
      "Loss train 0.6307784099400043 valid 0.3115912205933316\n",
      "Epoch 66:\n",
      "Loss train 0.6094140400528908 valid 0.42817960512119135\n",
      "Epoch 67:\n",
      "Loss train 0.6499421292960644 valid 0.250723183239339\n",
      "Epoch 68:\n",
      "Loss train 0.6159036199241876 valid 0.24789019434085016\n",
      "Epoch 69:\n",
      "Loss train 0.629155275683105 valid 0.26372286746809337\n",
      "Epoch 70:\n",
      "Loss train 0.6156369108736515 valid 0.2190533533184974\n",
      "Epoch 71:\n",
      "Loss train 0.6341698395073414 valid 0.3295386621302368\n",
      "Epoch 72:\n",
      "Loss train 0.6055433673635126 valid 0.18224064398728174\n",
      "Epoch 73:\n",
      "Loss train 0.6131331040769815 valid 0.565663143496389\n",
      "Epoch 74:\n",
      "Loss train 0.7078665800184012 valid 0.21311552699189493\n",
      "Epoch 75:\n",
      "Loss train 0.5578162165015936 valid 0.2167629986374965\n",
      "Epoch 76:\n",
      "Loss train 0.615249166405201 valid 1.500924369286498\n",
      "Epoch 77:\n",
      "Loss train 0.6084015092492103 valid 0.24723795206140253\n",
      "Epoch 78:\n",
      "Loss train 0.5855169812455774 valid 0.2340056193630491\n",
      "Epoch 79:\n",
      "Loss train 0.6247690862759948 valid 0.28486968498409165\n",
      "Epoch 80:\n",
      "Loss train 0.5685368557378649 valid 0.2044562873791697\n",
      "Epoch 81:\n",
      "Loss train 0.5909765903711319 valid 0.5746888265290411\n",
      "Epoch 82:\n",
      "Loss train 0.6214935641258955 valid 1.210495906628852\n",
      "Epoch 83:\n",
      "Loss train 0.5734645910486579 valid 0.1596752816487345\n",
      "Epoch 84:\n",
      "Loss train 0.5666893027037382 valid 0.22831895883060638\n",
      "Epoch 85:\n",
      "Loss train 0.567431949712336 valid 0.18390878158302953\n",
      "Epoch 86:\n",
      "Loss train 0.5700793811887502 valid 0.23096486806698438\n",
      "Epoch 87:\n",
      "Loss train 0.560006329074502 valid 0.31329112175040064\n",
      "Epoch 88:\n",
      "Loss train 0.5789316285774112 valid 0.9320698349623131\n",
      "Epoch 89:\n",
      "Loss train 0.555970944467187 valid 0.6347843116742782\n",
      "Epoch 90:\n",
      "Loss train 0.5765495688244701 valid 0.12788676625673867\n",
      "Epoch 91:\n",
      "Loss train 0.5985056920364499 valid 0.7625399392362046\n",
      "Epoch 92:\n",
      "Loss train 0.5476748127743601 valid 0.6406939302964856\n",
      "Epoch 93:\n",
      "Loss train 0.5637913420125842 valid 0.22241236815456863\n",
      "Epoch 94:\n",
      "Loss train 0.527403009699285 valid 0.9877109296285412\n",
      "Epoch 95:\n",
      "Loss train 0.5709333626687527 valid 1.9553478107797864\n",
      "Epoch 96:\n",
      "Loss train 0.532361253093183 valid 0.15803670722483512\n",
      "Epoch 97:\n",
      "Loss train 0.561563463486731 valid 1.3686533181831388\n",
      "Epoch 98:\n",
      "Loss train 0.5164003240838647 valid 0.7307236653468817\n",
      "Epoch 99:\n",
      "Loss train 0.5371786837249994 valid 0.28618324320070687\n",
      "Epoch 100:\n",
      "Loss train 0.5519020007833838 valid 2.9485193856094205\n",
      "Epoch 101:\n",
      "Loss train 0.5217751131653786 valid 0.1364876152240837\n",
      "Epoch 102:\n",
      "Loss train 0.5491564204052091 valid 0.6785209306926099\n",
      "Epoch 103:\n",
      "Loss train 0.5392033228322863 valid 0.4681456888351044\n",
      "Epoch 104:\n",
      "Loss train 0.5439498281568289 valid 0.2602123428707957\n",
      "Epoch 105:\n",
      "Loss train 0.5217182128325105 valid 0.5521195029741226\n",
      "Epoch 106:\n",
      "Loss train 0.5314004745349288 valid 0.2921627595736603\n",
      "Epoch 107:\n",
      "Loss train 0.5378179205343128 valid 1.0673467452790655\n",
      "Epoch 108:\n",
      "Loss train 0.5123920948654413 valid 0.32158629145812545\n",
      "Epoch 109:\n",
      "Loss train 0.5334370097532868 valid 0.24014739857060421\n",
      "Epoch 110:\n",
      "Loss train 0.48767507276088 valid 0.13007880349925108\n",
      "Epoch 111:\n",
      "Loss train 0.5090550236687064 valid 0.47189850641426767\n",
      "Epoch 112:\n",
      "Loss train 0.5288115906402469 valid 1.6499280572403672\n",
      "Epoch 113:\n",
      "Loss train 0.5088045855298639 valid 4.204334960766384\n",
      "Epoch 114:\n",
      "Loss train 0.5014148596897722 valid 0.5526281709209859\n",
      "Epoch 115:\n",
      "Loss train 0.5355861017987132 valid 0.14420616643121592\n",
      "Epoch 116:\n",
      "Loss train 0.48610439460277555 valid 0.7215121700169156\n",
      "Epoch 117:\n",
      "Loss train 0.5114511778831482 valid 4.565031779681127\n",
      "Epoch 118:\n",
      "Loss train 0.5023823747053743 valid 0.22109650215379084\n",
      "Epoch 119:\n",
      "Loss train 0.48710157751441 valid 0.15269608913606156\n",
      "Epoch 120:\n",
      "Loss train 0.5065916879504919 valid 0.11361884508523303\n",
      "Epoch 121:\n",
      "Loss train 0.48502844184339045 valid 0.2251873487703782\n",
      "Epoch 122:\n",
      "Loss train 0.49568426166027785 valid 0.65077096577285\n",
      "Epoch 123:\n",
      "Loss train 0.49204696954637767 valid 0.13680929177488613\n",
      "Epoch 124:\n",
      "Loss train 0.464833776538074 valid 0.1647884819856575\n",
      "Epoch 125:\n",
      "Loss train 0.5227100754186511 valid 0.7021334583010762\n",
      "Epoch 126:\n",
      "Loss train 0.5007042695537209 valid 0.10399393051534403\n",
      "Epoch 127:\n",
      "Loss train 0.4791442502915859 valid 0.27299967596985364\n",
      "Epoch 128:\n",
      "Loss train 0.4735804866284132 valid 0.18195417842744066\n",
      "Epoch 129:\n",
      "Loss train 0.5094379069492221 valid 0.2267811199299197\n",
      "Epoch 130:\n",
      "Loss train 0.4953869232147932 valid 1.3765494398029712\n",
      "Epoch 131:\n",
      "Loss train 0.45577666765898467 valid 0.32459035517679763\n",
      "Epoch 132:\n",
      "Loss train 0.47482281513214114 valid 0.23808741346589737\n",
      "Epoch 133:\n",
      "Loss train 0.4981661937057972 valid 0.5224188151024063\n",
      "Epoch 134:\n",
      "Loss train 0.49020075979083777 valid 0.10629653687172577\n",
      "Epoch 135:\n",
      "Loss train 0.44481289147734643 valid 0.12186760500443353\n",
      "Epoch 136:\n",
      "Loss train 0.45610630012005565 valid 0.6086369901631634\n",
      "Epoch 137:\n",
      "Loss train 0.46489358328878877 valid 0.4587625072975434\n",
      "Epoch 138:\n",
      "Loss train 0.47855691388845445 valid 0.16708711375037197\n",
      "Epoch 139:\n",
      "Loss train 0.4653100390315056 valid 0.6263261971565258\n",
      "Epoch 140:\n",
      "Loss train 0.46854520045369863 valid 0.17219113197446104\n",
      "Epoch 141:\n",
      "Loss train 0.4963722766205668 valid 0.12991436051348745\n",
      "Epoch 142:\n",
      "Loss train 0.46636967991441486 valid 0.9617880837061582\n",
      "Epoch 143:\n",
      "Loss train 0.489949057136476 valid 0.10540588712954081\n",
      "Epoch 144:\n",
      "Loss train 0.4453399617999792 valid 0.209109652570186\n",
      "Epoch 145:\n",
      "Loss train 0.4652174356579781 valid 4.113892611186878\n",
      "Epoch 146:\n",
      "Loss train 0.4388172022521496 valid 1.1857939256273118\n",
      "Epoch 147:\n",
      "Loss train 0.4494430667459965 valid 0.11508644015456782\n",
      "Epoch 148:\n",
      "Loss train 0.46038397787213325 valid 0.08814854839062645\n",
      "Epoch 149:\n",
      "Loss train 0.4311166246458888 valid 0.41992541026009705\n",
      "Epoch 150:\n",
      "Loss train 0.45853210247904064 valid 0.5774357983003745\n",
      "Epoch 151:\n",
      "Loss train 0.5216280462443829 valid 1.4782961778742838\n",
      "Epoch 152:\n",
      "Loss train 0.39835502061247824 valid 0.323655996240567\n",
      "Epoch 153:\n",
      "Loss train 0.42835541145950556 valid 0.1825467481238438\n",
      "Epoch 154:\n",
      "Loss train 0.43872643683850765 valid 0.3343829909424017\n",
      "Epoch 155:\n",
      "Loss train 0.45513755611032247 valid 0.3310824388823568\n",
      "Epoch 156:\n",
      "Loss train 0.44030612370073796 valid 0.7586597656913094\n",
      "Epoch 157:\n",
      "Loss train 0.411101433506608 valid 0.37046178491842063\n",
      "Epoch 158:\n",
      "Loss train 0.4320381973132491 valid 0.28064680359406685\n",
      "Epoch 159:\n",
      "Loss train 0.42726174006313083 valid 0.12138092625407679\n",
      "Epoch 160:\n",
      "Loss train 0.42195981194525956 valid 0.14830161141839718\n",
      "Epoch 161:\n",
      "Loss train 0.44450999721437695 valid 0.32618701035673\n",
      "Epoch 162:\n",
      "Loss train 0.4122564799696207 valid 0.20110536071341972\n",
      "Epoch 163:\n",
      "Loss train 0.4424693799674511 valid 0.263269133917683\n",
      "Epoch 164:\n",
      "Loss train 0.43352428860664366 valid 0.8278765598722659\n",
      "Epoch 165:\n",
      "Loss train 0.4124699393212795 valid 0.14083278920690132\n",
      "Epoch 166:\n",
      "Loss train 0.413574352709949 valid 0.21826883938105282\n",
      "Epoch 167:\n",
      "Loss train 0.4375788355246186 valid 0.18211010334309732\n",
      "Epoch 168:\n",
      "Loss train 0.4152173602357507 valid 0.17995377074111243\n",
      "Epoch 169:\n",
      "Loss train 0.40963569732308386 valid 1.0236142776852943\n",
      "Epoch 170:\n",
      "Loss train 0.40764508853554726 valid 0.7466359042818367\n",
      "Epoch 171:\n",
      "Loss train 0.43116430161744357 valid 0.23307171891637568\n",
      "Epoch 172:\n",
      "Loss train 0.4431588651917875 valid 0.10268725805617362\n",
      "Epoch 173:\n",
      "Loss train 0.3938343564748764 valid 0.5299387742421864\n",
      "Epoch 174:\n",
      "Loss train 0.4335611698627472 valid 0.4379275085898652\n",
      "Epoch 175:\n",
      "Loss train 0.40453227592557667 valid 1.373716279926447\n",
      "Epoch 176:\n",
      "Loss train 0.4021234229132533 valid 0.28516571698393783\n",
      "Epoch 177:\n",
      "Loss train 0.4052983280226588 valid 0.16935456522272638\n",
      "Epoch 178:\n",
      "Loss train 0.4166066850028932 valid 0.22460758854686452\n",
      "Epoch 179:\n",
      "Loss train 0.3943200167834759 valid 0.1028729016735778\n",
      "Epoch 180:\n",
      "Loss train 0.39060953955501315 valid 1.0145555299722913\n",
      "Epoch 181:\n",
      "Loss train 0.39147009054422377 valid 0.2323389902805171\n",
      "Epoch 182:\n",
      "Loss train 0.4197010510444641 valid 0.26475133645185434\n",
      "Epoch 183:\n",
      "Loss train 0.3881448556996882 valid 0.1800501442761136\n",
      "Epoch 184:\n",
      "Loss train 0.4011137130305171 valid 0.6946982123933527\n",
      "Epoch 185:\n",
      "Loss train 0.4254207593411207 valid 1.7290749869895556\n",
      "Epoch 186:\n",
      "Loss train 0.3721509404577315 valid 0.3815990253082592\n",
      "Epoch 187:\n",
      "Loss train 0.4018010108806193 valid 0.5004501558902704\n",
      "Epoch 188:\n",
      "Loss train 0.4092704029455781 valid 1.4252598335121898\n",
      "Epoch 189:\n",
      "Loss train 0.39841741292476657 valid 0.11251051722206647\n",
      "Epoch 190:\n",
      "Loss train 0.38644685484245417 valid 0.10407253818669852\n",
      "Epoch 191:\n",
      "Loss train 0.42285788334310054 valid 0.21411377343589558\n",
      "Epoch 192:\n",
      "Loss train 0.35420532986223696 valid 0.446156909549893\n",
      "Epoch 193:\n",
      "Loss train 0.4069904817901552 valid 0.24463377191140057\n",
      "Epoch 194:\n",
      "Loss train 0.39566129791289567 valid 0.17286424999773023\n",
      "Epoch 195:\n",
      "Loss train 0.3855926220118999 valid 0.1325117024640424\n",
      "Epoch 196:\n",
      "Loss train 0.37898177402988076 valid 0.1404836268336798\n",
      "Epoch 197:\n",
      "Loss train 0.41671927438899875 valid 0.17706556299448828\n",
      "Epoch 198:\n",
      "Loss train 0.36971521361619236 valid 0.17523659221835688\n",
      "Epoch 199:\n",
      "Loss train 0.38761270004585385 valid 0.9691792046237304\n",
      "Epoch 200:\n",
      "Loss train 0.3896418513327837 valid 0.33831137939591405\n",
      "Epoch 201:\n",
      "Loss train 0.3921063513942063 valid 0.09266104350922921\n",
      "Epoch 202:\n",
      "Loss train 0.3611069457747042 valid 0.84034814888156\n",
      "Epoch 203:\n",
      "Loss train 0.3920607224434614 valid 0.578032002512509\n",
      "Epoch 204:\n",
      "Loss train 0.365438248090446 valid 0.17667864231255695\n",
      "Epoch 205:\n",
      "Loss train 0.3814202613957226 valid 0.1836995012925587\n",
      "Epoch 206:\n",
      "Loss train 0.3926694284655154 valid 0.07360053506033025\n",
      "Epoch 207:\n",
      "Loss train 0.36822640994787215 valid 0.21586160097420512\n",
      "Epoch 208:\n",
      "Loss train 0.3946387713603675 valid 0.1227571845748234\n",
      "Epoch 209:\n",
      "Loss train 0.39987576366066935 valid 5.173568820797909\n",
      "Epoch 210:\n",
      "Loss train 0.3717873904377222 valid 0.18676452356940632\n",
      "Epoch 211:\n",
      "Loss train 0.3939964975945652 valid 0.4903386895880469\n",
      "Epoch 212:\n",
      "Loss train 0.3872473557323217 valid 0.10038184947450257\n",
      "Epoch 213:\n",
      "Loss train 0.37369959287270904 valid 0.13065286309514643\n",
      "Epoch 214:\n",
      "Loss train 0.3721145629219711 valid 0.10080227573135964\n",
      "Epoch 215:\n",
      "Loss train 0.3540990246258676 valid 0.24927324119533883\n",
      "Epoch 216:\n",
      "Loss train 0.3871550967894494 valid 0.1420608689129263\n",
      "Epoch 217:\n",
      "Loss train 0.374698344630748 valid 0.09243195223065577\n",
      "Epoch 218:\n",
      "Loss train 0.3618791740879416 valid 0.10834669730718051\n",
      "Epoch 219:\n",
      "Loss train 0.400999801902473 valid 1.8043137339162298\n",
      "Epoch 220:\n",
      "Loss train 0.3361625062048435 valid 0.06275918395984267\n",
      "Epoch 221:\n",
      "Loss train 0.3626701417148113 valid 0.13207997331328497\n",
      "Epoch 222:\n",
      "Loss train 0.3674490944162011 valid 0.15118732778268149\n",
      "Epoch 223:\n",
      "Loss train 0.3764953307434917 valid 0.47373739354710404\n",
      "Epoch 224:\n",
      "Loss train 0.3638653856918216 valid 0.10069488374162863\n",
      "Epoch 225:\n",
      "Loss train 0.3646869486540556 valid 0.07679447255201872\n",
      "Epoch 226:\n",
      "Loss train 0.3852078262925148 valid 0.10378302801547044\n",
      "Epoch 227:\n",
      "Loss train 0.35263411258012056 valid 0.1997819082118508\n",
      "Epoch 228:\n",
      "Loss train 0.37062541122213005 valid 0.2646703063646377\n",
      "Epoch 229:\n",
      "Loss train 0.3460516746692359 valid 0.2930336788851316\n",
      "Epoch 230:\n",
      "Loss train 0.3512907729908824 valid 0.182538456612908\n",
      "Epoch 231:\n",
      "Loss train 0.40053114693313835 valid 0.1285558919596848\n",
      "Epoch 232:\n",
      "Loss train 0.3644979083262384 valid 0.070124553172557\n",
      "Epoch 233:\n",
      "Loss train 0.3524815612860024 valid 0.9203581004095254\n",
      "Epoch 234:\n",
      "Loss train 0.3650563720561564 valid 0.1246991082239773\n",
      "Epoch 235:\n",
      "Loss train 0.366253628154099 valid 0.1467074905745161\n",
      "Epoch 236:\n",
      "Loss train 0.35881169491559267 valid 0.09053721379839064\n",
      "Epoch 237:\n",
      "Loss train 0.37601560942679646 valid 0.07789322264882613\n",
      "Epoch 238:\n",
      "Loss train 0.3334340024620295 valid 0.12853468556359993\n",
      "Epoch 239:\n",
      "Loss train 0.3627693310186267 valid 0.10825297187556814\n",
      "Epoch 240:\n",
      "Loss train 0.35566831007450816 valid 0.09856317680722693\n",
      "Epoch 241:\n",
      "Loss train 0.34630671409741043 valid 0.09279401993599189\n",
      "Epoch 242:\n",
      "Loss train 0.3486764780640602 valid 0.34021654034238386\n",
      "Epoch 243:\n",
      "Loss train 0.37056794006079435 valid 0.07665317088252507\n",
      "Epoch 244:\n",
      "Loss train 0.360960142827034 valid 0.12674941517153132\n",
      "Epoch 245:\n",
      "Loss train 0.347820490847528 valid 0.06838615334645894\n",
      "Epoch 246:\n",
      "Loss train 0.3495868248723447 valid 0.07154078668155806\n",
      "Epoch 247:\n",
      "Loss train 0.38700129644870757 valid 0.2154747991159407\n",
      "Epoch 248:\n",
      "Loss train 0.3254482853829861 valid 0.06543340064577219\n",
      "Epoch 249:\n",
      "Loss train 0.3798947721727192 valid 0.10807402362372256\n",
      "Epoch 250:\n",
      "Loss train 0.350691545484215 valid 0.08600789915869468\n",
      "Epoch 251:\n",
      "Loss train 0.335721025826782 valid 0.14333741620061874\n",
      "Epoch 252:\n",
      "Loss train 0.3480341316901147 valid 0.0782048157439617\n",
      "Epoch 253:\n",
      "Loss train 0.36574954015985134 valid 0.11362588216750368\n",
      "Epoch 254:\n",
      "Loss train 0.3410073852114379 valid 0.1463262640531153\n",
      "Epoch 255:\n",
      "Loss train 0.33405567979738116 valid 1.9742349117137166\n",
      "Epoch 256:\n",
      "Loss train 0.36208545780107376 valid 0.20964183035681871\n",
      "Epoch 257:\n",
      "Loss train 0.3447877451345325 valid 0.32577461424746934\n",
      "Epoch 258:\n",
      "Loss train 0.3376522884145379 valid 0.1740606290079103\n",
      "Epoch 259:\n",
      "Loss train 0.35210085895806553 valid 0.22895389058857893\n",
      "Epoch 260:\n",
      "Loss train 0.32199343130737546 valid 0.5485564109539415\n",
      "Epoch 261:\n",
      "Loss train 0.3452643780447543 valid 5.592630509925651\n",
      "Epoch 262:\n",
      "Loss train 0.3445836392760277 valid 0.2865015622424248\n",
      "Epoch 263:\n",
      "Loss train 0.33208015475571157 valid 0.05914117577724394\n",
      "Epoch 264:\n",
      "Loss train 0.32174941826313735 valid 0.09428628507576539\n",
      "Epoch 265:\n",
      "Loss train 0.3730477440185845 valid 0.12952653683151227\n",
      "Epoch 266:\n",
      "Loss train 0.3106110776223242 valid 0.1732062647572256\n",
      "Epoch 267:\n",
      "Loss train 0.40259719085395335 valid 0.05626055848014961\n",
      "Epoch 268:\n",
      "Loss train 0.29941305848285554 valid 0.12012154417531692\n",
      "Epoch 269:\n",
      "Loss train 0.3339389170177281 valid 0.11102174667655597\n",
      "Epoch 270:\n",
      "Loss train 0.33026171877086163 valid 0.411857890502949\n",
      "Epoch 271:\n",
      "Loss train 0.36798519727513196 valid 0.5330561228595764\n",
      "Epoch 272:\n",
      "Loss train 0.30148715584129093 valid 0.10442448011769517\n",
      "Epoch 273:\n",
      "Loss train 0.32591271747201683 valid 0.603268597942056\n",
      "Epoch 274:\n",
      "Loss train 0.33801701036691667 valid 0.1062875618521703\n",
      "Epoch 275:\n",
      "Loss train 0.35138288675695656 valid 0.7505187458184417\n",
      "Epoch 276:\n",
      "Loss train 0.3220461414635181 valid 0.08274531190089188\n",
      "Epoch 277:\n",
      "Loss train 0.32441782910823824 valid 0.06365674263333079\n",
      "Epoch 278:\n",
      "Loss train 0.3218663842685521 valid 0.12151361508819136\n",
      "Epoch 279:\n",
      "Loss train 0.3162790180020034 valid 0.23942746935809286\n",
      "Epoch 280:\n",
      "Loss train 0.3219515962064266 valid 3.059484471607581\n",
      "Epoch 281:\n",
      "Loss train 0.3387310493975878 valid 4.022658680315596\n",
      "Epoch 282:\n",
      "Loss train 0.3325961707964539 valid 0.47388057633440184\n",
      "Epoch 283:\n",
      "Loss train 0.3217828085117042 valid 0.19602642150672542\n",
      "Epoch 284:\n",
      "Loss train 0.3353585140936077 valid 0.07327740688134976\n",
      "Epoch 285:\n",
      "Loss train 0.3203776783883572 valid 0.10724594042069356\n",
      "Epoch 286:\n",
      "Loss train 0.34457876603677867 valid 0.22688519579789548\n",
      "Epoch 287:\n",
      "Loss train 0.3338819872051477 valid 1.118126655007984\n",
      "Epoch 288:\n",
      "Loss train 0.30627515280768275 valid 0.07908128382663286\n",
      "Epoch 289:\n",
      "Loss train 0.31471191158294676 valid 0.3494597461154988\n",
      "Epoch 290:\n",
      "Loss train 0.34317971611618997 valid 0.09939714753063092\n",
      "Epoch 291:\n",
      "Loss train 0.31953817531391976 valid 0.06671470212666597\n",
      "Epoch 292:\n",
      "Loss train 0.3243178058922291 valid 0.10587866006134572\n",
      "Epoch 293:\n",
      "Loss train 0.30261796656250955 valid 1.2246148275998014\n",
      "Epoch 294:\n",
      "Loss train 0.3237311158083379 valid 0.12430748271955648\n",
      "Epoch 295:\n",
      "Loss train 0.3041319429606199 valid 0.3493064658438165\n",
      "Epoch 296:\n",
      "Loss train 0.3223402948103845 valid 0.07070767412766776\n",
      "Epoch 297:\n",
      "Loss train 0.293849786362797 valid 0.06847844172562476\n",
      "Epoch 298:\n",
      "Loss train 0.3368525326758623 valid 0.24476455167972042\n",
      "Epoch 299:\n",
      "Loss train 0.3096081470429897 valid 0.05033810821091909\n",
      "Epoch 300:\n",
      "Loss train 0.3320696709305048 valid 0.16047211039478426\n",
      "Epoch 301:\n",
      "Loss train 0.3175914358988404 valid 0.11762859530741536\n",
      "Epoch 302:\n",
      "Loss train 0.31688432354182006 valid 0.13145166261226596\n",
      "Epoch 303:\n",
      "Loss train 0.3153786082558334 valid 0.29501895460253924\n",
      "Epoch 304:\n",
      "Loss train 0.3131298902608454 valid 0.050013752407796444\n",
      "Epoch 305:\n",
      "Loss train 0.3145568818032742 valid 0.523004621594317\n",
      "Epoch 306:\n",
      "Loss train 0.32397517848685387 valid 0.15922492525691673\n",
      "Epoch 307:\n",
      "Loss train 0.3087988456726074 valid 0.061686784187170544\n",
      "Epoch 308:\n",
      "Loss train 0.3076679699294269 valid 1.5125499777920637\n",
      "Epoch 309:\n",
      "Loss train 0.32594914962649346 valid 0.06019975873798174\n",
      "Epoch 310:\n",
      "Loss train 0.301318453040719 valid 0.29812743286077503\n",
      "Epoch 311:\n",
      "Loss train 0.32024483211785554 valid 0.05218006708136512\n",
      "Epoch 312:\n",
      "Loss train 0.29225235950350764 valid 0.7976986706626054\n",
      "Epoch 313:\n",
      "Loss train 0.3386677277214825 valid 1.1588419224009787\n",
      "Epoch 314:\n",
      "Loss train 0.31306253123581407 valid 0.18058867226012565\n",
      "Epoch 315:\n",
      "Loss train 0.3000696312978864 valid 0.055319378766319725\n",
      "Epoch 316:\n",
      "Loss train 0.3080116975180805 valid 0.059022098226507454\n",
      "Epoch 317:\n",
      "Loss train 0.30920423769652844 valid 0.2664473455267955\n",
      "Epoch 318:\n",
      "Loss train 0.3108531311169267 valid 0.12550339536912436\n",
      "Epoch 319:\n",
      "Loss train 0.2976442909732461 valid 0.3447821953837917\n",
      "Epoch 320:\n",
      "Loss train 0.292058127541095 valid 0.11363988900833934\n",
      "Epoch 321:\n",
      "Loss train 0.3166437294743955 valid 0.056665063654434425\n",
      "Epoch 322:\n",
      "Loss train 0.3158400792501867 valid 0.16039988085734394\n",
      "Epoch 323:\n",
      "Loss train 0.3043286452069879 valid 0.09214705264624373\n",
      "Epoch 324:\n",
      "Loss train 0.3017491178929806 valid 0.06526136339558426\n",
      "Epoch 325:\n",
      "Loss train 0.3106160184681416 valid 0.11521747285918073\n",
      "Epoch 326:\n",
      "Loss train 0.31002862547934057 valid 0.07082619041342175\n",
      "Epoch 327:\n",
      "Loss train 0.31603187995627524 valid 0.0588559105166248\n",
      "Epoch 328:\n",
      "Loss train 0.313895913553983 valid 0.13052958010995985\n",
      "Epoch 329:\n",
      "Loss train 0.2945564557731152 valid 0.1508176550790624\n",
      "Epoch 330:\n",
      "Loss train 0.2945883127629757 valid 0.3345171346633086\n",
      "Epoch 331:\n",
      "Loss train 0.2937907840907574 valid 0.15506987687873916\n",
      "Epoch 332:\n",
      "Loss train 0.31615358528867366 valid 0.0712780510896194\n",
      "Epoch 333:\n",
      "Loss train 0.3101368536002934 valid 0.3511794537389533\n",
      "Epoch 334:\n",
      "Loss train 0.3849605867832899 valid 0.05222634161659879\n",
      "Epoch 335:\n",
      "Loss train 0.2753945348717272 valid 0.07155284721497772\n",
      "Epoch 336:\n",
      "Loss train 0.2942061433106661 valid 0.06611177810498772\n",
      "Epoch 337:\n",
      "Loss train 0.2946407867617905 valid 0.17643902317425592\n",
      "Epoch 338:\n",
      "Loss train 0.3038491316795349 valid 0.08833931655376442\n",
      "Epoch 339:\n",
      "Loss train 0.2862030924618244 valid 0.04612215221097334\n",
      "Epoch 340:\n",
      "Loss train 0.290025340846926 valid 0.09722137435756428\n",
      "Epoch 341:\n",
      "Loss train 0.28396213392987846 valid 0.13023295460400033\n",
      "Epoch 342:\n",
      "Loss train 0.3141064035430551 valid 0.05157898381812103\n",
      "Epoch 343:\n",
      "Loss train 0.2782985837645829 valid 0.2599312322184405\n",
      "Epoch 344:\n",
      "Loss train 0.2839845218345523 valid 0.12035034311580443\n",
      "Epoch 345:\n",
      "Loss train 0.29767203439325096 valid 0.15371056244067186\n",
      "Epoch 346:\n",
      "Loss train 0.28326084118410944 valid 2.0394389033113307\n",
      "Epoch 347:\n",
      "Loss train 0.3052618377484381 valid 0.05090741370814606\n",
      "Epoch 348:\n",
      "Loss train 0.2959837323486805 valid 0.23540270444555222\n",
      "Epoch 349:\n",
      "Loss train 0.28794762522131206 valid 0.07918943115239994\n",
      "Epoch 350:\n",
      "Loss train 0.3023505514197051 valid 0.11931229234207587\n",
      "Epoch 351:\n",
      "Loss train 0.3085715738274157 valid 0.11806318960748168\n",
      "Epoch 352:\n",
      "Loss train 0.2829190111652017 valid 0.24348967704066807\n",
      "Epoch 353:\n",
      "Loss train 0.2930507936105132 valid 0.7450804852442781\n",
      "Epoch 354:\n",
      "Loss train 0.3018686923701316 valid 0.27367599721323815\n",
      "Epoch 355:\n",
      "Loss train 0.2953933300614357 valid 0.06467135003802428\n",
      "Epoch 356:\n",
      "Loss train 0.3088337108582258 valid 0.044665426674658316\n",
      "Epoch 357:\n",
      "Loss train 0.2771645969629288 valid 0.060453110111989276\n",
      "Epoch 358:\n",
      "Loss train 0.2897694654103369 valid 0.04556117081032704\n",
      "Epoch 359:\n",
      "Loss train 0.28259233138114215 valid 0.7025314090442478\n",
      "Epoch 360:\n",
      "Loss train 0.28892943934239446 valid 0.0568024373220759\n",
      "Epoch 361:\n",
      "Loss train 0.28747479700781403 valid 0.06780247825816074\n",
      "Epoch 362:\n",
      "Loss train 0.2982072571121156 valid 0.21900210697753633\n",
      "Epoch 363:\n",
      "Loss train 0.2819779219597578 valid 1.5200557149894685\n",
      "Epoch 364:\n",
      "Loss train 0.26950673022046684 valid 0.11396895243342552\n",
      "Epoch 365:\n",
      "Loss train 0.2908714325584471 valid 0.09680200351172957\n",
      "Epoch 366:\n",
      "Loss train 0.28714740694388746 valid 0.04854941776663763\n",
      "Epoch 367:\n",
      "Loss train 0.2741354102872312 valid 0.11064687572002865\n",
      "Epoch 368:\n",
      "Loss train 0.2952438408214599 valid 0.15311701745373324\n",
      "Epoch 369:\n",
      "Loss train 0.3071093077994883 valid 0.04552124250021939\n",
      "Epoch 370:\n",
      "Loss train 0.2688079775586724 valid 0.2891803939249853\n",
      "Epoch 371:\n",
      "Loss train 0.2725050102442503 valid 0.2664286609839261\n",
      "Epoch 372:\n",
      "Loss train 0.2762235439568758 valid 0.08060160124148928\n",
      "Epoch 373:\n",
      "Loss train 0.2889071211449802 valid 0.2450588767957427\n",
      "Epoch 374:\n",
      "Loss train 0.27532876196429135 valid 0.06393921028514746\n",
      "Epoch 375:\n",
      "Loss train 0.303080985782668 valid 0.2238480467077196\n",
      "Epoch 376:\n",
      "Loss train 0.2956412335418165 valid 0.06278057909461562\n",
      "Epoch 377:\n",
      "Loss train 0.2714217463128269 valid 0.36465373273650764\n",
      "Epoch 378:\n",
      "Loss train 0.261866877547279 valid 0.12056019127411113\n",
      "Epoch 379:\n",
      "Loss train 0.29466618727818134 valid 0.06476420961171939\n",
      "Epoch 380:\n",
      "Loss train 0.2686415796451271 valid 0.44886680440827587\n",
      "Epoch 381:\n",
      "Loss train 0.279734781492874 valid 0.15736445650891454\n",
      "Epoch 382:\n",
      "Loss train 0.27880702359341086 valid 0.09256212100358042\n",
      "Epoch 383:\n",
      "Loss train 0.2868212707154453 valid 0.11367890095405057\n",
      "Epoch 384:\n",
      "Loss train 0.2635250518403947 valid 0.0822776125510052\n",
      "Epoch 385:\n",
      "Loss train 0.2712147461369634 valid 0.11624617477532066\n",
      "Epoch 386:\n",
      "Loss train 0.30242601260989904 valid 0.7282641572630061\n",
      "Epoch 387:\n",
      "Loss train 0.2619717663090676 valid 1.7788858629502833\n",
      "Epoch 388:\n",
      "Loss train 0.2781553433198482 valid 0.22832409244828253\n",
      "Epoch 389:\n",
      "Loss train 0.28957996311336753 valid 0.045194168523857776\n",
      "Epoch 390:\n",
      "Loss train 0.2580694081351161 valid 0.06817780825769493\n",
      "Epoch 391:\n",
      "Loss train 0.28884326828941703 valid 0.08445697301849799\n",
      "Epoch 392:\n",
      "Loss train 0.25712511400207877 valid 0.23611738908960567\n",
      "Epoch 393:\n",
      "Loss train 0.2848064181104302 valid 0.0463171248969975\n",
      "Epoch 394:\n",
      "Loss train 0.27403470214009285 valid 0.05743609145047289\n",
      "Epoch 395:\n",
      "Loss train 0.2719287027195096 valid 0.5092035717845516\n",
      "Epoch 396:\n",
      "Loss train 0.2769905946377665 valid 0.07043944538221918\n",
      "Epoch 397:\n",
      "Loss train 0.2799121376380324 valid 0.6236172636485928\n",
      "Epoch 398:\n",
      "Loss train 0.29009913259521125 valid 0.042214380396323796\n",
      "Epoch 399:\n",
      "Loss train 0.2621563882295042 valid 0.11068845457427719\n",
      "Epoch 400:\n",
      "Loss train 0.27782706950716674 valid 0.04272535689856673\n",
      "Epoch 401:\n",
      "Loss train 0.2738617404744029 valid 3.2619535428802418\n",
      "Epoch 402:\n",
      "Loss train 0.2711526934929192 valid 0.04279568491425312\n",
      "Epoch 403:\n",
      "Loss train 0.262875266122818 valid 0.0680665073638075\n",
      "Epoch 404:\n",
      "Loss train 0.26952282326482235 valid 0.06366029191425203\n",
      "Epoch 405:\n",
      "Loss train 0.2764833306610584 valid 0.0988455696902489\n",
      "Epoch 406:\n",
      "Loss train 0.26194490073882043 valid 0.14553286012574296\n",
      "Epoch 407:\n",
      "Loss train 0.2830706294167787 valid 0.10752233387890339\n",
      "Epoch 408:\n",
      "Loss train 0.25225063449703156 valid 0.03550321801418917\n",
      "Epoch 409:\n",
      "Loss train 0.26995986783877013 valid 0.12278785571681215\n",
      "Epoch 410:\n",
      "Loss train 0.2873912329018116 valid 0.21817228401078237\n",
      "Epoch 411:\n",
      "Loss train 0.2581682644337416 valid 0.22963236605967574\n",
      "Epoch 412:\n",
      "Loss train 0.27801706899479034 valid 0.050887185137336664\n",
      "Epoch 413:\n",
      "Loss train 0.2645769990839064 valid 0.2182231165057064\n",
      "Epoch 414:\n",
      "Loss train 0.2770877362649888 valid 0.0924465293672964\n",
      "Epoch 415:\n",
      "Loss train 0.2893991383295506 valid 0.0702582958310075\n",
      "Epoch 416:\n",
      "Loss train 0.2553932318206876 valid 3.4657240180642974\n",
      "Epoch 417:\n",
      "Loss train 0.25570193465054036 valid 0.14789567291762032\n",
      "Epoch 418:\n",
      "Loss train 0.28265118339434264 valid 0.04003000097887983\n",
      "Epoch 419:\n",
      "Loss train 0.2633882566589862 valid 0.049192077262852005\n",
      "Epoch 420:\n",
      "Loss train 0.26157053169645367 valid 0.05267410387743126\n",
      "Epoch 421:\n",
      "Loss train 0.2663280940309167 valid 0.2783324953745104\n",
      "Epoch 422:\n",
      "Loss train 0.2690981198936701 valid 0.10328256281860027\n",
      "Epoch 423:\n",
      "Loss train 0.2639181042037904 valid 0.046038091531284006\n",
      "Epoch 424:\n",
      "Loss train 0.2682517765313387 valid 3.1099180906432067\n",
      "Epoch 425:\n",
      "Loss train 0.2604886411912739 valid 0.2628443104962871\n",
      "Epoch 426:\n",
      "Loss train 0.2672268733434379 valid 0.08601128864608494\n",
      "Epoch 427:\n",
      "Loss train 0.26248840057440104 valid 0.9020928024409908\n",
      "Epoch 428:\n",
      "Loss train 0.26106652102209627 valid 0.050131022423305575\n",
      "Epoch 429:\n",
      "Loss train 0.26621836308538915 valid 0.06463561927339592\n",
      "Epoch 430:\n",
      "Loss train 0.26783436134532096 valid 0.04685411129791301\n",
      "Epoch 431:\n",
      "Loss train 0.2697629549924284 valid 0.09332207441143885\n",
      "Epoch 432:\n",
      "Loss train 0.2607601545687765 valid 0.04168908392494276\n",
      "Epoch 433:\n",
      "Loss train 0.26932601035237314 valid 0.05929230546096429\n",
      "Epoch 434:\n",
      "Loss train 0.24576678765192628 valid 0.11663448758783532\n",
      "Epoch 435:\n",
      "Loss train 0.25672073004208507 valid 0.0896920017325009\n",
      "Epoch 436:\n",
      "Loss train 0.2628527333140373 valid 0.11731493526075225\n",
      "Epoch 437:\n",
      "Loss train 0.2540751418132335 valid 0.07435091977608774\n",
      "Epoch 438:\n",
      "Loss train 0.2516202335909009 valid 1.2027746828686332\n",
      "Epoch 439:\n",
      "Loss train 0.26680653175376357 valid 0.08843592987270053\n",
      "Epoch 440:\n",
      "Loss train 0.2664885463580489 valid 0.19069280898458674\n",
      "Epoch 441:\n",
      "Loss train 0.2459220541253686 valid 0.0979332310859403\n",
      "Epoch 442:\n",
      "Loss train 0.26608654950782656 valid 0.19391693337016083\n",
      "Epoch 443:\n",
      "Loss train 0.2724190081961453 valid 0.08163371495587388\n",
      "Epoch 444:\n",
      "Loss train 0.24476361748576164 valid 0.04147035394124025\n",
      "Epoch 445:\n",
      "Loss train 0.2453073145892471 valid 0.05658895076579608\n",
      "Epoch 446:\n",
      "Loss train 0.25656777456253765 valid 0.05794140343210369\n",
      "Epoch 447:\n",
      "Loss train 0.2482542356994003 valid 0.06464412932853875\n",
      "Epoch 448:\n",
      "Loss train 0.2593195403780788 valid 0.18833441813485904\n",
      "Epoch 449:\n",
      "Loss train 0.27531545525602996 valid 0.19226052665827426\n",
      "Epoch 450:\n",
      "Loss train 0.25162837382145226 valid 0.15807461675788054\n",
      "Epoch 451:\n",
      "Loss train 0.25957803093902765 valid 0.2483222101980809\n",
      "Epoch 452:\n",
      "Loss train 0.24181076794601977 valid 0.050537049656939226\n",
      "Epoch 453:\n",
      "Loss train 0.2558078589946032 valid 0.4905413733507934\n",
      "Epoch 454:\n",
      "Loss train 0.2753860695216805 valid 0.06625779457201712\n",
      "Epoch 455:\n",
      "Loss train 0.25149029369875786 valid 0.04950100701122501\n",
      "Epoch 456:\n",
      "Loss train 0.2649125948380679 valid 1.168850373710623\n",
      "Epoch 457:\n",
      "Loss train 0.2341553613767028 valid 2.557086900633106\n",
      "Epoch 458:\n",
      "Loss train 0.24688720485419036 valid 0.04668810321181794\n",
      "Epoch 459:\n",
      "Loss train 0.2470107437375933 valid 0.8054571535874193\n",
      "Epoch 460:\n",
      "Loss train 0.27488766508474943 valid 0.06153114745906928\n",
      "Epoch 461:\n",
      "Loss train 0.22886693101376296 valid 0.05703958082930173\n",
      "Epoch 462:\n",
      "Loss train 0.2603318317577243 valid 0.045241154401888965\n",
      "Epoch 463:\n",
      "Loss train 0.24776195373311638 valid 0.060187302277079656\n",
      "Epoch 464:\n",
      "Loss train 0.2544906493879855 valid 0.6534573879910643\n",
      "Epoch 465:\n",
      "Loss train 0.24403667720071973 valid 4.003815259463176\n",
      "Epoch 466:\n",
      "Loss train 0.24243463933803142 valid 0.18598633795891334\n",
      "Epoch 467:\n",
      "Loss train 0.23492320376746356 valid 0.05235627093404305\n",
      "Epoch 468:\n",
      "Loss train 0.2535991655781865 valid 0.07161115349329679\n",
      "Epoch 469:\n",
      "Loss train 0.2524019913777709 valid 0.05221515938820327\n",
      "Epoch 470:\n",
      "Loss train 0.2399502175860107 valid 0.2360181586637765\n",
      "Epoch 471:\n",
      "Loss train 0.2513564883697778 valid 0.08757863120129511\n",
      "Epoch 472:\n",
      "Loss train 0.28049244931787254 valid 0.04577793766640085\n",
      "Epoch 473:\n",
      "Loss train 0.23221161189526318 valid 0.09195065392553466\n",
      "Epoch 474:\n",
      "Loss train 0.24176382415890693 valid 0.2066539663063503\n",
      "Epoch 475:\n",
      "Loss train 0.2727027462929487 valid 0.03975474700009317\n",
      "Epoch 476:\n",
      "Loss train 0.25670459344014523 valid 0.04322620248745167\n",
      "Epoch 477:\n",
      "Loss train 0.24107465141490103 valid 0.05957163867759201\n",
      "Epoch 478:\n",
      "Loss train 0.25632867865748704 valid 0.10964948800121445\n",
      "Epoch 479:\n",
      "Loss train 0.2613867131575942 valid 0.055316137451903\n",
      "Epoch 480:\n",
      "Loss train 0.24976002070195974 valid 0.034039126970137557\n",
      "Epoch 481:\n",
      "Loss train 0.23811033228188752 valid 0.05840087835070765\n",
      "Epoch 482:\n",
      "Loss train 0.24552563168480993 valid 0.09033833917231504\n",
      "Epoch 483:\n",
      "Loss train 0.24349317464642226 valid 0.22346183659774163\n",
      "Epoch 484:\n",
      "Loss train 0.240277945189178 valid 0.41525618028086647\n",
      "Epoch 485:\n",
      "Loss train 0.24330471944063903 valid 0.04882492732118508\n",
      "Epoch 486:\n",
      "Loss train 0.24454492628760635 valid 0.045099570914068784\n",
      "Epoch 487:\n",
      "Loss train 0.24617184078842402 valid 0.10822611861396658\n",
      "Epoch 488:\n",
      "Loss train 0.2616009688824415 valid 0.15831405037661334\n",
      "Epoch 489:\n",
      "Loss train 0.25966845047064124 valid 0.03708138574868941\n",
      "Epoch 490:\n",
      "Loss train 0.22374168778508902 valid 0.050704399128501956\n",
      "Epoch 491:\n",
      "Loss train 0.24976421832665802 valid 1.4001951732677864\n",
      "Epoch 492:\n",
      "Loss train 0.2690330589234829 valid 0.03439607356262086\n",
      "Epoch 493:\n",
      "Loss train 0.23398135897703468 valid 0.04669054661147062\n",
      "Epoch 494:\n",
      "Loss train 0.24271757917255163 valid 0.11109709626751632\n",
      "Epoch 495:\n",
      "Loss train 0.24684134412668646 valid 0.41264233071413947\n",
      "Epoch 496:\n",
      "Loss train 0.23832212309874595 valid 0.053003710137344416\n",
      "Epoch 497:\n",
      "Loss train 0.2578283553183079 valid 0.04826234829367789\n",
      "Epoch 498:\n",
      "Loss train 0.22603235124610366 valid 1.1697457109758207\n",
      "Epoch 499:\n",
      "Loss train 0.23185091348029674 valid 0.0746766954152333\n",
      "Epoch 500:\n",
      "Loss train 0.24632566859610378 valid 0.10167531212854088\n",
      "Epoch 501:\n",
      "Loss train 0.2512545460831374 valid 0.33735138752741056\n",
      "Epoch 502:\n",
      "Loss train 0.2198344702973962 valid 0.2636820303368526\n",
      "Epoch 503:\n",
      "Loss train 0.23570800959393381 valid 0.04102164743671818\n",
      "Epoch 504:\n",
      "Loss train 0.2695876458887011 valid 0.031491378185711724\n",
      "Epoch 505:\n",
      "Loss train 0.24074512050263583 valid 0.0813948348663006\n",
      "Epoch 506:\n",
      "Loss train 0.264832468925789 valid 0.03221324119512421\n",
      "Epoch 507:\n",
      "Loss train 0.21224093478247524 valid 0.250687421441843\n",
      "Epoch 508:\n",
      "Loss train 0.2555022423863411 valid 0.03255744829237089\n",
      "Epoch 509:\n",
      "Loss train 0.2225967515166849 valid 0.30500727269433686\n",
      "Epoch 510:\n",
      "Loss train 0.2403447401933372 valid 0.04754448429276075\n",
      "Epoch 511:\n",
      "Loss train 0.23888177729174495 valid 0.21818501248449773\n",
      "Epoch 512:\n",
      "Loss train 0.246067056985572 valid 0.048992585145212444\n",
      "Epoch 513:\n",
      "Loss train 0.23419397496432065 valid 0.274111698844745\n",
      "Epoch 514:\n",
      "Loss train 0.24431367623247205 valid 0.049626115701088006\n",
      "Epoch 515:\n",
      "Loss train 0.2411160818759352 valid 0.039089238981537655\n",
      "Epoch 516:\n",
      "Loss train 0.2210206704404205 valid 0.1484185658203776\n",
      "Epoch 517:\n",
      "Loss train 0.24127040815390646 valid 0.044049459694697486\n",
      "Epoch 518:\n",
      "Loss train 0.244734248303622 valid 0.3242622698089131\n",
      "Epoch 519:\n",
      "Loss train 0.23199624548479914 valid 0.12348285229923618\n",
      "Epoch 520:\n",
      "Loss train 0.242864267577976 valid 0.11516634220148134\n",
      "Epoch 521:\n",
      "Loss train 0.2357440432306379 valid 0.12698381793276248\n",
      "Epoch 522:\n",
      "Loss train 0.24031217721812426 valid 0.0834655004833232\n",
      "Epoch 523:\n",
      "Loss train 0.2504228646080941 valid 0.04808939203513083\n",
      "Epoch 524:\n",
      "Loss train 0.21841240424998104 valid 0.03222358526906643\n",
      "Epoch 525:\n",
      "Loss train 0.23265439776591956 valid 0.030081982540738315\n",
      "Epoch 526:\n",
      "Loss train 0.2263770112171769 valid 0.22419732351803098\n",
      "Epoch 527:\n",
      "Loss train 0.23603693619444965 valid 0.05939495634833659\n",
      "Epoch 528:\n",
      "Loss train 0.2365259804069996 valid 0.03260435815392078\n",
      "Epoch 529:\n",
      "Loss train 0.22457827608399092 valid 0.07040223724437068\n",
      "Epoch 530:\n",
      "Loss train 0.23703673571161926 valid 0.04890863469485396\n",
      "Epoch 531:\n",
      "Loss train 0.2401830478068441 valid 0.043984942924745085\n",
      "Epoch 532:\n",
      "Loss train 0.22892085795663297 valid 0.32441155437081354\n",
      "Epoch 533:\n",
      "Loss train 0.23828529440946877 valid 0.2079895904508893\n",
      "Epoch 534:\n",
      "Loss train 0.22795080695338546 valid 0.03308078620330538\n",
      "Epoch 535:\n",
      "Loss train 0.2304140519477427 valid 0.0911843440461432\n",
      "Epoch 536:\n",
      "Loss train 0.22110504867434502 valid 0.045682993003568456\n",
      "Epoch 537:\n",
      "Loss train 0.23637250957936048 valid 0.06517100107161582\n",
      "Epoch 538:\n",
      "Loss train 0.21953699111789465 valid 0.054780373167598685\n",
      "Epoch 539:\n",
      "Loss train 0.2332150313768536 valid 0.05326354608613927\n",
      "Epoch 540:\n",
      "Loss train 0.23421451716832817 valid 0.0834077218140928\n",
      "Epoch 541:\n",
      "Loss train 0.23262103762663902 valid 0.04314094544905558\n",
      "Epoch 542:\n",
      "Loss train 0.2446718256752938 valid 0.02561542418464568\n",
      "Epoch 543:\n",
      "Loss train 0.23141197247505188 valid 0.028289127146726472\n",
      "Epoch 544:\n",
      "Loss train 0.22416780662201344 valid 0.08606526549145872\n",
      "Epoch 545:\n",
      "Loss train 0.2344245252262801 valid 0.06233654288465933\n",
      "Epoch 546:\n",
      "Loss train 0.220556661798805 valid 0.3377086867882685\n",
      "Epoch 547:\n",
      "Loss train 0.23422253610678018 valid 0.037737616018003635\n",
      "Epoch 548:\n",
      "Loss train 0.22974059570245445 valid 0.06321484766715953\n",
      "Epoch 549:\n",
      "Loss train 0.22453881316930055 valid 0.03170806524323556\n",
      "Epoch 550:\n",
      "Loss train 0.22814595840834082 valid 0.06101747515502376\n",
      "Epoch 551:\n",
      "Loss train 0.22046757589951158 valid 0.07131044420006813\n",
      "Epoch 552:\n",
      "Loss train 0.22934149359948933 valid 0.13922065607600528\n",
      "Epoch 553:\n",
      "Loss train 0.2533220457408577 valid 0.03741412487647183\n",
      "Epoch 554:\n",
      "Loss train 0.2224286497808993 valid 0.08242602290566534\n",
      "Epoch 555:\n",
      "Loss train 0.21071508585959672 valid 0.04215396149460996\n",
      "Epoch 556:\n",
      "Loss train 0.21667923742830753 valid 0.04303116480180578\n",
      "Epoch 557:\n",
      "Loss train 0.2255392364885658 valid 0.041996418122986365\n",
      "Epoch 558:\n",
      "Loss train 0.2299852333974093 valid 0.11677877836771161\n",
      "Epoch 559:\n",
      "Loss train 0.22404543619155884 valid 0.15692567923996922\n",
      "Epoch 560:\n",
      "Loss train 0.2327886520911008 valid 0.21867623728889973\n",
      "Epoch 561:\n",
      "Loss train 0.2262025075599551 valid 0.37360393189633484\n",
      "Epoch 562:\n",
      "Loss train 0.22278905111923814 valid 0.05950573468853044\n",
      "Epoch 563:\n",
      "Loss train 0.2323511175725609 valid 3.748497481304726\n",
      "Epoch 564:\n",
      "Loss train 0.2373806367415935 valid 0.11092784215019769\n",
      "Epoch 565:\n",
      "Loss train 0.22738420185334982 valid 0.6109618248387474\n",
      "Epoch 566:\n",
      "Loss train 0.23686547433622182 valid 0.06777637245964854\n",
      "Epoch 567:\n",
      "Loss train 0.22445820864401758 valid 0.19826579125380026\n",
      "Epoch 568:\n",
      "Loss train 0.2247766068674624 valid 0.07584222419704532\n",
      "Epoch 569:\n",
      "Loss train 0.2339030514191836 valid 0.04812346977713227\n",
      "Epoch 570:\n",
      "Loss train 0.20907474055103958 valid 0.09443478105797526\n",
      "Epoch 571:\n",
      "Loss train 0.2474591474648565 valid 0.16113895541198844\n",
      "Epoch 572:\n",
      "Loss train 0.22469056049622596 valid 0.5355348488845002\n",
      "Epoch 573:\n",
      "Loss train 0.22892888924144209 valid 0.14017907075832597\n",
      "Epoch 574:\n",
      "Loss train 0.2228231219973415 valid 0.20678863693001748\n",
      "Epoch 575:\n",
      "Loss train 0.22014619084857404 valid 0.5439962744562916\n",
      "Epoch 576:\n",
      "Loss train 0.2317802782945335 valid 0.03320690560919838\n",
      "Epoch 577:\n",
      "Loss train 0.21821531346738338 valid 0.29975862774572737\n",
      "Epoch 578:\n",
      "Loss train 0.23057524241022764 valid 0.07373579271623253\n",
      "Epoch 579:\n",
      "Loss train 0.23378459453284742 valid 0.02932500551159413\n",
      "Epoch 580:\n",
      "Loss train 0.20397851551137863 valid 0.035027178879236225\n",
      "Epoch 581:\n",
      "Loss train 0.23146546473652124 valid 1.7677928625098016\n",
      "Epoch 582:\n",
      "Loss train 0.2196959372200072 valid 0.0326369782023161\n",
      "Epoch 583:\n",
      "Loss train 0.23279439681433142 valid 0.04908036631393417\n",
      "Epoch 584:\n",
      "Loss train 0.2121654206313193 valid 0.05821462378958605\n",
      "Epoch 585:\n",
      "Loss train 0.2181866849962622 valid 0.04458518129074123\n",
      "Epoch 586:\n",
      "Loss train 0.22130925839059054 valid 1.1493049586634456\n",
      "Epoch 587:\n",
      "Loss train 0.22465662561282515 valid 0.32209124466855116\n",
      "Epoch 588:\n",
      "Loss train 0.21615362720787526 valid 0.11979484615649087\n",
      "Epoch 589:\n",
      "Loss train 0.22916381048485637 valid 0.4382492743500809\n",
      "Epoch 590:\n",
      "Loss train 0.23698464856408535 valid 0.03272413204134924\n",
      "Epoch 591:\n",
      "Loss train 0.20556128358952702 valid 0.07471241249049233\n",
      "Epoch 592:\n",
      "Loss train 0.2246030658815056 valid 0.045596573446205726\n",
      "Epoch 593:\n",
      "Loss train 0.22485565988905729 valid 0.0586944079064368\n",
      "Epoch 594:\n",
      "Loss train 0.24779655158892275 valid 0.8261021192468312\n",
      "Epoch 595:\n",
      "Loss train 0.21238404307216405 valid 0.041533481033690936\n",
      "Epoch 596:\n",
      "Loss train 0.21268493794724344 valid 0.07864211303394379\n",
      "Epoch 597:\n",
      "Loss train 0.22909329737052322 valid 0.05000123900058932\n",
      "Epoch 598:\n",
      "Loss train 0.23975651337876916 valid 0.05678461968882057\n",
      "Epoch 599:\n",
      "Loss train 0.22505368681661786 valid 0.05006819473828456\n",
      "Epoch 600:\n",
      "Loss train 0.21832804152816535 valid 0.06629706203915676\n",
      "Epoch 601:\n",
      "Loss train 0.20701260887086392 valid 0.20406907578216824\n",
      "Epoch 602:\n",
      "Loss train 0.21638922287747264 valid 0.029084204614141053\n",
      "Epoch 603:\n",
      "Loss train 0.22429385696724058 valid 0.02838248766482472\n",
      "Epoch 604:\n",
      "Loss train 0.22249019541107118 valid 0.04143954755455724\n",
      "Epoch 605:\n",
      "Loss train 0.21283151678442955 valid 0.16265370206170257\n",
      "Epoch 606:\n",
      "Loss train 0.2236913061529398 valid 0.03506377860461389\n",
      "Epoch 607:\n",
      "Loss train 0.22073399963490664 valid 0.10979247665496004\n",
      "Epoch 608:\n",
      "Loss train 0.22014772166088223 valid 0.10955229904077836\n",
      "Epoch 609:\n",
      "Loss train 0.2192862414494157 valid 0.02401491554855863\n",
      "Epoch 610:\n",
      "Loss train 0.21937434531338512 valid 0.5802391777495278\n",
      "Epoch 611:\n",
      "Loss train 0.22131869562268258 valid 0.059224832481127074\n",
      "Epoch 612:\n",
      "Loss train 0.2205974089719355 valid 0.042784580484976305\n",
      "Epoch 613:\n",
      "Loss train 0.21192958628199995 valid 0.03750303091657853\n",
      "Epoch 614:\n",
      "Loss train 0.22414162234067916 valid 0.4731974473218175\n",
      "Epoch 615:\n",
      "Loss train 0.21245939516089857 valid 0.03782755660260592\n",
      "Epoch 616:\n",
      "Loss train 0.21070086255855858 valid 0.046797062995777565\n",
      "Epoch 617:\n",
      "Loss train 0.22101595171913505 valid 0.11258418527545772\n",
      "Epoch 618:\n",
      "Loss train 0.20911529206112028 valid 0.3948424768346994\n",
      "Epoch 619:\n",
      "Loss train 0.21028167722560465 valid 0.8919591045901903\n",
      "Epoch 620:\n",
      "Loss train 0.21321399520598353 valid 0.09693024691236998\n",
      "Epoch 621:\n",
      "Loss train 0.2085426485840231 valid 0.27238126072393626\n",
      "Epoch 622:\n",
      "Loss train 0.22520089396759868 valid 0.22356466668624994\n",
      "Epoch 623:\n",
      "Loss train 0.1987779319319874 valid 0.05957336701928935\n",
      "Epoch 624:\n",
      "Loss train 0.2149583771519363 valid 0.3184435176918484\n",
      "Epoch 625:\n",
      "Loss train 0.2123773812290281 valid 0.047576010271529336\n",
      "Epoch 626:\n",
      "Loss train 0.21109922495409847 valid 0.033196045434412166\n",
      "Epoch 627:\n",
      "Loss train 0.22338797102235258 valid 0.02705175163190349\n",
      "Epoch 628:\n",
      "Loss train 0.20656506663039326 valid 0.26089650371829\n",
      "Epoch 629:\n",
      "Loss train 0.19961056053340434 valid 0.09038637271735851\n",
      "Epoch 630:\n",
      "Loss train 0.2243852319329977 valid 0.038205054116112004\n",
      "Epoch 631:\n",
      "Loss train 0.21167274202667177 valid 0.044382781435384676\n",
      "Epoch 632:\n",
      "Loss train 0.22103555657230317 valid 0.03295341989831663\n",
      "Epoch 633:\n",
      "Loss train 0.20782567293718457 valid 0.08240808588178251\n",
      "Epoch 634:\n",
      "Loss train 0.20504686210863293 valid 0.23018153169005684\n",
      "Epoch 635:\n",
      "Loss train 0.20505133984982968 valid 0.1760804968971578\n",
      "Epoch 636:\n",
      "Loss train 0.21051688742935656 valid 0.04546961114234771\n",
      "Epoch 637:\n",
      "Loss train 0.22926262447945775 valid 0.9779081342524699\n",
      "Epoch 638:\n",
      "Loss train 0.21797505357973276 valid 0.07235315780998314\n",
      "Epoch 639:\n",
      "Loss train 0.2199924067195505 valid 0.6855103683263927\n",
      "Epoch 640:\n",
      "Loss train 0.20545624460689724 valid 0.06265447160361111\n",
      "Epoch 641:\n",
      "Loss train 0.20602853718400002 valid 0.6335140794234332\n",
      "Epoch 642:\n",
      "Loss train 0.21126749553829433 valid 0.03643807811469028\n",
      "Epoch 643:\n",
      "Loss train 0.2215190715394914 valid 0.023514848961187582\n",
      "Epoch 644:\n",
      "Loss train 0.19186689768321813 valid 0.10224695392629728\n",
      "Epoch 645:\n",
      "Loss train 0.21146201443746687 valid 0.03706392338704966\n",
      "Epoch 646:\n",
      "Loss train 0.207102657488361 valid 0.10736057326651532\n",
      "Epoch 647:\n",
      "Loss train 0.21276358077563345 valid 0.7077847962030285\n",
      "Epoch 648:\n",
      "Loss train 0.19768829421103 valid 0.0538251859167028\n",
      "Epoch 649:\n",
      "Loss train 0.21095739185921847 valid 0.15013192867021036\n",
      "Epoch 650:\n",
      "Loss train 0.21081764423698188 valid 0.027478359343306655\n",
      "Epoch 651:\n",
      "Loss train 0.20379791535399855 valid 0.03568025901114918\n",
      "Epoch 652:\n",
      "Loss train 0.203634750630334 valid 0.4970977673007338\n",
      "Epoch 653:\n",
      "Loss train 0.20331462180167437 valid 0.044845275391279825\n",
      "Epoch 654:\n",
      "Loss train 0.21062363960780203 valid 0.039905689061301444\n",
      "Epoch 655:\n",
      "Loss train 0.2172117677498609 valid 0.04076079631575865\n",
      "Epoch 656:\n",
      "Loss train 0.20701706069670617 valid 0.050116427785606064\n",
      "Epoch 657:\n",
      "Loss train 0.20571631038188934 valid 1.0187838845124286\n",
      "Epoch 658:\n",
      "Loss train 0.2132424949452281 valid 0.17633901980765238\n",
      "Epoch 659:\n",
      "Loss train 0.21350080433413388 valid 0.030518946530307373\n",
      "Epoch 660:\n",
      "Loss train 0.20613598517775536 valid 0.04790742944997106\n",
      "Epoch 661:\n",
      "Loss train 0.204124287211895 valid 0.10016511630924854\n",
      "Epoch 662:\n",
      "Loss train 0.19730330011546612 valid 0.24265198182662476\n",
      "Epoch 663:\n",
      "Loss train 0.20170080726668238 valid 0.11968546789108193\n",
      "Epoch 664:\n",
      "Loss train 0.2028620919946581 valid 0.34262927704740936\n",
      "Epoch 665:\n",
      "Loss train 0.20277155199274421 valid 0.22091535683766259\n",
      "Epoch 666:\n",
      "Loss train 0.20188821244090796 valid 0.07590713576953059\n",
      "Epoch 667:\n",
      "Loss train 0.2025516242649406 valid 0.3268165960410661\n",
      "Epoch 668:\n",
      "Loss train 0.2189732810854912 valid 0.035789887453758565\n",
      "Epoch 669:\n",
      "Loss train 0.20760472397878765 valid 0.46295619737082194\n",
      "Epoch 670:\n",
      "Loss train 0.20183707663938402 valid 0.1141754549082233\n",
      "Epoch 671:\n",
      "Loss train 0.2204938271164894 valid 0.036012607857441294\n",
      "Epoch 672:\n",
      "Loss train 0.18758664092384278 valid 0.22993155604421636\n",
      "Epoch 673:\n",
      "Loss train 0.20652628969550132 valid 0.03159480845524269\n",
      "Epoch 674:\n",
      "Loss train 0.19812589437030256 valid 0.09998461128746593\n",
      "Epoch 675:\n",
      "Loss train 0.21678569216653704 valid 0.13618546785837068\n",
      "Epoch 676:\n",
      "Loss train 0.20261821286939086 valid 0.44587775610898844\n",
      "Epoch 677:\n",
      "Loss train 0.20928039930090309 valid 0.10115790162315662\n",
      "Epoch 678:\n",
      "Loss train 0.1975476662710309 valid 0.30864126542104037\n",
      "Epoch 679:\n",
      "Loss train 0.20353674732334912 valid 3.051895471210841\n",
      "Epoch 680:\n",
      "Loss train 0.22009102910235523 valid 0.04053718709574148\n",
      "Epoch 681:\n",
      "Loss train 0.1913167562931776 valid 0.03278996596306471\n",
      "Epoch 682:\n",
      "Loss train 0.2081474570631981 valid 0.3545243708313694\n",
      "Epoch 683:\n",
      "Loss train 0.20790572014898062 valid 0.049345877187180406\n",
      "Epoch 684:\n",
      "Loss train 0.19854987724646925 valid 0.058847235023470154\n",
      "Epoch 685:\n",
      "Loss train 0.20490033995732665 valid 0.034766844507891324\n",
      "Epoch 686:\n",
      "Loss train 0.21145163093544542 valid 1.2042478249059643\n",
      "Epoch 687:\n",
      "Loss train 0.20230314662940801 valid 0.025895941977534732\n",
      "Epoch 688:\n",
      "Loss train 0.20640118413046002 valid 0.03615977988442828\n",
      "Epoch 689:\n",
      "Loss train 0.2017682983417064 valid 0.031161660337785335\n",
      "Epoch 690:\n",
      "Loss train 0.2053442744895816 valid 0.41743369924474294\n",
      "Epoch 691:\n",
      "Loss train 0.19465449207909405 valid 0.0377036300006816\n",
      "Epoch 692:\n",
      "Loss train 0.1983291411407292 valid 0.4911826231856294\n",
      "Epoch 693:\n",
      "Loss train 0.19901913528740406 valid 0.15350724760859308\n",
      "Epoch 694:\n",
      "Loss train 0.20065627285838128 valid 0.10468848676151302\n",
      "Epoch 695:\n",
      "Loss train 0.2212505591608584 valid 0.029856042656858314\n",
      "Epoch 696:\n",
      "Loss train 0.18581214275024832 valid 0.05677168134238354\n",
      "Epoch 697:\n",
      "Loss train 0.1996606510914862 valid 0.040637449157859516\n",
      "Epoch 698:\n",
      "Loss train 0.23752460499312728 valid 0.037467127546070726\n",
      "Epoch 699:\n",
      "Loss train 0.2053204056687653 valid 6.742879011971995\n",
      "Epoch 700:\n",
      "Loss train 0.18006875243224205 valid 0.12454556650612712\n",
      "Epoch 701:\n",
      "Loss train 0.1990421772075817 valid 0.21392350626448933\n",
      "Epoch 702:\n",
      "Loss train 0.19779568955935536 valid 0.032089208328813165\n",
      "Epoch 703:\n",
      "Loss train 0.2233679037157446 valid 0.025370393670832547\n",
      "Epoch 704:\n",
      "Loss train 0.1827842508148402 valid 0.5537585818751452\n",
      "Epoch 705:\n",
      "Loss train 0.19317911602146923 valid 0.07707738641338714\n",
      "Epoch 706:\n",
      "Loss train 0.1953225240457803 valid 0.18470348955870586\n",
      "Epoch 707:\n",
      "Loss train 0.19892339395321906 valid 0.5271358980588114\n",
      "Epoch 708:\n",
      "Loss train 0.20794331911578776 valid 0.47347228680236675\n",
      "Epoch 709:\n",
      "Loss train 0.19014305660203099 valid 0.2512322073762677\n",
      "Epoch 710:\n",
      "Loss train 0.20824054449088872 valid 0.3718134086061952\n",
      "Epoch 711:\n",
      "Loss train 0.19319413328133522 valid 0.02297758493907961\n",
      "Epoch 712:\n",
      "Loss train 0.20301640175394714 valid 0.03989902952232313\n",
      "Epoch 713:\n",
      "Loss train 0.2104428894562647 valid 0.03944386768036687\n",
      "Epoch 714:\n",
      "Loss train 0.1963750505849719 valid 0.09313992461452948\n",
      "Epoch 715:\n",
      "Loss train 0.18812956368923187 valid 0.030424293074775018\n",
      "Epoch 716:\n",
      "Loss train 0.21506165888272225 valid 0.14047457307971453\n",
      "Epoch 717:\n",
      "Loss train 0.18837890526764095 valid 0.14335751175288833\n",
      "Epoch 718:\n",
      "Loss train 0.1872179182773456 valid 0.02618489245965951\n",
      "Epoch 719:\n",
      "Loss train 0.20233925208821893 valid 0.03514104707443187\n",
      "Epoch 720:\n",
      "Loss train 0.1990202368915081 valid 0.09636207658950724\n",
      "Epoch 721:\n",
      "Loss train 0.19500890905559062 valid 0.0384805276073508\n",
      "Epoch 722:\n",
      "Loss train 0.2022707979079336 valid 0.04817234705074776\n",
      "Epoch 723:\n",
      "Loss train 0.1872847441583872 valid 0.1383256654983784\n",
      "Epoch 724:\n",
      "Loss train 0.1992264026824385 valid 0.040722128496921285\n",
      "Epoch 725:\n",
      "Loss train 0.20154759858585894 valid 0.02774605234449979\n",
      "Epoch 726:\n",
      "Loss train 0.1774016940202564 valid 0.03944532203574479\n",
      "Epoch 727:\n",
      "Loss train 0.21143405514508487 valid 0.024334010680290754\n",
      "Epoch 728:\n",
      "Loss train 0.18449014518857001 valid 0.2198122496168235\n",
      "Epoch 729:\n",
      "Loss train 0.21871944108903407 valid 0.02259252116553081\n",
      "Epoch 730:\n",
      "Loss train 0.17509748506173492 valid 0.5735840239494643\n",
      "Epoch 731:\n",
      "Loss train 0.1974851300597191 valid 0.06836239778451245\n",
      "Epoch 732:\n",
      "Loss train 0.20317644547633826 valid 0.21095229275066588\n",
      "Epoch 733:\n",
      "Loss train 0.19250695787221192 valid 0.27164967651428235\n",
      "Epoch 734:\n",
      "Loss train 0.18828568552173675 valid 0.0650430063350856\n",
      "Epoch 735:\n",
      "Loss train 0.2069661007564515 valid 0.028150298962803542\n",
      "Epoch 736:\n",
      "Loss train 0.18586169016584753 valid 0.5738618831942114\n",
      "Epoch 737:\n",
      "Loss train 0.20599781826101243 valid 0.07803678296761156\n",
      "Epoch 738:\n",
      "Loss train 0.19503684468865395 valid 0.03120737175178496\n",
      "Epoch 739:\n",
      "Loss train 0.1993965014776215 valid 0.05117076745449178\n",
      "Epoch 740:\n",
      "Loss train 0.1918396413754672 valid 0.0318998596322155\n",
      "Epoch 741:\n",
      "Loss train 0.19161984221283346 valid 0.19437192023470315\n",
      "Epoch 742:\n",
      "Loss train 0.18744507352337242 valid 0.9111546662343148\n",
      "Epoch 743:\n",
      "Loss train 0.20461226298213006 valid 0.02637037463697506\n",
      "Epoch 744:\n",
      "Loss train 0.20024673723708838 valid 0.03961886769288715\n",
      "Epoch 745:\n",
      "Loss train 0.18804956691749394 valid 0.03655528389915753\n",
      "Epoch 746:\n",
      "Loss train 0.1927126812234521 valid 0.13823552895929614\n",
      "Epoch 747:\n",
      "Loss train 0.21702844563499094 valid 0.040035805694725576\n",
      "Epoch 748:\n",
      "Loss train 0.18670948579087854 valid 0.03120994160785226\n",
      "Epoch 749:\n",
      "Loss train 0.18826334936711936 valid 0.15840048089508077\n",
      "Epoch 750:\n",
      "Loss train 0.19148492973744868 valid 0.3560588570993494\n",
      "Epoch 751:\n",
      "Loss train 0.18900186831839383 valid 0.094484400066319\n",
      "Epoch 752:\n",
      "Loss train 0.19935533487685025 valid 0.20342403020674724\n",
      "Epoch 753:\n",
      "Loss train 0.18865045165922492 valid 0.021271746798803722\n",
      "Epoch 754:\n",
      "Loss train 0.200926372901164 valid 0.5414583404305336\n",
      "Epoch 755:\n",
      "Loss train 0.19397468937449158 valid 0.024430381676128217\n",
      "Epoch 756:\n",
      "Loss train 0.20478708807267249 valid 0.02838523137739622\n",
      "Epoch 757:\n",
      "Loss train 0.18920467077828942 valid 0.04420607126290284\n",
      "Epoch 758:\n",
      "Loss train 0.1948874677889049 valid 0.05000301572611976\n",
      "Epoch 759:\n",
      "Loss train 0.19156615769863128 valid 0.09592900243410765\n",
      "Epoch 760:\n",
      "Loss train 0.2002763424757868 valid 0.6069769212556303\n",
      "Epoch 761:\n",
      "Loss train 0.18039620234183967 valid 0.049037427337247745\n",
      "Epoch 762:\n",
      "Loss train 0.20867328750267625 valid 0.05494404365278777\n",
      "Epoch 763:\n",
      "Loss train 0.20097585207372903 valid 0.04937150653601744\n",
      "Epoch 764:\n",
      "Loss train 0.18861195887252688 valid 4.110800811453084\n",
      "Epoch 765:\n",
      "Loss train 0.18024139051660895 valid 0.20747701609984942\n",
      "Epoch 766:\n",
      "Loss train 0.19967834983151406 valid 0.022684716332178934\n",
      "Epoch 767:\n",
      "Loss train 0.19130123590957374 valid 1.3193011310755158\n",
      "Epoch 768:\n",
      "Loss train 0.1757543268075213 valid 0.049527397051230194\n",
      "Epoch 769:\n",
      "Loss train 0.19144303652830422 valid 0.025699437146876083\n",
      "Epoch 770:\n",
      "Loss train 0.1823777949243784 valid 0.21954806133240293\n",
      "Epoch 771:\n",
      "Loss train 0.18351410906966775 valid 0.8299137656076739\n",
      "Epoch 772:\n",
      "Loss train 0.18396333495005965 valid 0.11015776067935167\n",
      "Epoch 773:\n",
      "Loss train 0.204292416879721 valid 0.02862151426974954\n",
      "Epoch 774:\n",
      "Loss train 0.20477979082856326 valid 0.02223059933975211\n",
      "Epoch 775:\n",
      "Loss train 0.17817269979119302 valid 0.055890938892336324\n",
      "Epoch 776:\n",
      "Loss train 0.21641276800688355 valid 0.032587970152517014\n",
      "Epoch 777:\n",
      "Loss train 0.16553076894506813 valid 0.047745348763836196\n",
      "Epoch 778:\n",
      "Loss train 0.18370015754625202 valid 0.044569077729321824\n",
      "Epoch 779:\n",
      "Loss train 0.18629839252717792 valid 0.10957246446129601\n",
      "Epoch 780:\n",
      "Loss train 0.20127499608024954 valid 0.10151910089929606\n",
      "Epoch 781:\n",
      "Loss train 0.17895105332843958 valid 0.09556623192914139\n",
      "Epoch 782:\n",
      "Loss train 0.21071491420455277 valid 0.03851573169585463\n",
      "Epoch 783:\n",
      "Loss train 0.17633301429133863 valid 0.039171943008761166\n",
      "Epoch 784:\n",
      "Loss train 0.1843263733148575 valid 0.18024439941301557\n",
      "Epoch 785:\n",
      "Loss train 0.19823504552077503 valid 0.04913628004489698\n",
      "Epoch 786:\n",
      "Loss train 0.19396345545463264 valid 0.04561238018989909\n",
      "Epoch 787:\n",
      "Loss train 0.19142933843620122 valid 0.08846659546781496\n",
      "Epoch 788:\n",
      "Loss train 0.17325297653265298 valid 0.03499215859616899\n",
      "Epoch 789:\n",
      "Loss train 0.196625617752783 valid 0.06001368758531477\n",
      "Epoch 790:\n",
      "Loss train 0.18258895044960083 valid 0.06452289452603543\n",
      "Epoch 791:\n",
      "Loss train 0.18547332630641758 valid 0.025459329376408085\n",
      "Epoch 792:\n",
      "Loss train 0.186515650241822 valid 0.020079105227497382\n",
      "Epoch 793:\n",
      "Loss train 0.18918328602071852 valid 0.04090701810868285\n",
      "Epoch 794:\n",
      "Loss train 0.18566332980170847 valid 0.186150954595648\n",
      "Epoch 795:\n",
      "Loss train 0.20537373785246163 valid 0.02590329437649671\n",
      "Epoch 796:\n",
      "Loss train 0.185001628466323 valid 0.027958947639728428\n",
      "Epoch 797:\n",
      "Loss train 0.2016255921330303 valid 0.017825490432543317\n",
      "Epoch 798:\n",
      "Loss train 0.17345038745310157 valid 0.03863045335390705\n",
      "Epoch 799:\n",
      "Loss train 0.19461220658905803 valid 0.07141026798745544\n",
      "Epoch 800:\n",
      "Loss train 0.17933577626235783 valid 0.11735045212625543\n",
      "Epoch 801:\n",
      "Loss train 0.1918801300484687 valid 0.4145893751254404\n",
      "Epoch 802:\n",
      "Loss train 0.17593476660475135 valid 0.08390206133422912\n",
      "Epoch 803:\n",
      "Loss train 0.1810280298454687 valid 0.1264664062571671\n",
      "Epoch 804:\n",
      "Loss train 0.1767725758789107 valid 0.12291554565579592\n",
      "Epoch 805:\n",
      "Loss train 0.19401032723784448 valid 0.47215525423302307\n",
      "Epoch 806:\n",
      "Loss train 0.1827253982156515 valid 0.025933846671337784\n",
      "Epoch 807:\n",
      "Loss train 0.1882967349819839 valid 0.06832735345568737\n",
      "Epoch 808:\n",
      "Loss train 0.1872337473200634 valid 0.2837968464189874\n",
      "Epoch 809:\n",
      "Loss train 0.17903569172825665 valid 0.6912400064668691\n",
      "Epoch 810:\n",
      "Loss train 0.18020672261677684 valid 0.07520851275224132\n",
      "Epoch 811:\n",
      "Loss train 0.1816513132346794 valid 0.03559513691496379\n",
      "Epoch 812:\n",
      "Loss train 0.1781893351720646 valid 0.3646179301953633\n",
      "Epoch 813:\n",
      "Loss train 0.20811253309175373 valid 0.022702903742822963\n",
      "Epoch 814:\n",
      "Loss train 0.1613239523436874 valid 0.029999219704751515\n",
      "Epoch 815:\n",
      "Loss train 0.17796495422273875 valid 0.17201762419221642\n",
      "Epoch 816:\n",
      "Loss train 0.18789960754401983 valid 0.5293055154215044\n",
      "Epoch 817:\n",
      "Loss train 0.20128863890208304 valid 0.4928907357254249\n",
      "Epoch 818:\n",
      "Loss train 0.18317396698761731 valid 0.12942364962806588\n",
      "Epoch 819:\n",
      "Loss train 0.1862122054014355 valid 0.13322926681086042\n",
      "Epoch 820:\n",
      "Loss train 0.18114926027785988 valid 0.026431412936650057\n",
      "Epoch 821:\n",
      "Loss train 0.18200378815308213 valid 0.04388000467815848\n",
      "Epoch 822:\n",
      "Loss train 0.18524659626297652 valid 0.023317876273533752\n",
      "Epoch 823:\n",
      "Loss train 0.1771964717734605 valid 0.21322547445997395\n",
      "Epoch 824:\n",
      "Loss train 0.18703703767824917 valid 0.028323371019168143\n",
      "Epoch 825:\n",
      "Loss train 0.17975319857914002 valid 0.04723511521024207\n",
      "Epoch 826:\n",
      "Loss train 0.18448070241808892 valid 0.33268274415269006\n",
      "Epoch 827:\n",
      "Loss train 0.1740329179879278 valid 0.08995433494713344\n",
      "Epoch 828:\n",
      "Loss train 0.18054890632051976 valid 0.022666667103411146\n",
      "Epoch 829:\n",
      "Loss train 0.17342167163509875 valid 0.05579203681677809\n",
      "Epoch 830:\n",
      "Loss train 0.17533631955944 valid 0.1117066764257531\n",
      "Epoch 831:\n",
      "Loss train 0.1807813369853422 valid 0.06399808287450873\n",
      "Epoch 832:\n",
      "Loss train 0.1739386832240969 valid 0.5561884774635647\n",
      "Epoch 833:\n",
      "Loss train 0.18698271626811475 valid 0.5747348941401832\n",
      "Epoch 834:\n",
      "Loss train 0.2037436635805294 valid 0.03132656651967204\n",
      "Epoch 835:\n",
      "Loss train 0.16956098663378508 valid 0.054427609403026565\n",
      "Epoch 836:\n",
      "Loss train 0.1765670436091721 valid 0.05475599482155751\n",
      "Epoch 837:\n",
      "Loss train 0.18272858699113131 valid 0.12402027406720659\n",
      "Epoch 838:\n",
      "Loss train 0.18316749029066415 valid 0.06555815441049938\n",
      "Epoch 839:\n",
      "Loss train 0.1851417524356395 valid 0.0667731241385182\n",
      "Epoch 840:\n",
      "Loss train 0.17246083630193024 valid 0.1172913175787295\n",
      "Epoch 841:\n",
      "Loss train 0.1810548700781539 valid 0.028088988377327432\n",
      "Epoch 842:\n",
      "Loss train 0.19028630401734262 valid 0.07239632805748199\n",
      "Epoch 843:\n",
      "Loss train 0.16180919438824057 valid 0.06674032409850497\n",
      "Epoch 844:\n",
      "Loss train 0.18153043086975812 valid 0.0536890699816439\n",
      "Epoch 845:\n",
      "Loss train 0.16941857763733714 valid 0.22287921978472816\n",
      "Epoch 846:\n",
      "Loss train 0.18782034754827617 valid 3.2513861582772563\n",
      "Epoch 847:\n",
      "Loss train 0.1860570357227698 valid 0.06977910481042915\n",
      "Epoch 848:\n",
      "Loss train 0.1768097612319514 valid 0.34038124204199643\n",
      "Epoch 849:\n",
      "Loss train 0.172126008255966 valid 0.07916956058839118\n",
      "Epoch 850:\n",
      "Loss train 0.1789507007393986 valid 0.037156779910934246\n",
      "Epoch 851:\n",
      "Loss train 0.1799452857332304 valid 0.041838953683729294\n",
      "Epoch 852:\n",
      "Loss train 0.17687888904958962 valid 0.11282526632417729\n",
      "Epoch 853:\n",
      "Loss train 0.1834550074500963 valid 0.08073359018869482\n",
      "Epoch 854:\n",
      "Loss train 0.17864432454723864 valid 0.028463845747626764\n",
      "Epoch 855:\n",
      "Loss train 0.18522293291185052 valid 0.02120903893338573\n",
      "Epoch 856:\n",
      "Loss train 0.18640688357334584 valid 0.0414284086333642\n",
      "Epoch 857:\n",
      "Loss train 0.1724668457195163 valid 0.1161234671329394\n",
      "Epoch 858:\n",
      "Loss train 0.17654230614453553 valid 0.049239935384822166\n",
      "Epoch 859:\n",
      "Loss train 0.18115182124450802 valid 0.03563276783120976\n",
      "Epoch 860:\n",
      "Loss train 0.17496652128957213 valid 0.03882443337770177\n",
      "Epoch 861:\n",
      "Loss train 0.18806788917612285 valid 0.035230947601631105\n",
      "Epoch 862:\n",
      "Loss train 0.1773327292151749 valid 0.04331920868435355\n",
      "Epoch 863:\n",
      "Loss train 0.17434484054259955 valid 0.021772041897535663\n",
      "Epoch 864:\n",
      "Loss train 0.17993016780484467 valid 0.024725113311018284\n",
      "Epoch 865:\n",
      "Loss train 0.1700619090259075 valid 0.6005551000606302\n",
      "Epoch 866:\n",
      "Loss train 0.17945166911352425 valid 0.05613331563547979\n",
      "Epoch 867:\n",
      "Loss train 0.17552795818187297 valid 0.06900052888302866\n",
      "Epoch 868:\n",
      "Loss train 0.1722542208738625 valid 0.035110359190470515\n",
      "Epoch 869:\n",
      "Loss train 0.19762640182189645 valid 0.4661331467110632\n",
      "Epoch 870:\n",
      "Loss train 0.16821009750366211 valid 0.0710041612546853\n",
      "Epoch 871:\n",
      "Loss train 0.18501483262274415 valid 0.5043374642813501\n",
      "Epoch 872:\n",
      "Loss train 0.18272522015087306 valid 0.026755384831494717\n",
      "Epoch 873:\n",
      "Loss train 0.19799344455692916 valid 0.18044294254293083\n",
      "Epoch 874:\n",
      "Loss train 0.16842754955627023 valid 0.20391634859754998\n",
      "Epoch 875:\n",
      "Loss train 0.1774637941641733 valid 0.6846044785848298\n",
      "Epoch 876:\n",
      "Loss train 0.16932235343325883 valid 0.03829922970439268\n",
      "Epoch 877:\n",
      "Loss train 0.18438568183425813 valid 0.08265824092325677\n",
      "Epoch 878:\n",
      "Loss train 0.1927028892416507 valid 0.03108377062400011\n",
      "Epoch 879:\n",
      "Loss train 0.19223584740422667 valid 0.533820678942741\n",
      "Epoch 880:\n",
      "Loss train 0.1898049732990563 valid 0.2884660166429093\n",
      "Epoch 881:\n",
      "Loss train 0.17038233367893846 valid 0.017241709923059963\n",
      "Epoch 882:\n",
      "Loss train 0.18735380959417672 valid 0.08610636833510717\n",
      "Epoch 883:\n",
      "Loss train 0.17955179237090052 valid 0.034366425262646105\n",
      "Epoch 884:\n",
      "Loss train 0.1684390414448455 valid 2.3575404093325236\n",
      "Epoch 885:\n",
      "Loss train 0.18860025683697312 valid 0.7444483206709314\n",
      "Epoch 886:\n",
      "Loss train 0.17184523348864167 valid 0.050336066866785094\n",
      "Epoch 887:\n",
      "Loss train 0.16929834397248925 valid 0.04693037862670732\n",
      "Epoch 888:\n",
      "Loss train 0.17762694122679532 valid 0.0931226471579867\n",
      "Epoch 889:\n",
      "Loss train 0.17310546676497907 valid 0.024672613835845476\n",
      "Epoch 890:\n",
      "Loss train 0.17504077338222415 valid 0.02573955561896636\n",
      "Epoch 891:\n",
      "Loss train 0.1716260477403179 valid 0.048708764796674846\n",
      "Epoch 892:\n",
      "Loss train 0.16281532911770047 valid 0.021621147459682098\n",
      "Epoch 893:\n",
      "Loss train 0.1677863207399845 valid 0.02247840520832595\n",
      "Epoch 894:\n",
      "Loss train 0.18432237849123775 valid 0.03514988792118016\n",
      "Epoch 895:\n",
      "Loss train 0.1762217441752553 valid 0.02582284246802143\n",
      "Epoch 896:\n",
      "Loss train 0.17688546362090857 valid 0.03863724923300988\n",
      "Epoch 897:\n",
      "Loss train 0.16532491634786128 valid 0.04648028231787841\n",
      "Epoch 898:\n",
      "Loss train 0.18574395769331603 valid 0.16778006881549576\n",
      "Epoch 899:\n",
      "Loss train 0.16619339942727238 valid 0.03654950018844679\n",
      "Epoch 900:\n",
      "Loss train 0.16873119108341633 valid 0.06778841867529735\n",
      "Epoch 901:\n",
      "Loss train 0.1633022014474496 valid 0.03842353790119199\n",
      "Epoch 902:\n",
      "Loss train 0.17299239125270396 valid 0.11382058887651854\n",
      "Epoch 903:\n",
      "Loss train 0.18131444498766214 valid 0.08550616468936731\n",
      "Epoch 904:\n",
      "Loss train 0.1691881571687758 valid 0.8455042234231542\n",
      "Epoch 905:\n",
      "Loss train 0.19170257618967443 valid 0.027142886112065445\n",
      "Epoch 906:\n",
      "Loss train 0.16925174323599784 valid 0.03173345949670214\n",
      "Epoch 907:\n",
      "Loss train 0.1818809893136844 valid 0.01771281860537059\n",
      "Epoch 908:\n",
      "Loss train 0.17774310005679728 valid 0.030092875172838884\n",
      "Epoch 909:\n",
      "Loss train 0.16807348268646746 valid 0.028842966294254525\n",
      "Epoch 910:\n",
      "Loss train 0.16712923911642283 valid 0.040147905526470316\n",
      "Epoch 911:\n",
      "Loss train 0.18219153181742878 valid 0.039120390301185186\n",
      "Epoch 912:\n",
      "Loss train 0.1671403150547296 valid 0.05317569164528664\n",
      "Epoch 913:\n",
      "Loss train 0.173701530748792 valid 0.16958566909052925\n",
      "Epoch 914:\n",
      "Loss train 0.1855191934959963 valid 0.03753759841644726\n",
      "Epoch 915:\n",
      "Loss train 0.1658202474957332 valid 0.030485980418769553\n",
      "Epoch 916:\n",
      "Loss train 0.17571960346139967 valid 0.023836433003220532\n",
      "Epoch 917:\n",
      "Loss train 0.16564287902750074 valid 0.023635462349629776\n",
      "Epoch 918:\n",
      "Loss train 0.18118219711724667 valid 2.4622539103556385\n",
      "Epoch 919:\n",
      "Loss train 0.16618452100474387 valid 0.02167017890433376\n",
      "Epoch 920:\n",
      "Loss train 0.16869137274120002 valid 0.22408613102807634\n",
      "Epoch 921:\n",
      "Loss train 0.16823219303768128 valid 0.45507788429778323\n",
      "Epoch 922:\n",
      "Loss train 0.1678338178049773 valid 0.02509479830862076\n",
      "Epoch 923:\n",
      "Loss train 0.17234096087999642 valid 0.24894610251489252\n",
      "Epoch 924:\n",
      "Loss train 0.16428949468266218 valid 0.4429760577172672\n",
      "Epoch 925:\n",
      "Loss train 0.19162394498251378 valid 0.019012154400851803\n",
      "Epoch 926:\n",
      "Loss train 0.15702633335664867 valid 0.20406494541041656\n",
      "Epoch 927:\n",
      "Loss train 0.18078697338029742 valid 0.020093874666025233\n",
      "Epoch 928:\n",
      "Loss train 0.15497751570530235 valid 0.3759948095611884\n",
      "Epoch 929:\n",
      "Loss train 0.17108356422111393 valid 0.05955304047582364\n",
      "Epoch 930:\n",
      "Loss train 0.17563627162836493 valid 0.07293808047476522\n",
      "Epoch 931:\n",
      "Loss train 0.16647466802727431 valid 0.03422353718160289\n",
      "Epoch 932:\n",
      "Loss train 0.1677431312661618 valid 0.044701644884046945\n",
      "Epoch 933:\n",
      "Loss train 0.17260821444559843 valid 0.019527183911403574\n",
      "Epoch 934:\n",
      "Loss train 0.1667437383910641 valid 0.27830569123008775\n",
      "Epoch 935:\n",
      "Loss train 0.18771461265571415 valid 0.06217082082131735\n",
      "Epoch 936:\n",
      "Loss train 0.1662532224951312 valid 0.12909733138571017\n",
      "Epoch 937:\n",
      "Loss train 0.16601032823827117 valid 0.04075805442420277\n",
      "Epoch 938:\n",
      "Loss train 0.17200356621220708 valid 0.01871896367845219\n",
      "Epoch 939:\n",
      "Loss train 0.15854665926210582 valid 0.15375223358363804\n",
      "Epoch 940:\n",
      "Loss train 0.16384592698980122 valid 0.13820605671569383\n",
      "Epoch 941:\n",
      "Loss train 0.1706407233837992 valid 0.019153867400875327\n",
      "Epoch 942:\n",
      "Loss train 0.16107726228032263 valid 0.11055326532606553\n",
      "Epoch 943:\n",
      "Loss train 0.17317499339580536 valid 0.15774648507732325\n",
      "Epoch 944:\n",
      "Loss train 0.17137962615191937 valid 0.10944246867915781\n",
      "Epoch 945:\n",
      "Loss train 0.18279669746011495 valid 0.0333713129944018\n",
      "Epoch 946:\n",
      "Loss train 0.16263790355846286 valid 1.7188432623470304\n",
      "Epoch 947:\n",
      "Loss train 0.17231589259598404 valid 2.1064142960235586\n",
      "Epoch 948:\n",
      "Loss train 0.16744820524565876 valid 0.03121788312911061\n",
      "Epoch 949:\n",
      "Loss train 0.16802311196718364 valid 0.2207297107192967\n",
      "Epoch 950:\n",
      "Loss train 0.1627567104667425 valid 0.02591677592448368\n",
      "Epoch 951:\n",
      "Loss train 0.1673848047675565 valid 0.024084786518260348\n",
      "Epoch 952:\n",
      "Loss train 0.17695866648573427 valid 0.06302180046339796\n",
      "Epoch 953:\n",
      "Loss train 0.17356998110134156 valid 0.04069758353352725\n",
      "Epoch 954:\n",
      "Loss train 0.1912163943592459 valid 0.027955626847709497\n",
      "Epoch 955:\n",
      "Loss train 0.15432668569479138 valid 0.02465870348108139\n",
      "Epoch 956:\n",
      "Loss train 0.1775641029091552 valid 0.7409357447334401\n",
      "Epoch 957:\n",
      "Loss train 0.15526880715657027 valid 0.1464802777012567\n",
      "Epoch 958:\n",
      "Loss train 0.17366126944515853 valid 0.09254735793784236\n",
      "Epoch 959:\n",
      "Loss train 0.18203204303253442 valid 1.0902764825198021\n",
      "Epoch 960:\n",
      "Loss train 0.17655164629872888 valid 0.03394269163462371\n",
      "Epoch 961:\n",
      "Loss train 0.1581042374946177 valid 0.05846533961440706\n",
      "Epoch 962:\n",
      "Loss train 0.16226155786011368 valid 0.16869680567627915\n",
      "Epoch 963:\n",
      "Loss train 0.1764861742535606 valid 0.03222283430714923\n",
      "Epoch 964:\n",
      "Loss train 0.1547467914581299 valid 0.07094407895395301\n",
      "Epoch 965:\n",
      "Loss train 0.17564930653423072 valid 0.03099374736994699\n",
      "Epoch 966:\n",
      "Loss train 0.16440820318572222 valid 0.17782002715714687\n",
      "Epoch 967:\n",
      "Loss train 0.15898268316816538 valid 0.08926199067195091\n",
      "Epoch 968:\n",
      "Loss train 0.1686484221270308 valid 0.11029853295896248\n",
      "Epoch 969:\n",
      "Loss train 0.1539212163472548 valid 0.09616168543802193\n",
      "Epoch 970:\n",
      "Loss train 0.16665995872057973 valid 0.0858159766452164\n",
      "Epoch 971:\n",
      "Loss train 0.17267948820572346 valid 0.033281315820940724\n",
      "Epoch 972:\n",
      "Loss train 0.15913773362860084 valid 0.03407370304608931\n",
      "Epoch 973:\n",
      "Loss train 0.17138249560464175 valid 0.4990480496487821\n",
      "Epoch 974:\n",
      "Loss train 0.16799384986367077 valid 0.05910124961845847\n",
      "Epoch 975:\n",
      "Loss train 0.16196731223929672 valid 0.038514397046557\n",
      "Epoch 976:\n",
      "Loss train 0.16658557111918926 valid 0.033138732341388956\n",
      "Epoch 977:\n",
      "Loss train 0.16758134554680437 valid 0.021615180704389544\n",
      "Epoch 978:\n",
      "Loss train 0.18422271278183908 valid 0.01912125365437183\n",
      "Epoch 979:\n",
      "Loss train 0.15852983926311134 valid 0.03818256098218772\n",
      "Epoch 980:\n",
      "Loss train 0.16210632182713597 valid 0.04146510162545816\n",
      "Epoch 981:\n",
      "Loss train 0.18581840192656965 valid 0.028883431881166266\n",
      "Epoch 982:\n",
      "Loss train 0.16629662055242805 valid 0.04855279853613968\n",
      "Epoch 983:\n",
      "Loss train 0.15422644227091223 valid 0.021848955425447614\n",
      "Epoch 984:\n",
      "Loss train 0.16370104867834598 valid 0.04071754197957444\n",
      "Epoch 985:\n",
      "Loss train 0.16560337258595972 valid 0.0984176383669595\n",
      "Epoch 986:\n",
      "Loss train 0.1611570833751932 valid 0.0855465530950012\n",
      "Epoch 987:\n",
      "Loss train 0.1631449706375599 valid 0.08485401193262852\n",
      "Epoch 988:\n",
      "Loss train 0.1787249537870288 valid 0.026312771285800708\n",
      "Epoch 989:\n",
      "Loss train 0.14835860472656787 valid 0.06910005686356833\n",
      "Epoch 990:\n",
      "Loss train 0.1774087253704667 valid 0.03742609750240713\n",
      "Epoch 991:\n",
      "Loss train 0.1612123584719375 valid 0.9451229417529965\n",
      "Epoch 992:\n",
      "Loss train 0.17763874096088111 valid 0.018973884910758285\n",
      "Epoch 993:\n",
      "Loss train 0.16982843615245075 valid 0.03668408765288832\n",
      "Epoch 994:\n",
      "Loss train 0.16419657049607486 valid 0.05798639184551672\n",
      "Epoch 995:\n",
      "Loss train 0.15677487897034734 valid 0.024634792624575134\n",
      "Epoch 996:\n",
      "Loss train 0.16203609336204827 valid 0.03512687568114171\n",
      "Epoch 997:\n",
      "Loss train 0.16463954126909375 valid 0.027211418927506625\n",
      "Epoch 998:\n",
      "Loss train 0.16627948591914027 valid 0.07847634167836705\n",
      "Epoch 999:\n",
      "Loss train 0.168872437473014 valid 0.020378244020951394\n",
      "Epoch 1000:\n",
      "Loss train 0.1576411345951259 valid 0.03005881125681568\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "learning_rate = 1e-3\n",
    "n_epochs = 1000\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {}:'.format(epoch + 1))\n",
    "\n",
    "    model.train()\n",
    "    avg_loss = train_one_epoch()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_outputs = torch.squeeze(model(torch.from_numpy(valid_features_normalized.astype(\"float32\"))))\n",
    "        valid_loss = loss_function(valid_outputs, torch.from_numpy(valid_fco2).to(torch.device(\"cuda\"))).detach().cpu().item()\n",
    "\n",
    "    print('Loss train {} valid {}'.format(avg_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "266af035-331f-4298-9df1-6a945273024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some tests of model performance\n",
      "Index:  2486469\n",
      "42.382211071672096\n",
      "tensor([42.3715], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  889830\n",
      "1006.2861357301771\n",
      "tensor([1006.3102], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  1523244\n",
      "503.3432186052197\n",
      "tensor([503.3603], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  4919512\n",
      "784.3194463700629\n",
      "tensor([784.4268], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  772012\n",
      "321.1514859455472\n",
      "tensor([321.2376], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  42812\n",
      "1363.8978478486451\n",
      "tensor([1364.0494], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  4554809\n",
      "349.59341067823567\n",
      "tensor([349.6873], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  4297866\n",
      "1577.4074128192042\n",
      "tensor([1577.5208], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  1591483\n",
      "45.524201800380936\n",
      "tensor([45.5593], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  3986147\n",
      "908.4829140524911\n",
      "tensor([908.6677], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "print(\"Some tests of model performance\")\n",
    "random_indices = np.random.randint(ntrain, size=10, dtype=int)\n",
    "for ind in random_indices:\n",
    "    print(\"Index: \", ind)\n",
    "    print(train_fco2[ind])\n",
    "    print(model(torch.from_numpy(train_features_normalized[ind, :].astype(\"float32\"))))\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30a78c67-fe00-43cb-83ca-49c20140fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"64x3_elu_1000epo.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d6fb27d-b958-4d69-90a2-f04edc638037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(x, y):\n",
    "    return np.sum((x-y)**2)/len(x)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    model_output_after_training = model(torch.from_numpy(valid_features_normalized.astype(\"float32\"))).detach().cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2fa4cc0c-bdcf-4702-bb6d-a354635e505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (for consistency check):  0.03005881125681567\n",
      "RMSE:  0.1733747711081854\n",
      "Maximum absolute deviation:  5.621064177502376\n",
      "99.9th percentile of absolute deviation (1000 val's larger):  0.9622477764905215\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE (for consistency check): \", MSE(valid_fco2, model_output_after_training))\n",
    "print(\"RMSE: \", np.sqrt(MSE(valid_fco2, model_output_after_training)))\n",
    "print(\"Maximum absolute deviation: \", np.max(np.abs(model_output_after_training-valid_fco2)))\n",
    "print(\"99.9th percentile of absolute deviation (1000 val's larger): \", np.percentile(np.abs(model_output_after_training-valid_fco2), q=99.9))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
