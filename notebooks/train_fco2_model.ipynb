{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f1e5dd-99a2-4065-ae58-8b28bdaa523a",
   "metadata": {},
   "source": [
    "#### Load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27b1bb0a-93ee-43fe-a22a-26fe0c830211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0,'../../mocsy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f974531b-e417-49ff-9c59-d45f4e31a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mocsy\n",
    "from mocsy import mvars\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c27fdcfc-6816-4aa0-88f6-75e5a80d8a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2696a537-4cc5-4088-83da-e853a259c478",
   "metadata": {},
   "source": [
    "#### Calculate fCO2 training data with mocsy routine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce5c4036-fa74-4a51-9ed4-c0f8fde59940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fCO2(alk, dic, tem, sal, sil, phos):\n",
    "    # input units\n",
    "    # alk in mol / kg\n",
    "    # dic in mol / kg\n",
    "    # tem in °C\n",
    "    # sal in PSU\n",
    "    # sil in mol / kg\n",
    "    # phos in mol / kg\n",
    "    n = len(alk)\n",
    "    return mvars(alk=alk,\n",
    "                     dic=dic,\n",
    "                     temp=tem,\n",
    "                     sal=sal,\n",
    "                     sil=sil,\n",
    "                     phos=phos,\n",
    "                     patm=tuple(1 for _ in range(n)),\n",
    "                     depth=tuple(5 for _ in range(n)),\n",
    "                     lat=tuple(np.nan for _ in range(n)),\n",
    "                     optcon='mol/kg',\n",
    "                     optt='Tpot',\n",
    "                     optp='db',\n",
    "                     optk1k2='l',\n",
    "                     optb='u74',\n",
    "                     optkf='pf',\n",
    "                     opts='Sprc')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "329f9dc6-8f17-41dc-9ba0-29cbcdd5c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "samples = []\n",
    "sample_size = 6000_000\n",
    "for i in range(sample_size):\n",
    "    # alk between 1700e-6 and 2700e-6 mol kg-1\n",
    "    alk = np.random.uniform(low=1700e-6, high=2700e-6)\n",
    "    # dic between 1700e-6 mol kg-1 and alk\n",
    "    dic = np.random.uniform(low=1700e-6, high=alk)\n",
    "    # tem between 2 and 35 °C\n",
    "    tem = np.random.uniform(low=2, high=35)\n",
    "    # sal between 19 and 43 PSU\n",
    "    sal = np.random.uniform(low=19, high=43)\n",
    "    # sil between 0 and 134 mumol kg-1\n",
    "    sil = np.random.uniform(low=0, high=134e-6)\n",
    "    # phos between 0 and 4 mumol kg-1\n",
    "    phos = np.random.uniform(low=0, high=4e-6)\n",
    "    sample = (alk, dic, tem, sal, sil, phos)\n",
    "    samples.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33a458ef-b191-4adf-908f-71409075faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_alk, sample_dic, sample_tem, sample_sal, sample_sil, sample_phos =\\\n",
    "zip(*samples)\n",
    "sample_fco2 = calc_fCO2(sample_alk, sample_dic, sample_tem, sample_sal, sample_sil, sample_phos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6553912d-693c-4564-9742-0f30e6497c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453.50214571167703\n",
      "[328.11596538]\n",
      "(6000000,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(np.median(sample_fco2))\n",
    "print(calc_fCO2(tuple([2200e-6]), tuple([1950e-6]), tuple([18.5]), tuple([31]), tuple([67e-6]), tuple([2e-6])))\n",
    "print(sample_fco2.shape)\n",
    "print(type(sample_fco2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d62cadd1-7d5b-44f9-b67b-7314ba3e783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 5000_000\n",
    "\n",
    "train_fco2, valid_fco2 = np.split(sample_fco2, [ntrain])\n",
    "train_alk, valid_alk = np.split(np.array(sample_alk), [ntrain])\n",
    "train_dic, valid_dic = np.split(np.array(sample_dic), [ntrain])\n",
    "train_tem, valid_tem = np.split(np.array(sample_tem), [ntrain])\n",
    "train_sal, valid_sal = np.split(np.array(sample_sal), [ntrain])\n",
    "train_sil, valid_sil = np.split(np.array(sample_sil), [ntrain])\n",
    "train_phos, valid_phos = np.split(np.array(sample_phos), [ntrain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9723b9f-5778-4b50-9fab-92bce16129a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some statistics:\n",
      "Mean for alk samples: 2.200027e-03, expected: 2.200000e-03\n",
      "Std for alk samples: 2.886953e-04, expected: 2.886751e-04\n",
      "-----\n",
      "Mean for dic samples: 1.950093e-03, expected: 1.950000e-03\n",
      "Std for dic samples: 2.205415e-04, expected: 2.204793e-04\n",
      "-----\n",
      "Mean for tem samples: 1.849863e+01, expected: 1.850000e+01\n",
      "Std for tem samples: 9.530802e+00, expected: 9.526279e+00\n",
      "-----\n",
      "Mean for sal samples: 3.099651e+01, expected: 3.100000e+01\n",
      "Std for sal samples: 6.927757e+00, expected: 6.928203e+00\n",
      "-----\n",
      "Mean for sil samples: 6.697189e-05, expected: 6.700000e-05\n",
      "Std for sil samples: 3.870030e-05, expected: 3.868247e-05\n",
      "-----\n",
      "Mean for phos samples: 1.999234e-06, expected: 2.000000e-06\n",
      "Std for phos samples: 1.154432e-06, expected: 1.154701e-06\n"
     ]
    }
   ],
   "source": [
    "sample_alk_mean = (1700e-6 + 2700e-6) / 2\n",
    "sample_alk_std = (2700e-6 - 1700e-6) / np.sqrt(12)\n",
    "\n",
    "sample_dic_mean = 1700e-6 + (2700e-6 - 1700e-6) / 4\n",
    "sample_dic_std = (2700e-6 - 1700e-6) * np.sqrt(7 / 144)\n",
    "\n",
    "sample_tem_mean = (2 + 35) / 2\n",
    "sample_tem_std = (35 - 2) / np.sqrt(12)\n",
    "\n",
    "sample_sal_mean = (19 + 43) / 2\n",
    "sample_sal_std = (43 - 19) / np.sqrt(12)\n",
    "\n",
    "sample_sil_mean = (134e-6 + 0e-6) / 2\n",
    "sample_sil_std = (134e-6 - 0e-6) / np.sqrt(12)\n",
    "\n",
    "sample_phos_mean = (4e-6 + 0e-6) / 2\n",
    "sample_phos_std = (4e-6 - 0e-6) / np.sqrt(12)\n",
    "\n",
    "print(\"Some statistics:\")\n",
    "print(\"Mean for alk samples: {:.6e}, expected: {:.6e}\".format(np.mean(train_alk), sample_alk_mean))\n",
    "print(\"Std for alk samples: {:.6e}, expected: {:.6e}\".format(np.std(train_alk), sample_alk_std))\n",
    "print(\"-----\")\n",
    "print(\"Mean for dic samples: {:.6e}, expected: {:.6e}\".format(np.mean(train_dic), sample_dic_mean))\n",
    "print(\"Std for dic samples: {:.6e}, expected: {:.6e}\".format(np.std(train_dic), sample_dic_std))\n",
    "print(\"-----\")\n",
    "print(\"Mean for tem samples: {:.6e}, expected: {:.6e}\".format(np.mean(train_tem), sample_tem_mean))\n",
    "print(\"Std for tem samples: {:.6e}, expected: {:.6e}\".format(np.std(train_tem), sample_tem_std))\n",
    "print(\"-----\")\n",
    "print(\"Mean for sal samples: {:.6e}, expected: {:.6e}\".format(np.mean(sample_sal), sample_sal_mean))\n",
    "print(\"Std for sal samples: {:.6e}, expected: {:.6e}\".format(np.std(sample_sal), sample_sal_std))\n",
    "print(\"-----\")\n",
    "print(\"Mean for sil samples: {:.6e}, expected: {:.6e}\".format(np.mean(train_sil), sample_sil_mean))\n",
    "print(\"Std for sil samples: {:.6e}, expected: {:.6e}\".format(np.std(train_sil), sample_sil_std))\n",
    "print(\"-----\")\n",
    "print(\"Mean for phos samples: {:.6e}, expected: {:.6e}\".format(np.mean(train_phos), sample_phos_mean))\n",
    "print(\"Std for phos samples: {:.6e}, expected: {:.6e}\".format(np.std(train_phos), sample_phos_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf96fd-d71d-4724-89e3-f50269240345",
   "metadata": {},
   "source": [
    "#### Normalize samples and train neural network with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e39058f-ec81-456c-a9ec-388dde07b785",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_means = np.array([sample_alk_mean, sample_dic_mean, sample_tem_mean,\n",
    "                         sample_sal_mean, sample_sil_mean, sample_phos_mean])\n",
    "sample_stds = np.array([sample_alk_std, sample_dic_std, sample_tem_std,\n",
    "                         sample_sal_std, sample_sil_std, sample_phos_std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed0d400c-fb81-4cb2-9453-057fc3f5268c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000000, 6)\n"
     ]
    }
   ],
   "source": [
    "train_features = np.concatenate([train_alk[:, np.newaxis], train_dic[:, np.newaxis], train_tem[:, np.newaxis],\n",
    "                               train_sal[:, np.newaxis], train_sil[:, np.newaxis], train_phos[:, np.newaxis]], axis=1)\n",
    "\n",
    "valid_features = np.concatenate([valid_alk[:, np.newaxis], valid_dic[:, np.newaxis], valid_tem[:, np.newaxis],\n",
    "                               valid_sal[:, np.newaxis], valid_sil[:, np.newaxis], valid_phos[:, np.newaxis]], axis=1)\n",
    "\n",
    "print(train_features.shape)\n",
    "train_features_normalized = (train_features - sample_means) / sample_stds\n",
    "valid_features_normalized = (valid_features - sample_means) / sample_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f36cf192-52e5-4370-beeb-dc28e51c08e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verify that data is normalized.\n",
      "[ 9.47580989e-05  4.23186847e-04 -1.44276512e-04 -4.32910793e-04\n",
      " -7.26599596e-04 -6.63078680e-04]\n",
      "[1.00006983 1.00028241 1.00047477 0.99986704 1.00046103 0.99976755]\n"
     ]
    }
   ],
   "source": [
    "print(\"Verify that data is normalized.\")\n",
    "print(np.mean(train_features_normalized, axis=0))\n",
    "print(np.std(train_features_normalized, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "933cfa31-3848-4efe-a686-e90a1bceca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size, device=self.device)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size, device=self.device)\n",
    "        self.linear3 = nn.Linear(hidden_size, hidden_size, device=self.device)\n",
    "        self.linear4 = nn.Linear(hidden_size, output_size, device=self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        x = F.elu(self.linear1(x))\n",
    "        x = F.elu(self.linear2(x))\n",
    "        x = F.elu(self.linear3(x))\n",
    "        x = F.elu(self.linear4(x))\n",
    "        return x\n",
    "\n",
    "    def save(self, file_name='model.pth'):\n",
    "        model_folder_path = './model'\n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "\n",
    "        file_name = os.path.join(model_folder_path, file_name)\n",
    "        torch.save(self.state_dict(), file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9edbbbae-1c19-42ed-86b4-a55bea37fcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear1): Linear(in_features=6, out_features=64, bias=True)\n",
      "  (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (linear3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (linear4): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Number of trainable parameters in the model: 8833\n"
     ]
    }
   ],
   "source": [
    "model = MLP(6, 64, 1)\n",
    "print(model)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of trainable parameters in the model:\", pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d143e2a-376d-4563-927c-a280ede841f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_dataloader(features, labels, batch_size):\n",
    "    ntrain = len(labels)\n",
    "    nbatch = ntrain // batch_size\n",
    "    indices = np.arange(ntrain, dtype=int)\n",
    "    random.shuffle(indices)\n",
    "    batch_indices = np.split(indices[:nbatch * batch_size], nbatch)\n",
    "    batch_data = [(torch.from_numpy(np.take(features, ind, axis=0).astype(\"float32\")),\n",
    "                   torch.from_numpy(np.take(labels, ind).astype(\"float32\"))) for ind in batch_indices]\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "166b8204-6654-4bae-8be9-18e25f949319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    running_loss = 0. # running loss over all batches in the epoch\n",
    "    \n",
    "    training_data = training_dataloader(train_features_normalized,\n",
    "                                        train_fco2, batch_size)\n",
    "    \n",
    "    for batch in training_data:\n",
    "        features, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = torch.squeeze(model(features))\n",
    "        loss = loss_function(outputs, labels.to(torch.device(\"cuda\")))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.detach().cpu().item()\n",
    "\n",
    "    ntrain = len(train_fco2)\n",
    "    nbatch = ntrain // batch_size\n",
    "\n",
    "    return running_loss / nbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09e2104f-fbc4-43e1-ad6b-456f6f882b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Loss train 28864.45091426239 valid 43.128937357100455\n",
      "Epoch 2:\n",
      "Loss train 18.55601351594925 valid 7.7519794439822824\n",
      "Epoch 3:\n",
      "Loss train 5.254506319570542 valid 3.7493398866832717\n",
      "Epoch 4:\n",
      "Loss train 2.9867678776979445 valid 2.6385124397728417\n",
      "Epoch 5:\n",
      "Loss train 2.2759529870986936 valid 1.7075538814436286\n",
      "Epoch 6:\n",
      "Loss train 1.8838338971495627 valid 1.1381169277901935\n",
      "Epoch 7:\n",
      "Loss train 1.7012697219491004 valid 1.136594912110647\n",
      "Epoch 8:\n",
      "Loss train 1.5039412677645683 valid 0.895458614610667\n",
      "Epoch 9:\n",
      "Loss train 1.380420422422886 valid 1.3525340934443202\n",
      "Epoch 10:\n",
      "Loss train 1.323362349307537 valid 0.8849860516594632\n",
      "Epoch 11:\n",
      "Loss train 1.2400273625016212 valid 0.6465340860687695\n",
      "Epoch 12:\n",
      "Loss train 1.1784311193466186 valid 1.227594006874498\n",
      "Epoch 13:\n",
      "Loss train 1.1239829023063184 valid 1.0609803077610296\n",
      "Epoch 14:\n",
      "Loss train 1.1019444766163826 valid 0.8497758823394075\n",
      "Epoch 15:\n",
      "Loss train 1.048903335148096 valid 0.950761750038526\n",
      "Epoch 16:\n",
      "Loss train 1.0574259937465191 valid 0.5225577440407224\n",
      "Epoch 17:\n",
      "Loss train 0.9908062466621399 valid 0.7670591776368622\n",
      "Epoch 18:\n",
      "Loss train 1.0190661082088948 valid 0.6587160208460815\n",
      "Epoch 19:\n",
      "Loss train 0.9427409795761108 valid 0.8245386440232108\n",
      "Epoch 20:\n",
      "Loss train 0.9855625270307065 valid 0.587197934354292\n",
      "Epoch 21:\n",
      "Loss train 0.893112448322773 valid 0.39425281274847007\n",
      "Epoch 22:\n",
      "Loss train 0.9084227944970131 valid 0.6393148624971157\n",
      "Epoch 23:\n",
      "Loss train 0.9192780799567699 valid 0.43799816442636563\n",
      "Epoch 24:\n",
      "Loss train 0.8908434274792671 valid 0.41672191841813644\n",
      "Epoch 25:\n",
      "Loss train 0.8419634033828973 valid 3.9924602517156536\n",
      "Epoch 26:\n",
      "Loss train 0.8929673182934522 valid 0.34239744397767774\n",
      "Epoch 27:\n",
      "Loss train 0.8628793999791146 valid 0.45411705711743305\n",
      "Epoch 28:\n",
      "Loss train 0.8247780751645565 valid 0.7373109105107915\n",
      "Epoch 29:\n",
      "Loss train 0.8063714539021254 valid 0.5314315213732573\n",
      "Epoch 30:\n",
      "Loss train 0.8290708865910769 valid 0.4373647800445702\n",
      "Epoch 31:\n",
      "Loss train 0.8053906405299902 valid 0.37082726728629795\n",
      "Epoch 32:\n",
      "Loss train 0.7906551250100136 valid 0.33838611561453785\n",
      "Epoch 33:\n",
      "Loss train 0.8832167794704437 valid 0.24542523778302613\n",
      "Epoch 34:\n",
      "Loss train 0.7546210051238537 valid 0.3714086359425492\n",
      "Epoch 35:\n",
      "Loss train 0.7766593624651432 valid 0.39229947473590093\n",
      "Epoch 36:\n",
      "Loss train 0.7681426560461521 valid 0.4752108176853643\n",
      "Epoch 37:\n",
      "Loss train 0.778305139914155 valid 0.3671825969913844\n",
      "Epoch 38:\n",
      "Loss train 0.7441631887972355 valid 0.2628647324148181\n",
      "Epoch 39:\n",
      "Loss train 0.7420820673316717 valid 0.2940084797979346\n",
      "Epoch 40:\n",
      "Loss train 0.7483630093663931 valid 0.3691240518066905\n",
      "Epoch 41:\n",
      "Loss train 0.7538265789836646 valid 0.31142887950725645\n",
      "Epoch 42:\n",
      "Loss train 0.7198828360378742 valid 0.4547985754777614\n",
      "Epoch 43:\n",
      "Loss train 0.736722896850109 valid 7.895510916763918\n",
      "Epoch 44:\n",
      "Loss train 0.6876316194027662 valid 0.7210958901341394\n",
      "Epoch 45:\n",
      "Loss train 0.7123326275110244 valid 0.3881572920061206\n",
      "Epoch 46:\n",
      "Loss train 0.6883053372830152 valid 0.3040276615819465\n",
      "Epoch 47:\n",
      "Loss train 0.7466783274024725 valid 0.26670745471747\n",
      "Epoch 48:\n",
      "Loss train 0.6923562159329653 valid 0.34294161107689974\n",
      "Epoch 49:\n",
      "Loss train 0.7196515589118004 valid 0.2129374819095023\n",
      "Epoch 50:\n",
      "Loss train 0.6812141857385635 valid 0.28053543245026574\n",
      "Epoch 51:\n",
      "Loss train 0.6973192566305398 valid 0.27164253992871074\n",
      "Epoch 52:\n",
      "Loss train 0.7683687580645084 valid 0.23276781716926487\n",
      "Epoch 53:\n",
      "Loss train 0.6572206220835447 valid 0.23706632747938927\n",
      "Epoch 54:\n",
      "Loss train 0.6891922513663769 valid 1.118736316072884\n",
      "Epoch 55:\n",
      "Loss train 0.6524696586817503 valid 0.19012883763328095\n",
      "Epoch 56:\n",
      "Loss train 0.6666517510712147 valid 1.102563928455632\n",
      "Epoch 57:\n",
      "Loss train 0.7186727279275655 valid 0.20988365193082947\n",
      "Epoch 58:\n",
      "Loss train 0.6516172331750393 valid 0.27431264614840334\n",
      "Epoch 59:\n",
      "Loss train 0.6329475774347783 valid 1.7679404777546157\n",
      "Epoch 60:\n",
      "Loss train 0.682136735624075 valid 0.39068631374852664\n",
      "Epoch 61:\n",
      "Loss train 0.6564082294046879 valid 3.3218289891782478\n",
      "Epoch 62:\n",
      "Loss train 0.6458747769057751 valid 1.300216516637017\n",
      "Epoch 63:\n",
      "Loss train 0.6262976589053869 valid 0.2560155129390194\n",
      "Epoch 64:\n",
      "Loss train 0.6537878454834223 valid 0.23272520735975732\n",
      "Epoch 65:\n",
      "Loss train 0.6307784099400043 valid 0.3115912205933316\n",
      "Epoch 66:\n",
      "Loss train 0.6094140400528908 valid 0.42817960512119135\n",
      "Epoch 67:\n",
      "Loss train 0.6499421292960644 valid 0.250723183239339\n",
      "Epoch 68:\n",
      "Loss train 0.6159036199241876 valid 0.24789019434085016\n",
      "Epoch 69:\n",
      "Loss train 0.629155275683105 valid 0.26372286746809337\n",
      "Epoch 70:\n",
      "Loss train 0.6156369108736515 valid 0.2190533533184974\n",
      "Epoch 71:\n",
      "Loss train 0.6341698395073414 valid 0.3295386621302368\n",
      "Epoch 72:\n",
      "Loss train 0.6055433673635126 valid 0.18224064398728174\n",
      "Epoch 73:\n",
      "Loss train 0.6131331040769815 valid 0.565663143496389\n",
      "Epoch 74:\n",
      "Loss train 0.7078665800184012 valid 0.21311552699189493\n",
      "Epoch 75:\n",
      "Loss train 0.5578162165015936 valid 0.2167629986374965\n",
      "Epoch 76:\n",
      "Loss train 0.615249166405201 valid 1.500924369286498\n",
      "Epoch 77:\n",
      "Loss train 0.6084015092492103 valid 0.24723795206140253\n",
      "Epoch 78:\n",
      "Loss train 0.5855169812455774 valid 0.2340056193630491\n",
      "Epoch 79:\n",
      "Loss train 0.6247690862759948 valid 0.28486968498409165\n",
      "Epoch 80:\n",
      "Loss train 0.5685368557378649 valid 0.2044562873791697\n",
      "Epoch 81:\n",
      "Loss train 0.5909765903711319 valid 0.5746888265290411\n",
      "Epoch 82:\n",
      "Loss train 0.6214935641258955 valid 1.210495906628852\n",
      "Epoch 83:\n",
      "Loss train 0.5734645910486579 valid 0.1596752816487345\n",
      "Epoch 84:\n",
      "Loss train 0.5666893027037382 valid 0.22831895883060638\n",
      "Epoch 85:\n",
      "Loss train 0.567431949712336 valid 0.18390878158302953\n",
      "Epoch 86:\n",
      "Loss train 0.5700793811887502 valid 0.23096486806698438\n",
      "Epoch 87:\n",
      "Loss train 0.560006329074502 valid 0.31329112175040064\n",
      "Epoch 88:\n",
      "Loss train 0.5789316285774112 valid 0.9320698349623131\n",
      "Epoch 89:\n",
      "Loss train 0.555970944467187 valid 0.6347843116742782\n",
      "Epoch 90:\n",
      "Loss train 0.5765495688244701 valid 0.12788676625673867\n",
      "Epoch 91:\n",
      "Loss train 0.5985056920364499 valid 0.7625399392362046\n",
      "Epoch 92:\n",
      "Loss train 0.5476748127743601 valid 0.6406939302964856\n",
      "Epoch 93:\n",
      "Loss train 0.5637913420125842 valid 0.22241236815456863\n",
      "Epoch 94:\n",
      "Loss train 0.527403009699285 valid 0.9877109296285412\n",
      "Epoch 95:\n",
      "Loss train 0.5709333626687527 valid 1.9553478107797864\n",
      "Epoch 96:\n",
      "Loss train 0.532361253093183 valid 0.15803670722483512\n",
      "Epoch 97:\n",
      "Loss train 0.561563463486731 valid 1.3686533181831388\n",
      "Epoch 98:\n",
      "Loss train 0.5164003240838647 valid 0.7307236653468817\n",
      "Epoch 99:\n",
      "Loss train 0.5371786837249994 valid 0.28618324320070687\n",
      "Epoch 100:\n",
      "Loss train 0.5519020007833838 valid 2.9485193856094205\n",
      "Epoch 101:\n",
      "Loss train 0.5217751131653786 valid 0.1364876152240837\n",
      "Epoch 102:\n",
      "Loss train 0.5491564204052091 valid 0.6785209306926099\n",
      "Epoch 103:\n",
      "Loss train 0.5392033228322863 valid 0.4681456888351044\n",
      "Epoch 104:\n",
      "Loss train 0.5439498281568289 valid 0.2602123428707957\n",
      "Epoch 105:\n",
      "Loss train 0.5217182128325105 valid 0.5521195029741226\n",
      "Epoch 106:\n",
      "Loss train 0.5314004745349288 valid 0.2921627595736603\n",
      "Epoch 107:\n",
      "Loss train 0.5378179205343128 valid 1.0673467452790655\n",
      "Epoch 108:\n",
      "Loss train 0.5123920948654413 valid 0.32158629145812545\n",
      "Epoch 109:\n",
      "Loss train 0.5334370097532868 valid 0.24014739857060421\n",
      "Epoch 110:\n",
      "Loss train 0.48767507276088 valid 0.13007880349925108\n",
      "Epoch 111:\n",
      "Loss train 0.5090550236687064 valid 0.47189850641426767\n",
      "Epoch 112:\n",
      "Loss train 0.5288115906402469 valid 1.6499280572403672\n",
      "Epoch 113:\n",
      "Loss train 0.5088045855298639 valid 4.204334960766384\n",
      "Epoch 114:\n",
      "Loss train 0.5014148596897722 valid 0.5526281709209859\n",
      "Epoch 115:\n",
      "Loss train 0.5355861017987132 valid 0.14420616643121592\n",
      "Epoch 116:\n",
      "Loss train 0.48610439460277555 valid 0.7215121700169156\n",
      "Epoch 117:\n",
      "Loss train 0.5114511778831482 valid 4.565031779681127\n",
      "Epoch 118:\n",
      "Loss train 0.5023823747053743 valid 0.22109650215379084\n",
      "Epoch 119:\n",
      "Loss train 0.48710157751441 valid 0.15269608913606156\n",
      "Epoch 120:\n",
      "Loss train 0.5065916879504919 valid 0.11361884508523303\n",
      "Epoch 121:\n",
      "Loss train 0.48502844184339045 valid 0.2251873487703782\n",
      "Epoch 122:\n",
      "Loss train 0.49568426166027785 valid 0.65077096577285\n",
      "Epoch 123:\n",
      "Loss train 0.49204696954637767 valid 0.13680929177488613\n",
      "Epoch 124:\n",
      "Loss train 0.464833776538074 valid 0.1647884819856575\n",
      "Epoch 125:\n",
      "Loss train 0.5227100754186511 valid 0.7021334583010762\n",
      "Epoch 126:\n",
      "Loss train 0.5007042695537209 valid 0.10399393051534403\n",
      "Epoch 127:\n",
      "Loss train 0.4791442502915859 valid 0.27299967596985364\n",
      "Epoch 128:\n",
      "Loss train 0.4735804866284132 valid 0.18195417842744066\n",
      "Epoch 129:\n",
      "Loss train 0.5094379069492221 valid 0.2267811199299197\n",
      "Epoch 130:\n",
      "Loss train 0.4953869232147932 valid 1.3765494398029712\n",
      "Epoch 131:\n",
      "Loss train 0.45577666765898467 valid 0.32459035517679763\n",
      "Epoch 132:\n",
      "Loss train 0.47482281513214114 valid 0.23808741346589737\n",
      "Epoch 133:\n",
      "Loss train 0.4981661937057972 valid 0.5224188151024063\n",
      "Epoch 134:\n",
      "Loss train 0.49020075979083777 valid 0.10629653687172577\n",
      "Epoch 135:\n",
      "Loss train 0.44481289147734643 valid 0.12186760500443353\n",
      "Epoch 136:\n",
      "Loss train 0.45610630012005565 valid 0.6086369901631634\n",
      "Epoch 137:\n",
      "Loss train 0.46489358328878877 valid 0.4587625072975434\n",
      "Epoch 138:\n",
      "Loss train 0.47855691388845445 valid 0.16708711375037197\n",
      "Epoch 139:\n",
      "Loss train 0.4653100390315056 valid 0.6263261971565258\n",
      "Epoch 140:\n",
      "Loss train 0.46854520045369863 valid 0.17219113197446104\n",
      "Epoch 141:\n",
      "Loss train 0.4963722766205668 valid 0.12991436051348745\n",
      "Epoch 142:\n",
      "Loss train 0.46636967991441486 valid 0.9617880837061582\n",
      "Epoch 143:\n",
      "Loss train 0.489949057136476 valid 0.10540588712954081\n",
      "Epoch 144:\n",
      "Loss train 0.4453399617999792 valid 0.209109652570186\n",
      "Epoch 145:\n",
      "Loss train 0.4652174356579781 valid 4.113892611186878\n",
      "Epoch 146:\n",
      "Loss train 0.4388172022521496 valid 1.1857939256273118\n",
      "Epoch 147:\n",
      "Loss train 0.4494430667459965 valid 0.11508644015456782\n",
      "Epoch 148:\n",
      "Loss train 0.46038397787213325 valid 0.08814854839062645\n",
      "Epoch 149:\n",
      "Loss train 0.4311166246458888 valid 0.41992541026009705\n",
      "Epoch 150:\n",
      "Loss train 0.45853210247904064 valid 0.5774357983003745\n",
      "Epoch 151:\n",
      "Loss train 0.5216280462443829 valid 1.4782961778742838\n",
      "Epoch 152:\n",
      "Loss train 0.39835502061247824 valid 0.323655996240567\n",
      "Epoch 153:\n",
      "Loss train 0.42835541145950556 valid 0.1825467481238438\n",
      "Epoch 154:\n",
      "Loss train 0.43872643683850765 valid 0.3343829909424017\n",
      "Epoch 155:\n",
      "Loss train 0.45513755611032247 valid 0.3310824388823568\n",
      "Epoch 156:\n",
      "Loss train 0.44030612370073796 valid 0.7586597656913094\n",
      "Epoch 157:\n",
      "Loss train 0.411101433506608 valid 0.37046178491842063\n",
      "Epoch 158:\n",
      "Loss train 0.4320381973132491 valid 0.28064680359406685\n",
      "Epoch 159:\n",
      "Loss train 0.42726174006313083 valid 0.12138092625407679\n",
      "Epoch 160:\n",
      "Loss train 0.42195981194525956 valid 0.14830161141839718\n",
      "Epoch 161:\n",
      "Loss train 0.44450999721437695 valid 0.32618701035673\n",
      "Epoch 162:\n",
      "Loss train 0.4122564799696207 valid 0.20110536071341972\n",
      "Epoch 163:\n",
      "Loss train 0.4424693799674511 valid 0.263269133917683\n",
      "Epoch 164:\n",
      "Loss train 0.43352428860664366 valid 0.8278765598722659\n",
      "Epoch 165:\n",
      "Loss train 0.4124699393212795 valid 0.14083278920690132\n",
      "Epoch 166:\n",
      "Loss train 0.413574352709949 valid 0.21826883938105282\n",
      "Epoch 167:\n",
      "Loss train 0.4375788355246186 valid 0.18211010334309732\n",
      "Epoch 168:\n",
      "Loss train 0.4152173602357507 valid 0.17995377074111243\n",
      "Epoch 169:\n",
      "Loss train 0.40963569732308386 valid 1.0236142776852943\n",
      "Epoch 170:\n",
      "Loss train 0.40764508853554726 valid 0.7466359042818367\n",
      "Epoch 171:\n",
      "Loss train 0.43116430161744357 valid 0.23307171891637568\n",
      "Epoch 172:\n",
      "Loss train 0.4431588651917875 valid 0.10268725805617362\n",
      "Epoch 173:\n",
      "Loss train 0.3938343564748764 valid 0.5299387742421864\n",
      "Epoch 174:\n",
      "Loss train 0.4335611698627472 valid 0.4379275085898652\n",
      "Epoch 175:\n",
      "Loss train 0.40453227592557667 valid 1.373716279926447\n",
      "Epoch 176:\n",
      "Loss train 0.4021234229132533 valid 0.28516571698393783\n",
      "Epoch 177:\n",
      "Loss train 0.4052983280226588 valid 0.16935456522272638\n",
      "Epoch 178:\n",
      "Loss train 0.4166066850028932 valid 0.22460758854686452\n",
      "Epoch 179:\n",
      "Loss train 0.3943200167834759 valid 0.1028729016735778\n",
      "Epoch 180:\n",
      "Loss train 0.39060953955501315 valid 1.0145555299722913\n",
      "Epoch 181:\n",
      "Loss train 0.39147009054422377 valid 0.2323389902805171\n",
      "Epoch 182:\n",
      "Loss train 0.4197010510444641 valid 0.26475133645185434\n",
      "Epoch 183:\n",
      "Loss train 0.3881448556996882 valid 0.1800501442761136\n",
      "Epoch 184:\n",
      "Loss train 0.4011137130305171 valid 0.6946982123933527\n",
      "Epoch 185:\n",
      "Loss train 0.4254207593411207 valid 1.7290749869895556\n",
      "Epoch 186:\n",
      "Loss train 0.3721509404577315 valid 0.3815990253082592\n",
      "Epoch 187:\n",
      "Loss train 0.4018010108806193 valid 0.5004501558902704\n",
      "Epoch 188:\n",
      "Loss train 0.4092704029455781 valid 1.4252598335121898\n",
      "Epoch 189:\n",
      "Loss train 0.39841741292476657 valid 0.11251051722206647\n",
      "Epoch 190:\n",
      "Loss train 0.38644685484245417 valid 0.10407253818669852\n",
      "Epoch 191:\n",
      "Loss train 0.42285788334310054 valid 0.21411377343589558\n",
      "Epoch 192:\n",
      "Loss train 0.35420532986223696 valid 0.446156909549893\n",
      "Epoch 193:\n",
      "Loss train 0.4069904817901552 valid 0.24463377191140057\n",
      "Epoch 194:\n",
      "Loss train 0.39566129791289567 valid 0.17286424999773023\n",
      "Epoch 195:\n",
      "Loss train 0.3855926220118999 valid 0.1325117024640424\n",
      "Epoch 196:\n",
      "Loss train 0.37898177402988076 valid 0.1404836268336798\n",
      "Epoch 197:\n",
      "Loss train 0.41671927438899875 valid 0.17706556299448828\n",
      "Epoch 198:\n",
      "Loss train 0.36971521361619236 valid 0.17523659221835688\n",
      "Epoch 199:\n",
      "Loss train 0.38761270004585385 valid 0.9691792046237304\n",
      "Epoch 200:\n",
      "Loss train 0.3896418513327837 valid 0.33831137939591405\n",
      "Epoch 201:\n",
      "Loss train 0.3921063513942063 valid 0.09266104350922921\n",
      "Epoch 202:\n",
      "Loss train 0.3611069457747042 valid 0.84034814888156\n",
      "Epoch 203:\n",
      "Loss train 0.3920607224434614 valid 0.578032002512509\n",
      "Epoch 204:\n",
      "Loss train 0.365438248090446 valid 0.17667864231255695\n",
      "Epoch 205:\n",
      "Loss train 0.3814202613957226 valid 0.1836995012925587\n",
      "Epoch 206:\n",
      "Loss train 0.3926694284655154 valid 0.07360053506033025\n",
      "Epoch 207:\n",
      "Loss train 0.36822640994787215 valid 0.21586160097420512\n",
      "Epoch 208:\n",
      "Loss train 0.3946387713603675 valid 0.1227571845748234\n",
      "Epoch 209:\n",
      "Loss train 0.39987576366066935 valid 5.173568820797909\n",
      "Epoch 210:\n",
      "Loss train 0.3717873904377222 valid 0.18676452356940632\n",
      "Epoch 211:\n",
      "Loss train 0.3939964975945652 valid 0.4903386895880469\n",
      "Epoch 212:\n",
      "Loss train 0.3872473557323217 valid 0.10038184947450257\n",
      "Epoch 213:\n",
      "Loss train 0.37369959287270904 valid 0.13065286309514643\n",
      "Epoch 214:\n",
      "Loss train 0.3721145629219711 valid 0.10080227573135964\n",
      "Epoch 215:\n",
      "Loss train 0.3540990246258676 valid 0.24927324119533883\n",
      "Epoch 216:\n",
      "Loss train 0.3871550967894494 valid 0.1420608689129263\n",
      "Epoch 217:\n",
      "Loss train 0.374698344630748 valid 0.09243195223065577\n",
      "Epoch 218:\n",
      "Loss train 0.3618791740879416 valid 0.10834669730718051\n",
      "Epoch 219:\n",
      "Loss train 0.400999801902473 valid 1.8043137339162298\n",
      "Epoch 220:\n",
      "Loss train 0.3361625062048435 valid 0.06275918395984267\n",
      "Epoch 221:\n",
      "Loss train 0.3626701417148113 valid 0.13207997331328497\n",
      "Epoch 222:\n",
      "Loss train 0.3674490944162011 valid 0.15118732778268149\n",
      "Epoch 223:\n",
      "Loss train 0.3764953307434917 valid 0.47373739354710404\n",
      "Epoch 224:\n",
      "Loss train 0.3638653856918216 valid 0.10069488374162863\n",
      "Epoch 225:\n",
      "Loss train 0.3646869486540556 valid 0.07679447255201872\n",
      "Epoch 226:\n",
      "Loss train 0.3852078262925148 valid 0.10378302801547044\n",
      "Epoch 227:\n",
      "Loss train 0.35263411258012056 valid 0.1997819082118508\n",
      "Epoch 228:\n",
      "Loss train 0.37062541122213005 valid 0.2646703063646377\n",
      "Epoch 229:\n",
      "Loss train 0.3460516746692359 valid 0.2930336788851316\n",
      "Epoch 230:\n",
      "Loss train 0.3512907729908824 valid 0.182538456612908\n",
      "Epoch 231:\n",
      "Loss train 0.40053114693313835 valid 0.1285558919596848\n",
      "Epoch 232:\n",
      "Loss train 0.3644979083262384 valid 0.070124553172557\n",
      "Epoch 233:\n",
      "Loss train 0.3524815612860024 valid 0.9203581004095254\n",
      "Epoch 234:\n",
      "Loss train 0.3650563720561564 valid 0.1246991082239773\n",
      "Epoch 235:\n",
      "Loss train 0.366253628154099 valid 0.1467074905745161\n",
      "Epoch 236:\n",
      "Loss train 0.35881169491559267 valid 0.09053721379839064\n",
      "Epoch 237:\n",
      "Loss train 0.37601560942679646 valid 0.07789322264882613\n",
      "Epoch 238:\n",
      "Loss train 0.3334340024620295 valid 0.12853468556359993\n",
      "Epoch 239:\n",
      "Loss train 0.3627693310186267 valid 0.10825297187556814\n",
      "Epoch 240:\n",
      "Loss train 0.35566831007450816 valid 0.09856317680722693\n",
      "Epoch 241:\n",
      "Loss train 0.34630671409741043 valid 0.09279401993599189\n",
      "Epoch 242:\n",
      "Loss train 0.3486764780640602 valid 0.34021654034238386\n",
      "Epoch 243:\n",
      "Loss train 0.37056794006079435 valid 0.07665317088252507\n",
      "Epoch 244:\n",
      "Loss train 0.360960142827034 valid 0.12674941517153132\n",
      "Epoch 245:\n",
      "Loss train 0.347820490847528 valid 0.06838615334645894\n",
      "Epoch 246:\n",
      "Loss train 0.3495868248723447 valid 0.07154078668155806\n",
      "Epoch 247:\n",
      "Loss train 0.38700129644870757 valid 0.2154747991159407\n",
      "Epoch 248:\n",
      "Loss train 0.3254482853829861 valid 0.06543340064577219\n",
      "Epoch 249:\n",
      "Loss train 0.3798947721727192 valid 0.10807402362372256\n",
      "Epoch 250:\n",
      "Loss train 0.350691545484215 valid 0.08600789915869468\n",
      "Epoch 251:\n",
      "Loss train 0.335721025826782 valid 0.14333741620061874\n",
      "Epoch 252:\n",
      "Loss train 0.3480341316901147 valid 0.0782048157439617\n",
      "Epoch 253:\n",
      "Loss train 0.36574954015985134 valid 0.11362588216750368\n",
      "Epoch 254:\n",
      "Loss train 0.3410073852114379 valid 0.1463262640531153\n",
      "Epoch 255:\n",
      "Loss train 0.33405567979738116 valid 1.9742349117137166\n",
      "Epoch 256:\n",
      "Loss train 0.36208545780107376 valid 0.20964183035681871\n",
      "Epoch 257:\n",
      "Loss train 0.3447877451345325 valid 0.32577461424746934\n",
      "Epoch 258:\n",
      "Loss train 0.3376522884145379 valid 0.1740606290079103\n",
      "Epoch 259:\n",
      "Loss train 0.35210085895806553 valid 0.22895389058857893\n",
      "Epoch 260:\n",
      "Loss train 0.32199343130737546 valid 0.5485564109539415\n",
      "Epoch 261:\n",
      "Loss train 0.3452643780447543 valid 5.592630509925651\n",
      "Epoch 262:\n",
      "Loss train 0.3445836392760277 valid 0.2865015622424248\n",
      "Epoch 263:\n",
      "Loss train 0.33208015475571157 valid 0.05914117577724394\n",
      "Epoch 264:\n",
      "Loss train 0.32174941826313735 valid 0.09428628507576539\n",
      "Epoch 265:\n",
      "Loss train 0.3730477440185845 valid 0.12952653683151227\n",
      "Epoch 266:\n",
      "Loss train 0.3106110776223242 valid 0.1732062647572256\n",
      "Epoch 267:\n",
      "Loss train 0.40259719085395335 valid 0.05626055848014961\n",
      "Epoch 268:\n",
      "Loss train 0.29941305848285554 valid 0.12012154417531692\n",
      "Epoch 269:\n",
      "Loss train 0.3339389170177281 valid 0.11102174667655597\n",
      "Epoch 270:\n",
      "Loss train 0.33026171877086163 valid 0.411857890502949\n",
      "Epoch 271:\n",
      "Loss train 0.36798519727513196 valid 0.5330561228595764\n",
      "Epoch 272:\n",
      "Loss train 0.30148715584129093 valid 0.10442448011769517\n",
      "Epoch 273:\n",
      "Loss train 0.32591271747201683 valid 0.603268597942056\n",
      "Epoch 274:\n",
      "Loss train 0.33801701036691667 valid 0.1062875618521703\n",
      "Epoch 275:\n",
      "Loss train 0.35138288675695656 valid 0.7505187458184417\n",
      "Epoch 276:\n",
      "Loss train 0.3220461414635181 valid 0.08274531190089188\n",
      "Epoch 277:\n",
      "Loss train 0.32441782910823824 valid 0.06365674263333079\n",
      "Epoch 278:\n",
      "Loss train 0.3218663842685521 valid 0.12151361508819136\n",
      "Epoch 279:\n",
      "Loss train 0.3162790180020034 valid 0.23942746935809286\n",
      "Epoch 280:\n",
      "Loss train 0.3219515962064266 valid 3.059484471607581\n",
      "Epoch 281:\n",
      "Loss train 0.3387310493975878 valid 4.022658680315596\n",
      "Epoch 282:\n",
      "Loss train 0.3325961707964539 valid 0.47388057633440184\n",
      "Epoch 283:\n",
      "Loss train 0.3217828085117042 valid 0.19602642150672542\n",
      "Epoch 284:\n",
      "Loss train 0.3353585140936077 valid 0.07327740688134976\n",
      "Epoch 285:\n",
      "Loss train 0.3203776783883572 valid 0.10724594042069356\n",
      "Epoch 286:\n",
      "Loss train 0.34457876603677867 valid 0.22688519579789548\n",
      "Epoch 287:\n",
      "Loss train 0.3338819872051477 valid 1.118126655007984\n",
      "Epoch 288:\n",
      "Loss train 0.30627515280768275 valid 0.07908128382663286\n",
      "Epoch 289:\n",
      "Loss train 0.31471191158294676 valid 0.3494597461154988\n",
      "Epoch 290:\n",
      "Loss train 0.34317971611618997 valid 0.09939714753063092\n",
      "Epoch 291:\n",
      "Loss train 0.31953817531391976 valid 0.06671470212666597\n",
      "Epoch 292:\n",
      "Loss train 0.3243178058922291 valid 0.10587866006134572\n",
      "Epoch 293:\n",
      "Loss train 0.30261796656250955 valid 1.2246148275998014\n",
      "Epoch 294:\n",
      "Loss train 0.3237311158083379 valid 0.12430748271955648\n",
      "Epoch 295:\n",
      "Loss train 0.3041319429606199 valid 0.3493064658438165\n",
      "Epoch 296:\n",
      "Loss train 0.3223402948103845 valid 0.07070767412766776\n",
      "Epoch 297:\n",
      "Loss train 0.293849786362797 valid 0.06847844172562476\n",
      "Epoch 298:\n",
      "Loss train 0.3368525326758623 valid 0.24476455167972042\n",
      "Epoch 299:\n",
      "Loss train 0.3096081470429897 valid 0.05033810821091909\n",
      "Epoch 300:\n",
      "Loss train 0.3320696709305048 valid 0.16047211039478426\n",
      "Epoch 301:\n",
      "Loss train 0.3175914358988404 valid 0.11762859530741536\n",
      "Epoch 302:\n",
      "Loss train 0.31688432354182006 valid 0.13145166261226596\n",
      "Epoch 303:\n",
      "Loss train 0.3153786082558334 valid 0.29501895460253924\n",
      "Epoch 304:\n",
      "Loss train 0.3131298902608454 valid 0.050013752407796444\n",
      "Epoch 305:\n",
      "Loss train 0.3145568818032742 valid 0.523004621594317\n",
      "Epoch 306:\n",
      "Loss train 0.32397517848685387 valid 0.15922492525691673\n",
      "Epoch 307:\n",
      "Loss train 0.3087988456726074 valid 0.061686784187170544\n",
      "Epoch 308:\n",
      "Loss train 0.3076679699294269 valid 1.5125499777920637\n",
      "Epoch 309:\n",
      "Loss train 0.32594914962649346 valid 0.06019975873798174\n",
      "Epoch 310:\n",
      "Loss train 0.301318453040719 valid 0.29812743286077503\n",
      "Epoch 311:\n",
      "Loss train 0.32024483211785554 valid 0.05218006708136512\n",
      "Epoch 312:\n",
      "Loss train 0.29225235950350764 valid 0.7976986706626054\n",
      "Epoch 313:\n",
      "Loss train 0.3386677277214825 valid 1.1588419224009787\n",
      "Epoch 314:\n",
      "Loss train 0.31306253123581407 valid 0.18058867226012565\n",
      "Epoch 315:\n",
      "Loss train 0.3000696312978864 valid 0.055319378766319725\n",
      "Epoch 316:\n",
      "Loss train 0.3080116975180805 valid 0.059022098226507454\n",
      "Epoch 317:\n",
      "Loss train 0.30920423769652844 valid 0.2664473455267955\n",
      "Epoch 318:\n",
      "Loss train 0.3108531311169267 valid 0.12550339536912436\n",
      "Epoch 319:\n",
      "Loss train 0.2976442909732461 valid 0.3447821953837917\n",
      "Epoch 320:\n",
      "Loss train 0.292058127541095 valid 0.11363988900833934\n",
      "Epoch 321:\n",
      "Loss train 0.3166437294743955 valid 0.056665063654434425\n",
      "Epoch 322:\n",
      "Loss train 0.3158400792501867 valid 0.16039988085734394\n",
      "Epoch 323:\n",
      "Loss train 0.3043286452069879 valid 0.09214705264624373\n",
      "Epoch 324:\n",
      "Loss train 0.3017491178929806 valid 0.06526136339558426\n",
      "Epoch 325:\n",
      "Loss train 0.3106160184681416 valid 0.11521747285918073\n",
      "Epoch 326:\n",
      "Loss train 0.31002862547934057 valid 0.07082619041342175\n",
      "Epoch 327:\n",
      "Loss train 0.31603187995627524 valid 0.0588559105166248\n",
      "Epoch 328:\n",
      "Loss train 0.313895913553983 valid 0.13052958010995985\n",
      "Epoch 329:\n",
      "Loss train 0.2945564557731152 valid 0.1508176550790624\n",
      "Epoch 330:\n",
      "Loss train 0.2945883127629757 valid 0.3345171346633086\n",
      "Epoch 331:\n",
      "Loss train 0.2937907840907574 valid 0.15506987687873916\n",
      "Epoch 332:\n",
      "Loss train 0.31615358528867366 valid 0.0712780510896194\n",
      "Epoch 333:\n",
      "Loss train 0.3101368536002934 valid 0.3511794537389533\n",
      "Epoch 334:\n",
      "Loss train 0.3849605867832899 valid 0.05222634161659879\n",
      "Epoch 335:\n",
      "Loss train 0.2753945348717272 valid 0.07155284721497772\n",
      "Epoch 336:\n",
      "Loss train 0.2942061433106661 valid 0.06611177810498772\n",
      "Epoch 337:\n",
      "Loss train 0.2946407867617905 valid 0.17643902317425592\n",
      "Epoch 338:\n",
      "Loss train 0.3038491316795349 valid 0.08833931655376442\n",
      "Epoch 339:\n",
      "Loss train 0.2862030924618244 valid 0.04612215221097334\n",
      "Epoch 340:\n",
      "Loss train 0.290025340846926 valid 0.09722137435756428\n",
      "Epoch 341:\n",
      "Loss train 0.28396213392987846 valid 0.13023295460400033\n",
      "Epoch 342:\n",
      "Loss train 0.3141064035430551 valid 0.05157898381812103\n",
      "Epoch 343:\n",
      "Loss train 0.2782985837645829 valid 0.2599312322184405\n",
      "Epoch 344:\n",
      "Loss train 0.2839845218345523 valid 0.12035034311580443\n",
      "Epoch 345:\n",
      "Loss train 0.29767203439325096 valid 0.15371056244067186\n",
      "Epoch 346:\n",
      "Loss train 0.28326084118410944 valid 2.0394389033113307\n",
      "Epoch 347:\n",
      "Loss train 0.3052618377484381 valid 0.05090741370814606\n",
      "Epoch 348:\n",
      "Loss train 0.2959837323486805 valid 0.23540270444555222\n",
      "Epoch 349:\n",
      "Loss train 0.28794762522131206 valid 0.07918943115239994\n",
      "Epoch 350:\n",
      "Loss train 0.3023505514197051 valid 0.11931229234207587\n",
      "Epoch 351:\n",
      "Loss train 0.3085715738274157 valid 0.11806318960748168\n",
      "Epoch 352:\n",
      "Loss train 0.2829190111652017 valid 0.24348967704066807\n",
      "Epoch 353:\n",
      "Loss train 0.2930507936105132 valid 0.7450804852442781\n",
      "Epoch 354:\n",
      "Loss train 0.3018686923701316 valid 0.27367599721323815\n",
      "Epoch 355:\n",
      "Loss train 0.2953933300614357 valid 0.06467135003802428\n",
      "Epoch 356:\n",
      "Loss train 0.3088337108582258 valid 0.044665426674658316\n",
      "Epoch 357:\n",
      "Loss train 0.2771645969629288 valid 0.060453110111989276\n",
      "Epoch 358:\n",
      "Loss train 0.2897694654103369 valid 0.04556117081032704\n",
      "Epoch 359:\n",
      "Loss train 0.28259233138114215 valid 0.7025314090442478\n",
      "Epoch 360:\n",
      "Loss train 0.28892943934239446 valid 0.0568024373220759\n",
      "Epoch 361:\n",
      "Loss train 0.28747479700781403 valid 0.06780247825816074\n",
      "Epoch 362:\n",
      "Loss train 0.2982072571121156 valid 0.21900210697753633\n",
      "Epoch 363:\n",
      "Loss train 0.2819779219597578 valid 1.5200557149894685\n",
      "Epoch 364:\n",
      "Loss train 0.26950673022046684 valid 0.11396895243342552\n",
      "Epoch 365:\n",
      "Loss train 0.2908714325584471 valid 0.09680200351172957\n",
      "Epoch 366:\n",
      "Loss train 0.28714740694388746 valid 0.04854941776663763\n",
      "Epoch 367:\n",
      "Loss train 0.2741354102872312 valid 0.11064687572002865\n",
      "Epoch 368:\n",
      "Loss train 0.2952438408214599 valid 0.15311701745373324\n",
      "Epoch 369:\n",
      "Loss train 0.3071093077994883 valid 0.04552124250021939\n",
      "Epoch 370:\n",
      "Loss train 0.2688079775586724 valid 0.2891803939249853\n",
      "Epoch 371:\n",
      "Loss train 0.2725050102442503 valid 0.2664286609839261\n",
      "Epoch 372:\n",
      "Loss train 0.2762235439568758 valid 0.08060160124148928\n",
      "Epoch 373:\n",
      "Loss train 0.2889071211449802 valid 0.2450588767957427\n",
      "Epoch 374:\n",
      "Loss train 0.27532876196429135 valid 0.06393921028514746\n",
      "Epoch 375:\n",
      "Loss train 0.303080985782668 valid 0.2238480467077196\n",
      "Epoch 376:\n",
      "Loss train 0.2956412335418165 valid 0.06278057909461562\n",
      "Epoch 377:\n",
      "Loss train 0.2714217463128269 valid 0.36465373273650764\n",
      "Epoch 378:\n",
      "Loss train 0.261866877547279 valid 0.12056019127411113\n",
      "Epoch 379:\n",
      "Loss train 0.29466618727818134 valid 0.06476420961171939\n",
      "Epoch 380:\n",
      "Loss train 0.2686415796451271 valid 0.44886680440827587\n",
      "Epoch 381:\n",
      "Loss train 0.279734781492874 valid 0.15736445650891454\n",
      "Epoch 382:\n",
      "Loss train 0.27880702359341086 valid 0.09256212100358042\n",
      "Epoch 383:\n",
      "Loss train 0.2868212707154453 valid 0.11367890095405057\n",
      "Epoch 384:\n",
      "Loss train 0.2635250518403947 valid 0.0822776125510052\n",
      "Epoch 385:\n",
      "Loss train 0.2712147461369634 valid 0.11624617477532066\n",
      "Epoch 386:\n",
      "Loss train 0.30242601260989904 valid 0.7282641572630061\n",
      "Epoch 387:\n",
      "Loss train 0.2619717663090676 valid 1.7788858629502833\n",
      "Epoch 388:\n",
      "Loss train 0.2781553433198482 valid 0.22832409244828253\n",
      "Epoch 389:\n",
      "Loss train 0.28957996311336753 valid 0.045194168523857776\n",
      "Epoch 390:\n",
      "Loss train 0.2580694081351161 valid 0.06817780825769493\n",
      "Epoch 391:\n",
      "Loss train 0.28884326828941703 valid 0.08445697301849799\n",
      "Epoch 392:\n",
      "Loss train 0.25712511400207877 valid 0.23611738908960567\n",
      "Epoch 393:\n",
      "Loss train 0.2848064181104302 valid 0.0463171248969975\n",
      "Epoch 394:\n",
      "Loss train 0.27403470214009285 valid 0.05743609145047289\n",
      "Epoch 395:\n",
      "Loss train 0.2719287027195096 valid 0.5092035717845516\n",
      "Epoch 396:\n",
      "Loss train 0.2769905946377665 valid 0.07043944538221918\n",
      "Epoch 397:\n",
      "Loss train 0.2799121376380324 valid 0.6236172636485928\n",
      "Epoch 398:\n",
      "Loss train 0.29009913259521125 valid 0.042214380396323796\n",
      "Epoch 399:\n",
      "Loss train 0.2621563882295042 valid 0.11068845457427719\n",
      "Epoch 400:\n",
      "Loss train 0.27782706950716674 valid 0.04272535689856673\n",
      "Epoch 401:\n",
      "Loss train 0.2738617404744029 valid 3.2619535428802418\n",
      "Epoch 402:\n",
      "Loss train 0.2711526934929192 valid 0.04279568491425312\n",
      "Epoch 403:\n",
      "Loss train 0.262875266122818 valid 0.0680665073638075\n",
      "Epoch 404:\n",
      "Loss train 0.26952282326482235 valid 0.06366029191425203\n",
      "Epoch 405:\n",
      "Loss train 0.2764833306610584 valid 0.0988455696902489\n",
      "Epoch 406:\n",
      "Loss train 0.26194490073882043 valid 0.14553286012574296\n",
      "Epoch 407:\n",
      "Loss train 0.2830706294167787 valid 0.10752233387890339\n",
      "Epoch 408:\n",
      "Loss train 0.25225063449703156 valid 0.03550321801418917\n",
      "Epoch 409:\n",
      "Loss train 0.26995986783877013 valid 0.12278785571681215\n",
      "Epoch 410:\n",
      "Loss train 0.2873912329018116 valid 0.21817228401078237\n",
      "Epoch 411:\n",
      "Loss train 0.2581682644337416 valid 0.22963236605967574\n",
      "Epoch 412:\n",
      "Loss train 0.27801706899479034 valid 0.050887185137336664\n",
      "Epoch 413:\n",
      "Loss train 0.2645769990839064 valid 0.2182231165057064\n",
      "Epoch 414:\n",
      "Loss train 0.2770877362649888 valid 0.0924465293672964\n",
      "Epoch 415:\n",
      "Loss train 0.2893991383295506 valid 0.0702582958310075\n",
      "Epoch 416:\n",
      "Loss train 0.2553932318206876 valid 3.4657240180642974\n",
      "Epoch 417:\n",
      "Loss train 0.25570193465054036 valid 0.14789567291762032\n",
      "Epoch 418:\n",
      "Loss train 0.28265118339434264 valid 0.04003000097887983\n",
      "Epoch 419:\n",
      "Loss train 0.2633882566589862 valid 0.049192077262852005\n",
      "Epoch 420:\n",
      "Loss train 0.26157053169645367 valid 0.05267410387743126\n",
      "Epoch 421:\n",
      "Loss train 0.2663280940309167 valid 0.2783324953745104\n",
      "Epoch 422:\n",
      "Loss train 0.2690981198936701 valid 0.10328256281860027\n",
      "Epoch 423:\n",
      "Loss train 0.2639181042037904 valid 0.046038091531284006\n",
      "Epoch 424:\n",
      "Loss train 0.2682517765313387 valid 3.1099180906432067\n",
      "Epoch 425:\n",
      "Loss train 0.2604886411912739 valid 0.2628443104962871\n",
      "Epoch 426:\n",
      "Loss train 0.2672268733434379 valid 0.08601128864608494\n",
      "Epoch 427:\n",
      "Loss train 0.26248840057440104 valid 0.9020928024409908\n",
      "Epoch 428:\n",
      "Loss train 0.26106652102209627 valid 0.050131022423305575\n",
      "Epoch 429:\n",
      "Loss train 0.26621836308538915 valid 0.06463561927339592\n",
      "Epoch 430:\n",
      "Loss train 0.26783436134532096 valid 0.04685411129791301\n",
      "Epoch 431:\n",
      "Loss train 0.2697629549924284 valid 0.09332207441143885\n",
      "Epoch 432:\n",
      "Loss train 0.2607601545687765 valid 0.04168908392494276\n",
      "Epoch 433:\n",
      "Loss train 0.26932601035237314 valid 0.05929230546096429\n",
      "Epoch 434:\n",
      "Loss train 0.24576678765192628 valid 0.11663448758783532\n",
      "Epoch 435:\n",
      "Loss train 0.25672073004208507 valid 0.0896920017325009\n",
      "Epoch 436:\n",
      "Loss train 0.2628527333140373 valid 0.11731493526075225\n",
      "Epoch 437:\n",
      "Loss train 0.2540751418132335 valid 0.07435091977608774\n",
      "Epoch 438:\n",
      "Loss train 0.2516202335909009 valid 1.2027746828686332\n",
      "Epoch 439:\n",
      "Loss train 0.26680653175376357 valid 0.08843592987270053\n",
      "Epoch 440:\n",
      "Loss train 0.2664885463580489 valid 0.19069280898458674\n",
      "Epoch 441:\n",
      "Loss train 0.2459220541253686 valid 0.0979332310859403\n",
      "Epoch 442:\n",
      "Loss train 0.26608654950782656 valid 0.19391693337016083\n",
      "Epoch 443:\n",
      "Loss train 0.2724190081961453 valid 0.08163371495587388\n",
      "Epoch 444:\n",
      "Loss train 0.24476361748576164 valid 0.04147035394124025\n",
      "Epoch 445:\n",
      "Loss train 0.2453073145892471 valid 0.05658895076579608\n",
      "Epoch 446:\n",
      "Loss train 0.25656777456253765 valid 0.05794140343210369\n",
      "Epoch 447:\n",
      "Loss train 0.2482542356994003 valid 0.06464412932853875\n",
      "Epoch 448:\n",
      "Loss train 0.2593195403780788 valid 0.18833441813485904\n",
      "Epoch 449:\n",
      "Loss train 0.27531545525602996 valid 0.19226052665827426\n",
      "Epoch 450:\n",
      "Loss train 0.25162837382145226 valid 0.15807461675788054\n",
      "Epoch 451:\n",
      "Loss train 0.25957803093902765 valid 0.2483222101980809\n",
      "Epoch 452:\n",
      "Loss train 0.24181076794601977 valid 0.050537049656939226\n",
      "Epoch 453:\n",
      "Loss train 0.2558078589946032 valid 0.4905413733507934\n",
      "Epoch 454:\n",
      "Loss train 0.2753860695216805 valid 0.06625779457201712\n",
      "Epoch 455:\n",
      "Loss train 0.25149029369875786 valid 0.04950100701122501\n",
      "Epoch 456:\n",
      "Loss train 0.2649125948380679 valid 1.168850373710623\n",
      "Epoch 457:\n",
      "Loss train 0.2341553613767028 valid 2.557086900633106\n",
      "Epoch 458:\n",
      "Loss train 0.24688720485419036 valid 0.04668810321181794\n",
      "Epoch 459:\n",
      "Loss train 0.2470107437375933 valid 0.8054571535874193\n",
      "Epoch 460:\n",
      "Loss train 0.27488766508474943 valid 0.06153114745906928\n",
      "Epoch 461:\n",
      "Loss train 0.22886693101376296 valid 0.05703958082930173\n",
      "Epoch 462:\n",
      "Loss train 0.2603318317577243 valid 0.045241154401888965\n",
      "Epoch 463:\n",
      "Loss train 0.24776195373311638 valid 0.060187302277079656\n",
      "Epoch 464:\n",
      "Loss train 0.2544906493879855 valid 0.6534573879910643\n",
      "Epoch 465:\n",
      "Loss train 0.24403667720071973 valid 4.003815259463176\n",
      "Epoch 466:\n",
      "Loss train 0.24243463933803142 valid 0.18598633795891334\n",
      "Epoch 467:\n",
      "Loss train 0.23492320376746356 valid 0.05235627093404305\n",
      "Epoch 468:\n",
      "Loss train 0.2535991655781865 valid 0.07161115349329679\n",
      "Epoch 469:\n",
      "Loss train 0.2524019913777709 valid 0.05221515938820327\n",
      "Epoch 470:\n",
      "Loss train 0.2399502175860107 valid 0.2360181586637765\n",
      "Epoch 471:\n",
      "Loss train 0.2513564883697778 valid 0.08757863120129511\n",
      "Epoch 472:\n",
      "Loss train 0.28049244931787254 valid 0.04577793766640085\n",
      "Epoch 473:\n",
      "Loss train 0.23221161189526318 valid 0.09195065392553466\n",
      "Epoch 474:\n",
      "Loss train 0.24176382415890693 valid 0.2066539663063503\n",
      "Epoch 475:\n",
      "Loss train 0.2727027462929487 valid 0.03975474700009317\n",
      "Epoch 476:\n",
      "Loss train 0.25670459344014523 valid 0.04322620248745167\n",
      "Epoch 477:\n",
      "Loss train 0.24107465141490103 valid 0.05957163867759201\n",
      "Epoch 478:\n",
      "Loss train 0.25632867865748704 valid 0.10964948800121445\n",
      "Epoch 479:\n",
      "Loss train 0.2613867131575942 valid 0.055316137451903\n",
      "Epoch 480:\n",
      "Loss train 0.24976002070195974 valid 0.034039126970137557\n",
      "Epoch 481:\n",
      "Loss train 0.23811033228188752 valid 0.05840087835070765\n",
      "Epoch 482:\n",
      "Loss train 0.24552563168480993 valid 0.09033833917231504\n",
      "Epoch 483:\n",
      "Loss train 0.24349317464642226 valid 0.22346183659774163\n",
      "Epoch 484:\n",
      "Loss train 0.240277945189178 valid 0.41525618028086647\n",
      "Epoch 485:\n",
      "Loss train 0.24330471944063903 valid 0.04882492732118508\n",
      "Epoch 486:\n",
      "Loss train 0.24454492628760635 valid 0.045099570914068784\n",
      "Epoch 487:\n",
      "Loss train 0.24617184078842402 valid 0.10822611861396658\n",
      "Epoch 488:\n",
      "Loss train 0.2616009688824415 valid 0.15831405037661334\n",
      "Epoch 489:\n",
      "Loss train 0.25966845047064124 valid 0.03708138574868941\n",
      "Epoch 490:\n",
      "Loss train 0.22374168778508902 valid 0.050704399128501956\n",
      "Epoch 491:\n",
      "Loss train 0.24976421832665802 valid 1.4001951732677864\n",
      "Epoch 492:\n",
      "Loss train 0.2690330589234829 valid 0.03439607356262086\n",
      "Epoch 493:\n",
      "Loss train 0.23398135897703468 valid 0.04669054661147062\n",
      "Epoch 494:\n",
      "Loss train 0.24271757917255163 valid 0.11109709626751632\n",
      "Epoch 495:\n",
      "Loss train 0.24684134412668646 valid 0.41264233071413947\n",
      "Epoch 496:\n",
      "Loss train 0.23832212309874595 valid 0.053003710137344416\n",
      "Epoch 497:\n",
      "Loss train 0.2578283553183079 valid 0.04826234829367789\n",
      "Epoch 498:\n",
      "Loss train 0.22603235124610366 valid 1.1697457109758207\n",
      "Epoch 499:\n",
      "Loss train 0.23185091348029674 valid 0.0746766954152333\n",
      "Epoch 500:\n",
      "Loss train 0.24632566859610378 valid 0.10167531212854088\n",
      "Epoch 501:\n",
      "Loss train 0.2512545460831374 valid 0.33735138752741056\n",
      "Epoch 502:\n",
      "Loss train 0.2198344702973962 valid 0.2636820303368526\n",
      "Epoch 503:\n",
      "Loss train 0.23570800959393381 valid 0.04102164743671818\n",
      "Epoch 504:\n",
      "Loss train 0.2695876458887011 valid 0.031491378185711724\n",
      "Epoch 505:\n",
      "Loss train 0.24074512050263583 valid 0.0813948348663006\n",
      "Epoch 506:\n",
      "Loss train 0.264832468925789 valid 0.03221324119512421\n",
      "Epoch 507:\n",
      "Loss train 0.21224093478247524 valid 0.250687421441843\n",
      "Epoch 508:\n",
      "Loss train 0.2555022423863411 valid 0.03255744829237089\n",
      "Epoch 509:\n",
      "Loss train 0.2225967515166849 valid 0.30500727269433686\n",
      "Epoch 510:\n",
      "Loss train 0.2403447401933372 valid 0.04754448429276075\n",
      "Epoch 511:\n",
      "Loss train 0.23888177729174495 valid 0.21818501248449773\n",
      "Epoch 512:\n",
      "Loss train 0.246067056985572 valid 0.048992585145212444\n",
      "Epoch 513:\n",
      "Loss train 0.23419397496432065 valid 0.274111698844745\n",
      "Epoch 514:\n",
      "Loss train 0.24431367623247205 valid 0.049626115701088006\n",
      "Epoch 515:\n",
      "Loss train 0.2411160818759352 valid 0.039089238981537655\n",
      "Epoch 516:\n",
      "Loss train 0.2210206704404205 valid 0.1484185658203776\n",
      "Epoch 517:\n",
      "Loss train 0.24127040815390646 valid 0.044049459694697486\n",
      "Epoch 518:\n",
      "Loss train 0.244734248303622 valid 0.3242622698089131\n",
      "Epoch 519:\n",
      "Loss train 0.23199624548479914 valid 0.12348285229923618\n",
      "Epoch 520:\n",
      "Loss train 0.242864267577976 valid 0.11516634220148134\n",
      "Epoch 521:\n",
      "Loss train 0.2357440432306379 valid 0.12698381793276248\n",
      "Epoch 522:\n",
      "Loss train 0.24031217721812426 valid 0.0834655004833232\n",
      "Epoch 523:\n",
      "Loss train 0.2504228646080941 valid 0.04808939203513083\n",
      "Epoch 524:\n",
      "Loss train 0.21841240424998104 valid 0.03222358526906643\n",
      "Epoch 525:\n",
      "Loss train 0.23265439776591956 valid 0.030081982540738315\n",
      "Epoch 526:\n",
      "Loss train 0.2263770112171769 valid 0.22419732351803098\n",
      "Epoch 527:\n",
      "Loss train 0.23603693619444965 valid 0.05939495634833659\n",
      "Epoch 528:\n",
      "Loss train 0.2365259804069996 valid 0.03260435815392078\n",
      "Epoch 529:\n",
      "Loss train 0.22457827608399092 valid 0.07040223724437068\n",
      "Epoch 530:\n",
      "Loss train 0.23703673571161926 valid 0.04890863469485396\n",
      "Epoch 531:\n",
      "Loss train 0.2401830478068441 valid 0.043984942924745085\n",
      "Epoch 532:\n",
      "Loss train 0.22892085795663297 valid 0.32441155437081354\n",
      "Epoch 533:\n",
      "Loss train 0.23828529440946877 valid 0.2079895904508893\n",
      "Epoch 534:\n",
      "Loss train 0.22795080695338546 valid 0.03308078620330538\n",
      "Epoch 535:\n",
      "Loss train 0.2304140519477427 valid 0.0911843440461432\n",
      "Epoch 536:\n",
      "Loss train 0.22110504867434502 valid 0.045682993003568456\n",
      "Epoch 537:\n",
      "Loss train 0.23637250957936048 valid 0.06517100107161582\n",
      "Epoch 538:\n",
      "Loss train 0.21953699111789465 valid 0.054780373167598685\n",
      "Epoch 539:\n",
      "Loss train 0.2332150313768536 valid 0.05326354608613927\n",
      "Epoch 540:\n",
      "Loss train 0.23421451716832817 valid 0.0834077218140928\n",
      "Epoch 541:\n",
      "Loss train 0.23262103762663902 valid 0.04314094544905558\n",
      "Epoch 542:\n",
      "Loss train 0.2446718256752938 valid 0.02561542418464568\n",
      "Epoch 543:\n",
      "Loss train 0.23141197247505188 valid 0.028289127146726472\n",
      "Epoch 544:\n",
      "Loss train 0.22416780662201344 valid 0.08606526549145872\n",
      "Epoch 545:\n",
      "Loss train 0.2344245252262801 valid 0.06233654288465933\n",
      "Epoch 546:\n",
      "Loss train 0.220556661798805 valid 0.3377086867882685\n",
      "Epoch 547:\n",
      "Loss train 0.23422253610678018 valid 0.037737616018003635\n",
      "Epoch 548:\n",
      "Loss train 0.22974059570245445 valid 0.06321484766715953\n",
      "Epoch 549:\n",
      "Loss train 0.22453881316930055 valid 0.03170806524323556\n",
      "Epoch 550:\n",
      "Loss train 0.22814595840834082 valid 0.06101747515502376\n",
      "Epoch 551:\n",
      "Loss train 0.22046757589951158 valid 0.07131044420006813\n",
      "Epoch 552:\n",
      "Loss train 0.22934149359948933 valid 0.13922065607600528\n",
      "Epoch 553:\n",
      "Loss train 0.2533220457408577 valid 0.03741412487647183\n",
      "Epoch 554:\n",
      "Loss train 0.2224286497808993 valid 0.08242602290566534\n",
      "Epoch 555:\n",
      "Loss train 0.21071508585959672 valid 0.04215396149460996\n",
      "Epoch 556:\n",
      "Loss train 0.21667923742830753 valid 0.04303116480180578\n",
      "Epoch 557:\n",
      "Loss train 0.2255392364885658 valid 0.041996418122986365\n",
      "Epoch 558:\n",
      "Loss train 0.2299852333974093 valid 0.11677877836771161\n",
      "Epoch 559:\n",
      "Loss train 0.22404543619155884 valid 0.15692567923996922\n",
      "Epoch 560:\n",
      "Loss train 0.2327886520911008 valid 0.21867623728889973\n",
      "Epoch 561:\n",
      "Loss train 0.2262025075599551 valid 0.37360393189633484\n",
      "Epoch 562:\n",
      "Loss train 0.22278905111923814 valid 0.05950573468853044\n",
      "Epoch 563:\n",
      "Loss train 0.2323511175725609 valid 3.748497481304726\n",
      "Epoch 564:\n",
      "Loss train 0.2373806367415935 valid 0.11092784215019769\n",
      "Epoch 565:\n",
      "Loss train 0.22738420185334982 valid 0.6109618248387474\n",
      "Epoch 566:\n",
      "Loss train 0.23686547433622182 valid 0.06777637245964854\n",
      "Epoch 567:\n",
      "Loss train 0.22445820864401758 valid 0.19826579125380026\n",
      "Epoch 568:\n",
      "Loss train 0.2247766068674624 valid 0.07584222419704532\n",
      "Epoch 569:\n",
      "Loss train 0.2339030514191836 valid 0.04812346977713227\n",
      "Epoch 570:\n",
      "Loss train 0.20907474055103958 valid 0.09443478105797526\n",
      "Epoch 571:\n",
      "Loss train 0.2474591474648565 valid 0.16113895541198844\n",
      "Epoch 572:\n",
      "Loss train 0.22469056049622596 valid 0.5355348488845002\n",
      "Epoch 573:\n",
      "Loss train 0.22892888924144209 valid 0.14017907075832597\n",
      "Epoch 574:\n",
      "Loss train 0.2228231219973415 valid 0.20678863693001748\n",
      "Epoch 575:\n",
      "Loss train 0.22014619084857404 valid 0.5439962744562916\n",
      "Epoch 576:\n",
      "Loss train 0.2317802782945335 valid 0.03320690560919838\n",
      "Epoch 577:\n",
      "Loss train 0.21821531346738338 valid 0.29975862774572737\n",
      "Epoch 578:\n",
      "Loss train 0.23057524241022764 valid 0.07373579271623253\n",
      "Epoch 579:\n",
      "Loss train 0.23378459453284742 valid 0.02932500551159413\n",
      "Epoch 580:\n",
      "Loss train 0.20397851551137863 valid 0.035027178879236225\n",
      "Epoch 581:\n",
      "Loss train 0.23146546473652124 valid 1.7677928625098016\n",
      "Epoch 582:\n",
      "Loss train 0.2196959372200072 valid 0.0326369782023161\n",
      "Epoch 583:\n",
      "Loss train 0.23279439681433142 valid 0.04908036631393417\n",
      "Epoch 584:\n",
      "Loss train 0.2121654206313193 valid 0.05821462378958605\n",
      "Epoch 585:\n",
      "Loss train 0.2181866849962622 valid 0.04458518129074123\n",
      "Epoch 586:\n",
      "Loss train 0.22130925839059054 valid 1.1493049586634456\n",
      "Epoch 587:\n",
      "Loss train 0.22465662561282515 valid 0.32209124466855116\n",
      "Epoch 588:\n",
      "Loss train 0.21615362720787526 valid 0.11979484615649087\n",
      "Epoch 589:\n",
      "Loss train 0.22916381048485637 valid 0.4382492743500809\n",
      "Epoch 590:\n",
      "Loss train 0.23698464856408535 valid 0.03272413204134924\n",
      "Epoch 591:\n",
      "Loss train 0.20556128358952702 valid 0.07471241249049233\n",
      "Epoch 592:\n",
      "Loss train 0.2246030658815056 valid 0.045596573446205726\n",
      "Epoch 593:\n",
      "Loss train 0.22485565988905729 valid 0.0586944079064368\n",
      "Epoch 594:\n",
      "Loss train 0.24779655158892275 valid 0.8261021192468312\n",
      "Epoch 595:\n",
      "Loss train 0.21238404307216405 valid 0.041533481033690936\n",
      "Epoch 596:\n",
      "Loss train 0.21268493794724344 valid 0.07864211303394379\n",
      "Epoch 597:\n",
      "Loss train 0.22909329737052322 valid 0.05000123900058932\n",
      "Epoch 598:\n",
      "Loss train 0.23975651337876916 valid 0.05678461968882057\n",
      "Epoch 599:\n",
      "Loss train 0.22505368681661786 valid 0.05006819473828456\n",
      "Epoch 600:\n",
      "Loss train 0.21832804152816535 valid 0.06629706203915676\n",
      "Epoch 601:\n",
      "Loss train 0.20701260887086392 valid 0.20406907578216824\n",
      "Epoch 602:\n",
      "Loss train 0.21638922287747264 valid 0.029084204614141053\n",
      "Epoch 603:\n",
      "Loss train 0.22429385696724058 valid 0.02838248766482472\n",
      "Epoch 604:\n",
      "Loss train 0.22249019541107118 valid 0.04143954755455724\n",
      "Epoch 605:\n",
      "Loss train 0.21283151678442955 valid 0.16265370206170257\n",
      "Epoch 606:\n",
      "Loss train 0.2236913061529398 valid 0.03506377860461389\n",
      "Epoch 607:\n",
      "Loss train 0.22073399963490664 valid 0.10979247665496004\n",
      "Epoch 608:\n",
      "Loss train 0.22014772166088223 valid 0.10955229904077836\n",
      "Epoch 609:\n",
      "Loss train 0.2192862414494157 valid 0.02401491554855863\n",
      "Epoch 610:\n",
      "Loss train 0.21937434531338512 valid 0.5802391777495278\n",
      "Epoch 611:\n",
      "Loss train 0.22131869562268258 valid 0.059224832481127074\n",
      "Epoch 612:\n",
      "Loss train 0.2205974089719355 valid 0.042784580484976305\n",
      "Epoch 613:\n",
      "Loss train 0.21192958628199995 valid 0.03750303091657853\n",
      "Epoch 614:\n",
      "Loss train 0.22414162234067916 valid 0.4731974473218175\n",
      "Epoch 615:\n",
      "Loss train 0.21245939516089857 valid 0.03782755660260592\n",
      "Epoch 616:\n",
      "Loss train 0.21070086255855858 valid 0.046797062995777565\n",
      "Epoch 617:\n",
      "Loss train 0.22101595171913505 valid 0.11258418527545772\n",
      "Epoch 618:\n",
      "Loss train 0.20911529206112028 valid 0.3948424768346994\n",
      "Epoch 619:\n",
      "Loss train 0.21028167722560465 valid 0.8919591045901903\n",
      "Epoch 620:\n",
      "Loss train 0.21321399520598353 valid 0.09693024691236998\n",
      "Epoch 621:\n",
      "Loss train 0.2085426485840231 valid 0.27238126072393626\n",
      "Epoch 622:\n",
      "Loss train 0.22520089396759868 valid 0.22356466668624994\n",
      "Epoch 623:\n",
      "Loss train 0.1987779319319874 valid 0.05957336701928935\n",
      "Epoch 624:\n",
      "Loss train 0.2149583771519363 valid 0.3184435176918484\n",
      "Epoch 625:\n",
      "Loss train 0.2123773812290281 valid 0.047576010271529336\n",
      "Epoch 626:\n",
      "Loss train 0.21109922495409847 valid 0.033196045434412166\n",
      "Epoch 627:\n",
      "Loss train 0.22338797102235258 valid 0.02705175163190349\n",
      "Epoch 628:\n",
      "Loss train 0.20656506663039326 valid 0.26089650371829\n",
      "Epoch 629:\n",
      "Loss train 0.19961056053340434 valid 0.09038637271735851\n",
      "Epoch 630:\n",
      "Loss train 0.2243852319329977 valid 0.038205054116112004\n",
      "Epoch 631:\n",
      "Loss train 0.21167274202667177 valid 0.044382781435384676\n",
      "Epoch 632:\n",
      "Loss train 0.22103555657230317 valid 0.03295341989831663\n",
      "Epoch 633:\n",
      "Loss train 0.20782567293718457 valid 0.08240808588178251\n",
      "Epoch 634:\n",
      "Loss train 0.20504686210863293 valid 0.23018153169005684\n",
      "Epoch 635:\n",
      "Loss train 0.20505133984982968 valid 0.1760804968971578\n",
      "Epoch 636:\n",
      "Loss train 0.21051688742935656 valid 0.04546961114234771\n",
      "Epoch 637:\n",
      "Loss train 0.22926262447945775 valid 0.9779081342524699\n",
      "Epoch 638:\n",
      "Loss train 0.21797505357973276 valid 0.07235315780998314\n",
      "Epoch 639:\n",
      "Loss train 0.2199924067195505 valid 0.6855103683263927\n",
      "Epoch 640:\n",
      "Loss train 0.20545624460689724 valid 0.06265447160361111\n",
      "Epoch 641:\n",
      "Loss train 0.20602853718400002 valid 0.6335140794234332\n",
      "Epoch 642:\n",
      "Loss train 0.21126749553829433 valid 0.03643807811469028\n",
      "Epoch 643:\n",
      "Loss train 0.2215190715394914 valid 0.023514848961187582\n",
      "Epoch 644:\n",
      "Loss train 0.19186689768321813 valid 0.10224695392629728\n",
      "Epoch 645:\n",
      "Loss train 0.21146201443746687 valid 0.03706392338704966\n",
      "Epoch 646:\n",
      "Loss train 0.207102657488361 valid 0.10736057326651532\n",
      "Epoch 647:\n",
      "Loss train 0.21276358077563345 valid 0.7077847962030285\n",
      "Epoch 648:\n",
      "Loss train 0.19768829421103 valid 0.0538251859167028\n",
      "Epoch 649:\n",
      "Loss train 0.21095739185921847 valid 0.15013192867021036\n",
      "Epoch 650:\n",
      "Loss train 0.21081764423698188 valid 0.027478359343306655\n",
      "Epoch 651:\n",
      "Loss train 0.20379791535399855 valid 0.03568025901114918\n",
      "Epoch 652:\n",
      "Loss train 0.203634750630334 valid 0.4970977673007338\n",
      "Epoch 653:\n",
      "Loss train 0.20331462180167437 valid 0.044845275391279825\n",
      "Epoch 654:\n",
      "Loss train 0.21062363960780203 valid 0.039905689061301444\n",
      "Epoch 655:\n",
      "Loss train 0.2172117677498609 valid 0.04076079631575865\n",
      "Epoch 656:\n",
      "Loss train 0.20701706069670617 valid 0.050116427785606064\n",
      "Epoch 657:\n",
      "Loss train 0.20571631038188934 valid 1.0187838845124286\n",
      "Epoch 658:\n",
      "Loss train 0.2132424949452281 valid 0.17633901980765238\n",
      "Epoch 659:\n",
      "Loss train 0.21350080433413388 valid 0.030518946530307373\n",
      "Epoch 660:\n",
      "Loss train 0.20613598517775536 valid 0.04790742944997106\n",
      "Epoch 661:\n",
      "Loss train 0.204124287211895 valid 0.10016511630924854\n",
      "Epoch 662:\n",
      "Loss train 0.19730330011546612 valid 0.24265198182662476\n",
      "Epoch 663:\n",
      "Loss train 0.20170080726668238 valid 0.11968546789108193\n",
      "Epoch 664:\n",
      "Loss train 0.2028620919946581 valid 0.34262927704740936\n",
      "Epoch 665:\n",
      "Loss train 0.20277155199274421 valid 0.22091535683766259\n",
      "Epoch 666:\n",
      "Loss train 0.20188821244090796 valid 0.07590713576953059\n",
      "Epoch 667:\n",
      "Loss train 0.2025516242649406 valid 0.3268165960410661\n",
      "Epoch 668:\n",
      "Loss train 0.2189732810854912 valid 0.035789887453758565\n",
      "Epoch 669:\n",
      "Loss train 0.20760472397878765 valid 0.46295619737082194\n",
      "Epoch 670:\n",
      "Loss train 0.20183707663938402 valid 0.1141754549082233\n",
      "Epoch 671:\n",
      "Loss train 0.2204938271164894 valid 0.036012607857441294\n",
      "Epoch 672:\n",
      "Loss train 0.18758664092384278 valid 0.22993155604421636\n",
      "Epoch 673:\n",
      "Loss train 0.20652628969550132 valid 0.03159480845524269\n",
      "Epoch 674:\n",
      "Loss train 0.19812589437030256 valid 0.09998461128746593\n",
      "Epoch 675:\n",
      "Loss train 0.21678569216653704 valid 0.13618546785837068\n",
      "Epoch 676:\n",
      "Loss train 0.20261821286939086 valid 0.44587775610898844\n",
      "Epoch 677:\n",
      "Loss train 0.20928039930090309 valid 0.10115790162315662\n",
      "Epoch 678:\n",
      "Loss train 0.1975476662710309 valid 0.30864126542104037\n",
      "Epoch 679:\n",
      "Loss train 0.20353674732334912 valid 3.051895471210841\n",
      "Epoch 680:\n",
      "Loss train 0.22009102910235523 valid 0.04053718709574148\n",
      "Epoch 681:\n",
      "Loss train 0.1913167562931776 valid 0.03278996596306471\n",
      "Epoch 682:\n",
      "Loss train 0.2081474570631981 valid 0.3545243708313694\n",
      "Epoch 683:\n",
      "Loss train 0.20790572014898062 valid 0.049345877187180406\n",
      "Epoch 684:\n",
      "Loss train 0.19854987724646925 valid 0.058847235023470154\n",
      "Epoch 685:\n",
      "Loss train 0.20490033995732665 valid 0.034766844507891324\n",
      "Epoch 686:\n",
      "Loss train 0.21145163093544542 valid 1.2042478249059643\n",
      "Epoch 687:\n",
      "Loss train 0.20230314662940801 valid 0.025895941977534732\n",
      "Epoch 688:\n",
      "Loss train 0.20640118413046002 valid 0.03615977988442828\n",
      "Epoch 689:\n",
      "Loss train 0.2017682983417064 valid 0.031161660337785335\n",
      "Epoch 690:\n",
      "Loss train 0.2053442744895816 valid 0.41743369924474294\n",
      "Epoch 691:\n",
      "Loss train 0.19465449207909405 valid 0.0377036300006816\n",
      "Epoch 692:\n",
      "Loss train 0.1983291411407292 valid 0.4911826231856294\n",
      "Epoch 693:\n",
      "Loss train 0.19901913528740406 valid 0.15350724760859308\n",
      "Epoch 694:\n",
      "Loss train 0.20065627285838128 valid 0.10468848676151302\n",
      "Epoch 695:\n",
      "Loss train 0.2212505591608584 valid 0.029856042656858314\n",
      "Epoch 696:\n",
      "Loss train 0.18581214275024832 valid 0.05677168134238354\n",
      "Epoch 697:\n",
      "Loss train 0.1996606510914862 valid 0.040637449157859516\n",
      "Epoch 698:\n",
      "Loss train 0.23752460499312728 valid 0.037467127546070726\n",
      "Epoch 699:\n",
      "Loss train 0.2053204056687653 valid 6.742879011971995\n",
      "Epoch 700:\n",
      "Loss train 0.18006875243224205 valid 0.12454556650612712\n",
      "Epoch 701:\n",
      "Loss train 0.1990421772075817 valid 0.21392350626448933\n",
      "Epoch 702:\n",
      "Loss train 0.19779568955935536 valid 0.032089208328813165\n",
      "Epoch 703:\n",
      "Loss train 0.2233679037157446 valid 0.025370393670832547\n",
      "Epoch 704:\n",
      "Loss train 0.1827842508148402 valid 0.5537585818751452\n",
      "Epoch 705:\n",
      "Loss train 0.19317911602146923 valid 0.07707738641338714\n",
      "Epoch 706:\n",
      "Loss train 0.1953225240457803 valid 0.18470348955870586\n",
      "Epoch 707:\n",
      "Loss train 0.19892339395321906 valid 0.5271358980588114\n",
      "Epoch 708:\n",
      "Loss train 0.20794331911578776 valid 0.47347228680236675\n",
      "Epoch 709:\n",
      "Loss train 0.19014305660203099 valid 0.2512322073762677\n",
      "Epoch 710:\n",
      "Loss train 0.20824054449088872 valid 0.3718134086061952\n",
      "Epoch 711:\n",
      "Loss train 0.19319413328133522 valid 0.02297758493907961\n",
      "Epoch 712:\n",
      "Loss train 0.20301640175394714 valid 0.03989902952232313\n",
      "Epoch 713:\n",
      "Loss train 0.2104428894562647 valid 0.03944386768036687\n",
      "Epoch 714:\n",
      "Loss train 0.1963750505849719 valid 0.09313992461452948\n",
      "Epoch 715:\n",
      "Loss train 0.18812956368923187 valid 0.030424293074775018\n",
      "Epoch 716:\n",
      "Loss train 0.21506165888272225 valid 0.14047457307971453\n",
      "Epoch 717:\n",
      "Loss train 0.18837890526764095 valid 0.14335751175288833\n",
      "Epoch 718:\n",
      "Loss train 0.1872179182773456 valid 0.02618489245965951\n",
      "Epoch 719:\n",
      "Loss train 0.20233925208821893 valid 0.03514104707443187\n",
      "Epoch 720:\n",
      "Loss train 0.1990202368915081 valid 0.09636207658950724\n",
      "Epoch 721:\n",
      "Loss train 0.19500890905559062 valid 0.0384805276073508\n",
      "Epoch 722:\n",
      "Loss train 0.2022707979079336 valid 0.04817234705074776\n",
      "Epoch 723:\n",
      "Loss train 0.1872847441583872 valid 0.1383256654983784\n",
      "Epoch 724:\n",
      "Loss train 0.1992264026824385 valid 0.040722128496921285\n",
      "Epoch 725:\n",
      "Loss train 0.20154759858585894 valid 0.02774605234449979\n",
      "Epoch 726:\n",
      "Loss train 0.1774016940202564 valid 0.03944532203574479\n",
      "Epoch 727:\n",
      "Loss train 0.21143405514508487 valid 0.024334010680290754\n",
      "Epoch 728:\n",
      "Loss train 0.18449014518857001 valid 0.2198122496168235\n",
      "Epoch 729:\n",
      "Loss train 0.21871944108903407 valid 0.02259252116553081\n",
      "Epoch 730:\n",
      "Loss train 0.17509748506173492 valid 0.5735840239494643\n",
      "Epoch 731:\n",
      "Loss train 0.1974851300597191 valid 0.06836239778451245\n",
      "Epoch 732:\n",
      "Loss train 0.20317644547633826 valid 0.21095229275066588\n",
      "Epoch 733:\n",
      "Loss train 0.19250695787221192 valid 0.27164967651428235\n",
      "Epoch 734:\n",
      "Loss train 0.18828568552173675 valid 0.0650430063350856\n",
      "Epoch 735:\n",
      "Loss train 0.2069661007564515 valid 0.028150298962803542\n",
      "Epoch 736:\n",
      "Loss train 0.18586169016584753 valid 0.5738618831942114\n",
      "Epoch 737:\n",
      "Loss train 0.20599781826101243 valid 0.07803678296761156\n",
      "Epoch 738:\n",
      "Loss train 0.19503684468865395 valid 0.03120737175178496\n",
      "Epoch 739:\n",
      "Loss train 0.1993965014776215 valid 0.05117076745449178\n",
      "Epoch 740:\n",
      "Loss train 0.1918396413754672 valid 0.0318998596322155\n",
      "Epoch 741:\n",
      "Loss train 0.19161984221283346 valid 0.19437192023470315\n",
      "Epoch 742:\n",
      "Loss train 0.18744507352337242 valid 0.9111546662343148\n",
      "Epoch 743:\n",
      "Loss train 0.20461226298213006 valid 0.02637037463697506\n",
      "Epoch 744:\n",
      "Loss train 0.20024673723708838 valid 0.03961886769288715\n",
      "Epoch 745:\n",
      "Loss train 0.18804956691749394 valid 0.03655528389915753\n",
      "Epoch 746:\n",
      "Loss train 0.1927126812234521 valid 0.13823552895929614\n",
      "Epoch 747:\n",
      "Loss train 0.21702844563499094 valid 0.040035805694725576\n",
      "Epoch 748:\n",
      "Loss train 0.18670948579087854 valid 0.03120994160785226\n",
      "Epoch 749:\n",
      "Loss train 0.18826334936711936 valid 0.15840048089508077\n",
      "Epoch 750:\n",
      "Loss train 0.19148492973744868 valid 0.3560588570993494\n",
      "Epoch 751:\n",
      "Loss train 0.18900186831839383 valid 0.094484400066319\n",
      "Epoch 752:\n",
      "Loss train 0.19935533487685025 valid 0.20342403020674724\n",
      "Epoch 753:\n",
      "Loss train 0.18865045165922492 valid 0.021271746798803722\n",
      "Epoch 754:\n",
      "Loss train 0.200926372901164 valid 0.5414583404305336\n",
      "Epoch 755:\n",
      "Loss train 0.19397468937449158 valid 0.024430381676128217\n",
      "Epoch 756:\n",
      "Loss train 0.20478708807267249 valid 0.02838523137739622\n",
      "Epoch 757:\n",
      "Loss train 0.18920467077828942 valid 0.04420607126290284\n",
      "Epoch 758:\n",
      "Loss train 0.1948874677889049 valid 0.05000301572611976\n",
      "Epoch 759:\n",
      "Loss train 0.19156615769863128 valid 0.09592900243410765\n",
      "Epoch 760:\n",
      "Loss train 0.2002763424757868 valid 0.6069769212556303\n",
      "Epoch 761:\n",
      "Loss train 0.18039620234183967 valid 0.049037427337247745\n",
      "Epoch 762:\n",
      "Loss train 0.20867328750267625 valid 0.05494404365278777\n",
      "Epoch 763:\n",
      "Loss train 0.20097585207372903 valid 0.04937150653601744\n",
      "Epoch 764:\n",
      "Loss train 0.18861195887252688 valid 4.110800811453084\n",
      "Epoch 765:\n",
      "Loss train 0.18024139051660895 valid 0.20747701609984942\n",
      "Epoch 766:\n",
      "Loss train 0.19967834983151406 valid 0.022684716332178934\n",
      "Epoch 767:\n",
      "Loss train 0.19130123590957374 valid 1.3193011310755158\n",
      "Epoch 768:\n",
      "Loss train 0.1757543268075213 valid 0.049527397051230194\n",
      "Epoch 769:\n",
      "Loss train 0.19144303652830422 valid 0.025699437146876083\n",
      "Epoch 770:\n",
      "Loss train 0.1823777949243784 valid 0.21954806133240293\n",
      "Epoch 771:\n",
      "Loss train 0.18351410906966775 valid 0.8299137656076739\n",
      "Epoch 772:\n",
      "Loss train 0.18396333495005965 valid 0.11015776067935167\n",
      "Epoch 773:\n",
      "Loss train 0.204292416879721 valid 0.02862151426974954\n",
      "Epoch 774:\n",
      "Loss train 0.20477979082856326 valid 0.02223059933975211\n",
      "Epoch 775:\n",
      "Loss train 0.17817269979119302 valid 0.055890938892336324\n",
      "Epoch 776:\n",
      "Loss train 0.21641276800688355 valid 0.032587970152517014\n",
      "Epoch 777:\n",
      "Loss train 0.16553076894506813 valid 0.047745348763836196\n",
      "Epoch 778:\n",
      "Loss train 0.18370015754625202 valid 0.044569077729321824\n",
      "Epoch 779:\n",
      "Loss train 0.18629839252717792 valid 0.10957246446129601\n",
      "Epoch 780:\n",
      "Loss train 0.20127499608024954 valid 0.10151910089929606\n",
      "Epoch 781:\n",
      "Loss train 0.17895105332843958 valid 0.09556623192914139\n",
      "Epoch 782:\n",
      "Loss train 0.21071491420455277 valid 0.03851573169585463\n",
      "Epoch 783:\n",
      "Loss train 0.17633301429133863 valid 0.039171943008761166\n",
      "Epoch 784:\n",
      "Loss train 0.1843263733148575 valid 0.18024439941301557\n",
      "Epoch 785:\n",
      "Loss train 0.19823504552077503 valid 0.04913628004489698\n",
      "Epoch 786:\n",
      "Loss train 0.19396345545463264 valid 0.04561238018989909\n",
      "Epoch 787:\n",
      "Loss train 0.19142933843620122 valid 0.08846659546781496\n",
      "Epoch 788:\n",
      "Loss train 0.17325297653265298 valid 0.03499215859616899\n",
      "Epoch 789:\n",
      "Loss train 0.196625617752783 valid 0.06001368758531477\n",
      "Epoch 790:\n",
      "Loss train 0.18258895044960083 valid 0.06452289452603543\n",
      "Epoch 791:\n",
      "Loss train 0.18547332630641758 valid 0.025459329376408085\n",
      "Epoch 792:\n",
      "Loss train 0.186515650241822 valid 0.020079105227497382\n",
      "Epoch 793:\n",
      "Loss train 0.18918328602071852 valid 0.04090701810868285\n",
      "Epoch 794:\n",
      "Loss train 0.18566332980170847 valid 0.186150954595648\n",
      "Epoch 795:\n",
      "Loss train 0.20537373785246163 valid 0.02590329437649671\n",
      "Epoch 796:\n",
      "Loss train 0.185001628466323 valid 0.027958947639728428\n",
      "Epoch 797:\n",
      "Loss train 0.2016255921330303 valid 0.017825490432543317\n",
      "Epoch 798:\n",
      "Loss train 0.17345038745310157 valid 0.03863045335390705\n",
      "Epoch 799:\n",
      "Loss train 0.19461220658905803 valid 0.07141026798745544\n",
      "Epoch 800:\n",
      "Loss train 0.17933577626235783 valid 0.11735045212625543\n",
      "Epoch 801:\n",
      "Loss train 0.1918801300484687 valid 0.4145893751254404\n",
      "Epoch 802:\n",
      "Loss train 0.17593476660475135 valid 0.08390206133422912\n",
      "Epoch 803:\n",
      "Loss train 0.1810280298454687 valid 0.1264664062571671\n",
      "Epoch 804:\n",
      "Loss train 0.1767725758789107 valid 0.12291554565579592\n",
      "Epoch 805:\n",
      "Loss train 0.19401032723784448 valid 0.47215525423302307\n",
      "Epoch 806:\n",
      "Loss train 0.1827253982156515 valid 0.025933846671337784\n",
      "Epoch 807:\n",
      "Loss train 0.1882967349819839 valid 0.06832735345568737\n",
      "Epoch 808:\n",
      "Loss train 0.1872337473200634 valid 0.2837968464189874\n",
      "Epoch 809:\n",
      "Loss train 0.17903569172825665 valid 0.6912400064668691\n",
      "Epoch 810:\n",
      "Loss train 0.18020672261677684 valid 0.07520851275224132\n",
      "Epoch 811:\n",
      "Loss train 0.1816513132346794 valid 0.03559513691496379\n",
      "Epoch 812:\n",
      "Loss train 0.1781893351720646 valid 0.3646179301953633\n",
      "Epoch 813:\n",
      "Loss train 0.20811253309175373 valid 0.022702903742822963\n",
      "Epoch 814:\n",
      "Loss train 0.1613239523436874 valid 0.029999219704751515\n",
      "Epoch 815:\n",
      "Loss train 0.17796495422273875 valid 0.17201762419221642\n",
      "Epoch 816:\n",
      "Loss train 0.18789960754401983 valid 0.5293055154215044\n",
      "Epoch 817:\n",
      "Loss train 0.20128863890208304 valid 0.4928907357254249\n",
      "Epoch 818:\n",
      "Loss train 0.18317396698761731 valid 0.12942364962806588\n",
      "Epoch 819:\n",
      "Loss train 0.1862122054014355 valid 0.13322926681086042\n",
      "Epoch 820:\n",
      "Loss train 0.18114926027785988 valid 0.026431412936650057\n",
      "Epoch 821:\n",
      "Loss train 0.18200378815308213 valid 0.04388000467815848\n",
      "Epoch 822:\n",
      "Loss train 0.18524659626297652 valid 0.023317876273533752\n",
      "Epoch 823:\n",
      "Loss train 0.1771964717734605 valid 0.21322547445997395\n",
      "Epoch 824:\n",
      "Loss train 0.18703703767824917 valid 0.028323371019168143\n",
      "Epoch 825:\n",
      "Loss train 0.17975319857914002 valid 0.04723511521024207\n",
      "Epoch 826:\n",
      "Loss train 0.18448070241808892 valid 0.33268274415269006\n",
      "Epoch 827:\n",
      "Loss train 0.1740329179879278 valid 0.08995433494713344\n",
      "Epoch 828:\n",
      "Loss train 0.18054890632051976 valid 0.022666667103411146\n",
      "Epoch 829:\n",
      "Loss train 0.17342167163509875 valid 0.05579203681677809\n",
      "Epoch 830:\n",
      "Loss train 0.17533631955944 valid 0.1117066764257531\n",
      "Epoch 831:\n",
      "Loss train 0.1807813369853422 valid 0.06399808287450873\n",
      "Epoch 832:\n",
      "Loss train 0.1739386832240969 valid 0.5561884774635647\n",
      "Epoch 833:\n",
      "Loss train 0.18698271626811475 valid 0.5747348941401832\n",
      "Epoch 834:\n",
      "Loss train 0.2037436635805294 valid 0.03132656651967204\n",
      "Epoch 835:\n",
      "Loss train 0.16956098663378508 valid 0.054427609403026565\n",
      "Epoch 836:\n",
      "Loss train 0.1765670436091721 valid 0.05475599482155751\n",
      "Epoch 837:\n",
      "Loss train 0.18272858699113131 valid 0.12402027406720659\n",
      "Epoch 838:\n",
      "Loss train 0.18316749029066415 valid 0.06555815441049938\n",
      "Epoch 839:\n",
      "Loss train 0.1851417524356395 valid 0.0667731241385182\n",
      "Epoch 840:\n",
      "Loss train 0.17246083630193024 valid 0.1172913175787295\n",
      "Epoch 841:\n",
      "Loss train 0.1810548700781539 valid 0.028088988377327432\n",
      "Epoch 842:\n",
      "Loss train 0.19028630401734262 valid 0.07239632805748199\n",
      "Epoch 843:\n",
      "Loss train 0.16180919438824057 valid 0.06674032409850497\n",
      "Epoch 844:\n",
      "Loss train 0.18153043086975812 valid 0.0536890699816439\n",
      "Epoch 845:\n",
      "Loss train 0.16941857763733714 valid 0.22287921978472816\n",
      "Epoch 846:\n",
      "Loss train 0.18782034754827617 valid 3.2513861582772563\n",
      "Epoch 847:\n",
      "Loss train 0.1860570357227698 valid 0.06977910481042915\n",
      "Epoch 848:\n",
      "Loss train 0.1768097612319514 valid 0.34038124204199643\n",
      "Epoch 849:\n",
      "Loss train 0.172126008255966 valid 0.07916956058839118\n",
      "Epoch 850:\n",
      "Loss train 0.1789507007393986 valid 0.037156779910934246\n",
      "Epoch 851:\n",
      "Loss train 0.1799452857332304 valid 0.041838953683729294\n",
      "Epoch 852:\n",
      "Loss train 0.17687888904958962 valid 0.11282526632417729\n",
      "Epoch 853:\n",
      "Loss train 0.1834550074500963 valid 0.08073359018869482\n",
      "Epoch 854:\n",
      "Loss train 0.17864432454723864 valid 0.028463845747626764\n",
      "Epoch 855:\n",
      "Loss train 0.18522293291185052 valid 0.02120903893338573\n",
      "Epoch 856:\n",
      "Loss train 0.18640688357334584 valid 0.0414284086333642\n",
      "Epoch 857:\n",
      "Loss train 0.1724668457195163 valid 0.1161234671329394\n",
      "Epoch 858:\n",
      "Loss train 0.17654230614453553 valid 0.049239935384822166\n",
      "Epoch 859:\n",
      "Loss train 0.18115182124450802 valid 0.03563276783120976\n",
      "Epoch 860:\n",
      "Loss train 0.17496652128957213 valid 0.03882443337770177\n",
      "Epoch 861:\n",
      "Loss train 0.18806788917612285 valid 0.035230947601631105\n",
      "Epoch 862:\n",
      "Loss train 0.1773327292151749 valid 0.04331920868435355\n",
      "Epoch 863:\n",
      "Loss train 0.17434484054259955 valid 0.021772041897535663\n",
      "Epoch 864:\n",
      "Loss train 0.17993016780484467 valid 0.024725113311018284\n",
      "Epoch 865:\n",
      "Loss train 0.1700619090259075 valid 0.6005551000606302\n",
      "Epoch 866:\n",
      "Loss train 0.17945166911352425 valid 0.05613331563547979\n",
      "Epoch 867:\n",
      "Loss train 0.17552795818187297 valid 0.06900052888302866\n",
      "Epoch 868:\n",
      "Loss train 0.1722542208738625 valid 0.035110359190470515\n",
      "Epoch 869:\n",
      "Loss train 0.19762640182189645 valid 0.4661331467110632\n",
      "Epoch 870:\n",
      "Loss train 0.16821009750366211 valid 0.0710041612546853\n",
      "Epoch 871:\n",
      "Loss train 0.18501483262274415 valid 0.5043374642813501\n",
      "Epoch 872:\n",
      "Loss train 0.18272522015087306 valid 0.026755384831494717\n",
      "Epoch 873:\n",
      "Loss train 0.19799344455692916 valid 0.18044294254293083\n",
      "Epoch 874:\n",
      "Loss train 0.16842754955627023 valid 0.20391634859754998\n",
      "Epoch 875:\n",
      "Loss train 0.1774637941641733 valid 0.6846044785848298\n",
      "Epoch 876:\n",
      "Loss train 0.16932235343325883 valid 0.03829922970439268\n",
      "Epoch 877:\n",
      "Loss train 0.18438568183425813 valid 0.08265824092325677\n",
      "Epoch 878:\n",
      "Loss train 0.1927028892416507 valid 0.03108377062400011\n",
      "Epoch 879:\n",
      "Loss train 0.19223584740422667 valid 0.533820678942741\n",
      "Epoch 880:\n",
      "Loss train 0.1898049732990563 valid 0.2884660166429093\n",
      "Epoch 881:\n",
      "Loss train 0.17038233367893846 valid 0.017241709923059963\n",
      "Epoch 882:\n",
      "Loss train 0.18735380959417672 valid 0.08610636833510717\n",
      "Epoch 883:\n",
      "Loss train 0.17955179237090052 valid 0.034366425262646105\n",
      "Epoch 884:\n",
      "Loss train 0.1684390414448455 valid 2.3575404093325236\n",
      "Epoch 885:\n",
      "Loss train 0.18860025683697312 valid 0.7444483206709314\n",
      "Epoch 886:\n",
      "Loss train 0.17184523348864167 valid 0.050336066866785094\n",
      "Epoch 887:\n",
      "Loss train 0.16929834397248925 valid 0.04693037862670732\n",
      "Epoch 888:\n",
      "Loss train 0.17762694122679532 valid 0.0931226471579867\n",
      "Epoch 889:\n",
      "Loss train 0.17310546676497907 valid 0.024672613835845476\n",
      "Epoch 890:\n",
      "Loss train 0.17504077338222415 valid 0.02573955561896636\n",
      "Epoch 891:\n",
      "Loss train 0.1716260477403179 valid 0.048708764796674846\n",
      "Epoch 892:\n",
      "Loss train 0.16281532911770047 valid 0.021621147459682098\n",
      "Epoch 893:\n",
      "Loss train 0.1677863207399845 valid 0.02247840520832595\n",
      "Epoch 894:\n",
      "Loss train 0.18432237849123775 valid 0.03514988792118016\n",
      "Epoch 895:\n",
      "Loss train 0.1762217441752553 valid 0.02582284246802143\n",
      "Epoch 896:\n",
      "Loss train 0.17688546362090857 valid 0.03863724923300988\n",
      "Epoch 897:\n",
      "Loss train 0.16532491634786128 valid 0.04648028231787841\n",
      "Epoch 898:\n",
      "Loss train 0.18574395769331603 valid 0.16778006881549576\n",
      "Epoch 899:\n",
      "Loss train 0.16619339942727238 valid 0.03654950018844679\n",
      "Epoch 900:\n",
      "Loss train 0.16873119108341633 valid 0.06778841867529735\n",
      "Epoch 901:\n",
      "Loss train 0.1633022014474496 valid 0.03842353790119199\n",
      "Epoch 902:\n",
      "Loss train 0.17299239125270396 valid 0.11382058887651854\n",
      "Epoch 903:\n",
      "Loss train 0.18131444498766214 valid 0.08550616468936731\n",
      "Epoch 904:\n",
      "Loss train 0.1691881571687758 valid 0.8455042234231542\n",
      "Epoch 905:\n",
      "Loss train 0.19170257618967443 valid 0.027142886112065445\n",
      "Epoch 906:\n",
      "Loss train 0.16925174323599784 valid 0.03173345949670214\n",
      "Epoch 907:\n",
      "Loss train 0.1818809893136844 valid 0.01771281860537059\n",
      "Epoch 908:\n",
      "Loss train 0.17774310005679728 valid 0.030092875172838884\n",
      "Epoch 909:\n",
      "Loss train 0.16807348268646746 valid 0.028842966294254525\n",
      "Epoch 910:\n",
      "Loss train 0.16712923911642283 valid 0.040147905526470316\n",
      "Epoch 911:\n",
      "Loss train 0.18219153181742878 valid 0.039120390301185186\n",
      "Epoch 912:\n",
      "Loss train 0.1671403150547296 valid 0.05317569164528664\n",
      "Epoch 913:\n",
      "Loss train 0.173701530748792 valid 0.16958566909052925\n",
      "Epoch 914:\n",
      "Loss train 0.1855191934959963 valid 0.03753759841644726\n",
      "Epoch 915:\n",
      "Loss train 0.1658202474957332 valid 0.030485980418769553\n",
      "Epoch 916:\n",
      "Loss train 0.17571960346139967 valid 0.023836433003220532\n",
      "Epoch 917:\n",
      "Loss train 0.16564287902750074 valid 0.023635462349629776\n",
      "Epoch 918:\n",
      "Loss train 0.18118219711724667 valid 2.4622539103556385\n",
      "Epoch 919:\n",
      "Loss train 0.16618452100474387 valid 0.02167017890433376\n",
      "Epoch 920:\n",
      "Loss train 0.16869137274120002 valid 0.22408613102807634\n",
      "Epoch 921:\n",
      "Loss train 0.16823219303768128 valid 0.45507788429778323\n",
      "Epoch 922:\n",
      "Loss train 0.1678338178049773 valid 0.02509479830862076\n",
      "Epoch 923:\n",
      "Loss train 0.17234096087999642 valid 0.24894610251489252\n",
      "Epoch 924:\n",
      "Loss train 0.16428949468266218 valid 0.4429760577172672\n",
      "Epoch 925:\n",
      "Loss train 0.19162394498251378 valid 0.019012154400851803\n",
      "Epoch 926:\n",
      "Loss train 0.15702633335664867 valid 0.20406494541041656\n",
      "Epoch 927:\n",
      "Loss train 0.18078697338029742 valid 0.020093874666025233\n",
      "Epoch 928:\n",
      "Loss train 0.15497751570530235 valid 0.3759948095611884\n",
      "Epoch 929:\n",
      "Loss train 0.17108356422111393 valid 0.05955304047582364\n",
      "Epoch 930:\n",
      "Loss train 0.17563627162836493 valid 0.07293808047476522\n",
      "Epoch 931:\n",
      "Loss train 0.16647466802727431 valid 0.03422353718160289\n",
      "Epoch 932:\n",
      "Loss train 0.1677431312661618 valid 0.044701644884046945\n",
      "Epoch 933:\n",
      "Loss train 0.17260821444559843 valid 0.019527183911403574\n",
      "Epoch 934:\n",
      "Loss train 0.1667437383910641 valid 0.27830569123008775\n",
      "Epoch 935:\n",
      "Loss train 0.18771461265571415 valid 0.06217082082131735\n",
      "Epoch 936:\n",
      "Loss train 0.1662532224951312 valid 0.12909733138571017\n",
      "Epoch 937:\n",
      "Loss train 0.16601032823827117 valid 0.04075805442420277\n",
      "Epoch 938:\n",
      "Loss train 0.17200356621220708 valid 0.01871896367845219\n",
      "Epoch 939:\n",
      "Loss train 0.15854665926210582 valid 0.15375223358363804\n",
      "Epoch 940:\n",
      "Loss train 0.16384592698980122 valid 0.13820605671569383\n",
      "Epoch 941:\n",
      "Loss train 0.1706407233837992 valid 0.019153867400875327\n",
      "Epoch 942:\n",
      "Loss train 0.16107726228032263 valid 0.11055326532606553\n",
      "Epoch 943:\n",
      "Loss train 0.17317499339580536 valid 0.15774648507732325\n",
      "Epoch 944:\n",
      "Loss train 0.17137962615191937 valid 0.10944246867915781\n",
      "Epoch 945:\n",
      "Loss train 0.18279669746011495 valid 0.0333713129944018\n",
      "Epoch 946:\n",
      "Loss train 0.16263790355846286 valid 1.7188432623470304\n",
      "Epoch 947:\n",
      "Loss train 0.17231589259598404 valid 2.1064142960235586\n",
      "Epoch 948:\n",
      "Loss train 0.16744820524565876 valid 0.03121788312911061\n",
      "Epoch 949:\n",
      "Loss train 0.16802311196718364 valid 0.2207297107192967\n",
      "Epoch 950:\n",
      "Loss train 0.1627567104667425 valid 0.02591677592448368\n",
      "Epoch 951:\n",
      "Loss train 0.1673848047675565 valid 0.024084786518260348\n",
      "Epoch 952:\n",
      "Loss train 0.17695866648573427 valid 0.06302180046339796\n",
      "Epoch 953:\n",
      "Loss train 0.17356998110134156 valid 0.04069758353352725\n",
      "Epoch 954:\n",
      "Loss train 0.1912163943592459 valid 0.027955626847709497\n",
      "Epoch 955:\n",
      "Loss train 0.15432668569479138 valid 0.02465870348108139\n",
      "Epoch 956:\n",
      "Loss train 0.1775641029091552 valid 0.7409357447334401\n",
      "Epoch 957:\n",
      "Loss train 0.15526880715657027 valid 0.1464802777012567\n",
      "Epoch 958:\n",
      "Loss train 0.17366126944515853 valid 0.09254735793784236\n",
      "Epoch 959:\n",
      "Loss train 0.18203204303253442 valid 1.0902764825198021\n",
      "Epoch 960:\n",
      "Loss train 0.17655164629872888 valid 0.03394269163462371\n",
      "Epoch 961:\n",
      "Loss train 0.1581042374946177 valid 0.05846533961440706\n",
      "Epoch 962:\n",
      "Loss train 0.16226155786011368 valid 0.16869680567627915\n",
      "Epoch 963:\n",
      "Loss train 0.1764861742535606 valid 0.03222283430714923\n",
      "Epoch 964:\n",
      "Loss train 0.1547467914581299 valid 0.07094407895395301\n",
      "Epoch 965:\n",
      "Loss train 0.17564930653423072 valid 0.03099374736994699\n",
      "Epoch 966:\n",
      "Loss train 0.16440820318572222 valid 0.17782002715714687\n",
      "Epoch 967:\n",
      "Loss train 0.15898268316816538 valid 0.08926199067195091\n",
      "Epoch 968:\n",
      "Loss train 0.1686484221270308 valid 0.11029853295896248\n",
      "Epoch 969:\n",
      "Loss train 0.1539212163472548 valid 0.09616168543802193\n",
      "Epoch 970:\n",
      "Loss train 0.16665995872057973 valid 0.0858159766452164\n",
      "Epoch 971:\n",
      "Loss train 0.17267948820572346 valid 0.033281315820940724\n",
      "Epoch 972:\n",
      "Loss train 0.15913773362860084 valid 0.03407370304608931\n",
      "Epoch 973:\n",
      "Loss train 0.17138249560464175 valid 0.4990480496487821\n",
      "Epoch 974:\n",
      "Loss train 0.16799384986367077 valid 0.05910124961845847\n",
      "Epoch 975:\n",
      "Loss train 0.16196731223929672 valid 0.038514397046557\n",
      "Epoch 976:\n",
      "Loss train 0.16658557111918926 valid 0.033138732341388956\n",
      "Epoch 977:\n",
      "Loss train 0.16758134554680437 valid 0.021615180704389544\n",
      "Epoch 978:\n",
      "Loss train 0.18422271278183908 valid 0.01912125365437183\n",
      "Epoch 979:\n",
      "Loss train 0.15852983926311134 valid 0.03818256098218772\n",
      "Epoch 980:\n",
      "Loss train 0.16210632182713597 valid 0.04146510162545816\n",
      "Epoch 981:\n",
      "Loss train 0.18581840192656965 valid 0.028883431881166266\n",
      "Epoch 982:\n",
      "Loss train 0.16629662055242805 valid 0.04855279853613968\n",
      "Epoch 983:\n",
      "Loss train 0.15422644227091223 valid 0.021848955425447614\n",
      "Epoch 984:\n",
      "Loss train 0.16370104867834598 valid 0.04071754197957444\n",
      "Epoch 985:\n",
      "Loss train 0.16560337258595972 valid 0.0984176383669595\n",
      "Epoch 986:\n",
      "Loss train 0.1611570833751932 valid 0.0855465530950012\n",
      "Epoch 987:\n",
      "Loss train 0.1631449706375599 valid 0.08485401193262852\n",
      "Epoch 988:\n",
      "Loss train 0.1787249537870288 valid 0.026312771285800708\n",
      "Epoch 989:\n",
      "Loss train 0.14835860472656787 valid 0.06910005686356833\n",
      "Epoch 990:\n",
      "Loss train 0.1774087253704667 valid 0.03742609750240713\n",
      "Epoch 991:\n",
      "Loss train 0.1612123584719375 valid 0.9451229417529965\n",
      "Epoch 992:\n",
      "Loss train 0.17763874096088111 valid 0.018973884910758285\n",
      "Epoch 993:\n",
      "Loss train 0.16982843615245075 valid 0.03668408765288832\n",
      "Epoch 994:\n",
      "Loss train 0.16419657049607486 valid 0.05798639184551672\n",
      "Epoch 995:\n",
      "Loss train 0.15677487897034734 valid 0.024634792624575134\n",
      "Epoch 996:\n",
      "Loss train 0.16203609336204827 valid 0.03512687568114171\n",
      "Epoch 997:\n",
      "Loss train 0.16463954126909375 valid 0.027211418927506625\n",
      "Epoch 998:\n",
      "Loss train 0.16627948591914027 valid 0.07847634167836705\n",
      "Epoch 999:\n",
      "Loss train 0.168872437473014 valid 0.020378244020951394\n",
      "Epoch 1000:\n",
      "Loss train 0.1576411345951259 valid 0.03005881125681568\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "learning_rate = 1e-3\n",
    "n_epochs = 1000\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {}:'.format(epoch + 1))\n",
    "\n",
    "    model.train()\n",
    "    avg_loss = train_one_epoch()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_outputs = torch.squeeze(model(torch.from_numpy(valid_features_normalized.astype(\"float32\"))))\n",
    "        valid_loss = loss_function(valid_outputs, torch.from_numpy(valid_fco2).to(torch.device(\"cuda\"))).detach().cpu().item()\n",
    "\n",
    "    print('Loss train {} valid {}'.format(avg_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "266af035-331f-4298-9df1-6a945273024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some tests of model performance\n",
      "Index:  2486469\n",
      "42.382211071672096\n",
      "tensor([42.3715], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  889830\n",
      "1006.2861357301771\n",
      "tensor([1006.3102], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  1523244\n",
      "503.3432186052197\n",
      "tensor([503.3603], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  4919512\n",
      "784.3194463700629\n",
      "tensor([784.4268], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  772012\n",
      "321.1514859455472\n",
      "tensor([321.2376], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  42812\n",
      "1363.8978478486451\n",
      "tensor([1364.0494], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  4554809\n",
      "349.59341067823567\n",
      "tensor([349.6873], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  4297866\n",
      "1577.4074128192042\n",
      "tensor([1577.5208], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  1591483\n",
      "45.524201800380936\n",
      "tensor([45.5593], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  3986147\n",
      "908.4829140524911\n",
      "tensor([908.6677], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "print(\"Some tests of model performance\")\n",
    "random_indices = np.random.randint(ntrain, size=10, dtype=int)\n",
    "for ind in random_indices:\n",
    "    print(\"Index: \", ind)\n",
    "    print(train_fco2[ind])\n",
    "    print(model(torch.from_numpy(train_features_normalized[ind, :].astype(\"float32\"))))\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30a78c67-fe00-43cb-83ca-49c20140fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"64x3_elu_1000epo.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d6fb27d-b958-4d69-90a2-f04edc638037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(x, y):\n",
    "    return np.sum((x-y)**2)/len(x)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    model_output_after_training = model(torch.from_numpy(valid_features_normalized.astype(\"float32\"))).detach().cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2fa4cc0c-bdcf-4702-bb6d-a354635e505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (for consistency check):  0.03005881125681567\n",
      "RMSE:  0.1733747711081854\n",
      "Maximum absolute deviation:  5.621064177502376\n",
      "99.9th percentile of absolute deviation (1000 val's larger):  0.9622477764905215\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE (for consistency check): \", MSE(valid_fco2, model_output_after_training))\n",
    "print(\"RMSE: \", np.sqrt(MSE(valid_fco2, model_output_after_training)))\n",
    "print(\"Maximum absolute deviation: \", np.max(np.abs(model_output_after_training-valid_fco2)))\n",
    "print(\"99.9th percentile of absolute deviation (1000 val's larger): \", np.percentile(np.abs(model_output_after_training-valid_fco2), q=99.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616a2d17-3bf0-4202-8107-2f4ae42badbc",
   "metadata": {},
   "source": [
    "#### Reload model and continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1c15cf4-f79c-48a7-bf8e-acf465b64621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(6, 64, 1)\n",
    "model_state_dict = torch.load(\"./model/64x3_elu_1000epo.pth\")\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aebe6748-48cc-4f89-9e9a-e19fae43c35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (for consistency check with previously trained state):  0.03005881125681567\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE (for consistency check with previously trained state): \", MSE(valid_fco2, model_output_after_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1af3e00-39f5-4b28-b489-aa214ecf6313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Loss train 0.01737227240204811 valid 0.0149102865658534\n",
      "Epoch 2:\n",
      "Loss train 0.01444323012791574 valid 0.015896345325769577\n",
      "Epoch 3:\n",
      "Loss train 0.014759655118919909 valid 0.014161524859751792\n",
      "Epoch 4:\n",
      "Loss train 0.01469262289069593 valid 0.015778170076080174\n",
      "Epoch 5:\n",
      "Loss train 0.01483348960056901 valid 0.014598819587945202\n",
      "Epoch 6:\n",
      "Loss train 0.014662792167626322 valid 0.013665436020092576\n",
      "Epoch 7:\n",
      "Loss train 0.014667167101055383 valid 0.013628174321405062\n",
      "Epoch 8:\n",
      "Loss train 0.014601152077317239 valid 0.014064701779326122\n",
      "Epoch 9:\n",
      "Loss train 0.014559559959918261 valid 0.014513459159013458\n",
      "Epoch 10:\n",
      "Loss train 0.014741618088446557 valid 0.014722781840103866\n",
      "Epoch 11:\n",
      "Loss train 0.014402410134673119 valid 0.013842873821217185\n",
      "Epoch 12:\n",
      "Loss train 0.014579445702955126 valid 0.01457811210223105\n",
      "Epoch 13:\n",
      "Loss train 0.014549458729103208 valid 0.017126131851204825\n",
      "Epoch 14:\n",
      "Loss train 0.014285227060317993 valid 0.013672627586360835\n",
      "Epoch 15:\n",
      "Loss train 0.014643997716717422 valid 0.013476749830436561\n",
      "Epoch 16:\n",
      "Loss train 0.014328436064533889 valid 0.01566913853704702\n",
      "Epoch 17:\n",
      "Loss train 0.014413971406407653 valid 0.015379970380196511\n",
      "Epoch 18:\n",
      "Loss train 0.01441319360397756 valid 0.013599159167844273\n",
      "Epoch 19:\n",
      "Loss train 0.014259268851950765 valid 0.013322498506235089\n",
      "Epoch 20:\n",
      "Loss train 0.014397120476700366 valid 0.014196518225543367\n",
      "Epoch 21:\n",
      "Loss train 0.014290709638036787 valid 0.015753727498240874\n",
      "Epoch 22:\n",
      "Loss train 0.014331802208907903 valid 0.013666596149273355\n",
      "Epoch 23:\n",
      "Loss train 0.014306486788205803 valid 0.014972273711381599\n",
      "Epoch 24:\n",
      "Loss train 0.014293063344433903 valid 0.013881546904317464\n",
      "Epoch 25:\n",
      "Loss train 0.014297583458013833 valid 0.014892214550631156\n",
      "Epoch 26:\n",
      "Loss train 0.014241375861689448 valid 0.013706779439125738\n",
      "Epoch 27:\n",
      "Loss train 0.014175606790930034 valid 0.013186987451484314\n",
      "Epoch 28:\n",
      "Loss train 0.014225722200237215 valid 0.013377164282231629\n",
      "Epoch 29:\n",
      "Loss train 0.014257335413247347 valid 0.014245433234741565\n",
      "Epoch 30:\n",
      "Loss train 0.014132082584314047 valid 0.01764166855061905\n",
      "Epoch 31:\n",
      "Loss train 0.014275663525797427 valid 0.013987464796416752\n",
      "Epoch 32:\n",
      "Loss train 0.014152884633280337 valid 0.01584875123208977\n",
      "Epoch 33:\n",
      "Loss train 0.014020092263817787 valid 0.015195109228446581\n",
      "Epoch 34:\n",
      "Loss train 0.014140929145738482 valid 0.012824201829545509\n",
      "Epoch 35:\n",
      "Loss train 0.014290878936648368 valid 0.01470427673819306\n",
      "Epoch 36:\n",
      "Loss train 0.013880028762854636 valid 0.014584691162053931\n",
      "Epoch 37:\n",
      "Loss train 0.014057376135140657 valid 0.01600881222446757\n",
      "Epoch 38:\n",
      "Loss train 0.014035710155963898 valid 0.015751090689825904\n",
      "Epoch 39:\n",
      "Loss train 0.014145032896660269 valid 0.013202008805302866\n",
      "Epoch 40:\n",
      "Loss train 0.013863725000061096 valid 0.015636900437954446\n",
      "Epoch 41:\n",
      "Loss train 0.01396669989079237 valid 0.015260500915729154\n",
      "Epoch 42:\n",
      "Loss train 0.013920566957443953 valid 0.01400685894292951\n",
      "Epoch 43:\n",
      "Loss train 0.014012616713531316 valid 0.012727430681883763\n",
      "Epoch 44:\n",
      "Loss train 0.013940875943750142 valid 0.01533227529558185\n",
      "Epoch 45:\n",
      "Loss train 0.013932540241628886 valid 0.013655005787357647\n",
      "Epoch 46:\n",
      "Loss train 0.013932251178659498 valid 0.012849581976329226\n",
      "Epoch 47:\n",
      "Loss train 0.013870998212136328 valid 0.013803901479436434\n",
      "Epoch 48:\n",
      "Loss train 0.013961987793445587 valid 0.012925090776525565\n",
      "Epoch 49:\n",
      "Loss train 0.01397036310657859 valid 0.013201331466884033\n",
      "Epoch 50:\n",
      "Loss train 0.013715693852864206 valid 0.012783278286525422\n",
      "Epoch 51:\n",
      "Loss train 0.014001067630946636 valid 0.012917044199017218\n",
      "Epoch 52:\n",
      "Loss train 0.013878267915919423 valid 0.012680372537986548\n",
      "Epoch 53:\n",
      "Loss train 0.013772262499667703 valid 0.014001375521248156\n",
      "Epoch 54:\n",
      "Loss train 0.013827110612764955 valid 0.013731454142578259\n",
      "Epoch 55:\n",
      "Loss train 0.013862759282812475 valid 0.013391306965128743\n",
      "Epoch 56:\n",
      "Loss train 0.01387943508476019 valid 0.015688292947944646\n",
      "Epoch 57:\n",
      "Loss train 0.013642477611079812 valid 0.01353812223291236\n",
      "Epoch 58:\n",
      "Loss train 0.013918808023445308 valid 0.01417960863944213\n",
      "Epoch 59:\n",
      "Loss train 0.013672522178851069 valid 0.015109521100960163\n",
      "Epoch 60:\n",
      "Loss train 0.013770280871540308 valid 0.013024751662989657\n",
      "Epoch 61:\n",
      "Loss train 0.01370481888204813 valid 0.013452048062799246\n",
      "Epoch 62:\n",
      "Loss train 0.013763817129656672 valid 0.016623735880949293\n",
      "Epoch 63:\n",
      "Loss train 0.013663017565384508 valid 0.012931926245854057\n",
      "Epoch 64:\n",
      "Loss train 0.013643405538983643 valid 0.013107823541091338\n",
      "Epoch 65:\n",
      "Loss train 0.013641637284308672 valid 0.01270526140557414\n",
      "Epoch 66:\n",
      "Loss train 0.013642380459234119 valid 0.014608702319382012\n",
      "Epoch 67:\n",
      "Loss train 0.013644325653091073 valid 0.013270539433666222\n",
      "Epoch 68:\n",
      "Loss train 0.01356267995107919 valid 0.012624002630457343\n",
      "Epoch 69:\n",
      "Loss train 0.013732260806486011 valid 0.014298391076627013\n",
      "Epoch 70:\n",
      "Loss train 0.013470179172232748 valid 0.012489690838384089\n",
      "Epoch 71:\n",
      "Loss train 0.013560999750159682 valid 0.013374826254250991\n",
      "Epoch 72:\n",
      "Loss train 0.013599661069922149 valid 0.01255969046239044\n",
      "Epoch 73:\n",
      "Loss train 0.013543109139427544 valid 0.012802229242527333\n",
      "Epoch 74:\n",
      "Loss train 0.013497533009387552 valid 0.013933908763317085\n",
      "Epoch 75:\n",
      "Loss train 0.013590309388935566 valid 0.013277653559296818\n",
      "Epoch 76:\n",
      "Loss train 0.013520962616428734 valid 0.012675345031828005\n",
      "Epoch 77:\n",
      "Loss train 0.013469507798552513 valid 0.014709229548627412\n",
      "Epoch 78:\n",
      "Loss train 0.013492239580489696 valid 0.012598762289424246\n",
      "Epoch 79:\n",
      "Loss train 0.013553682455793022 valid 0.013300612212637368\n",
      "Epoch 80:\n",
      "Loss train 0.013591656746342778 valid 0.013013211871070138\n",
      "Epoch 81:\n",
      "Loss train 0.013411259166896344 valid 0.012819783116552668\n",
      "Epoch 82:\n",
      "Loss train 0.013560278609395028 valid 0.016089401339643237\n",
      "Epoch 83:\n",
      "Loss train 0.013545793672092259 valid 0.015495171857997715\n",
      "Epoch 84:\n",
      "Loss train 0.013246576422825456 valid 0.01327913803064622\n",
      "Epoch 85:\n",
      "Loss train 0.013432314305566252 valid 0.012958833390041365\n",
      "Epoch 86:\n",
      "Loss train 0.013417767274193465 valid 0.01301399338450709\n",
      "Epoch 87:\n",
      "Loss train 0.013357552404515445 valid 0.0214571051653175\n",
      "Epoch 88:\n",
      "Loss train 0.013459158015437424 valid 0.019327496169100258\n",
      "Epoch 89:\n",
      "Loss train 0.01329026633873582 valid 0.012206185001022586\n",
      "Epoch 90:\n",
      "Loss train 0.013395706178620457 valid 0.013329470516741609\n",
      "Epoch 91:\n",
      "Loss train 0.013400128968060017 valid 0.013522392018808754\n",
      "Epoch 92:\n",
      "Loss train 0.013199007105082273 valid 0.014722512213949686\n",
      "Epoch 93:\n",
      "Loss train 0.013450221471488475 valid 0.01212351810989919\n",
      "Epoch 94:\n",
      "Loss train 0.013304475804790855 valid 0.012444098257305561\n",
      "Epoch 95:\n",
      "Loss train 0.013229588047601283 valid 0.013158513010591134\n",
      "Epoch 96:\n",
      "Loss train 0.013293091942556203 valid 0.013228103120326483\n",
      "Epoch 97:\n",
      "Loss train 0.013390129929408431 valid 0.014071585419925741\n",
      "Epoch 98:\n",
      "Loss train 0.01334348967205733 valid 0.01591330767129789\n",
      "Epoch 99:\n",
      "Loss train 0.013193364836275577 valid 0.01269888729092822\n",
      "Epoch 100:\n",
      "Loss train 0.013142775112763048 valid 0.012301601507282968\n",
      "Epoch 101:\n",
      "Loss train 0.013362876353785396 valid 0.014447269673328952\n",
      "Epoch 102:\n",
      "Loss train 0.013104762863367796 valid 0.01210640294510969\n",
      "Epoch 103:\n",
      "Loss train 0.01322307261545211 valid 0.012283527487330553\n",
      "Epoch 104:\n",
      "Loss train 0.013184106016531586 valid 0.012444569252973357\n",
      "Epoch 105:\n",
      "Loss train 0.013125744498334825 valid 0.012499314615044629\n",
      "Epoch 106:\n",
      "Loss train 0.01320878710784018 valid 0.012315026689202632\n",
      "Epoch 107:\n",
      "Loss train 0.013097028863616288 valid 0.013160666817876594\n",
      "Epoch 108:\n",
      "Loss train 0.01316145019326359 valid 0.012453666032697172\n",
      "Epoch 109:\n",
      "Loss train 0.013080397965386509 valid 0.012073523634434626\n",
      "Epoch 110:\n",
      "Loss train 0.013097925957292318 valid 0.01591148282493912\n",
      "Epoch 111:\n",
      "Loss train 0.013170346708036958 valid 0.011842388737671797\n",
      "Epoch 112:\n",
      "Loss train 0.013031889368779958 valid 0.01215580026056244\n",
      "Epoch 113:\n",
      "Loss train 0.012993151986971497 valid 0.012318089884375872\n",
      "Epoch 114:\n",
      "Loss train 0.013008902746252715 valid 0.015080889912244832\n",
      "Epoch 115:\n",
      "Loss train 0.013054945282638073 valid 0.012085480599113598\n",
      "Epoch 116:\n",
      "Loss train 0.01320690092910081 valid 0.012426332332971185\n",
      "Epoch 117:\n",
      "Loss train 0.012827391611412168 valid 0.012108748473111332\n",
      "Epoch 118:\n",
      "Loss train 0.013080028844997286 valid 0.011935477786235731\n",
      "Epoch 119:\n",
      "Loss train 0.01292468153964728 valid 0.012368583121581508\n",
      "Epoch 120:\n",
      "Loss train 0.013056739063002169 valid 0.012082757900234163\n",
      "Epoch 121:\n",
      "Loss train 0.012858631504699588 valid 0.012508607650374483\n",
      "Epoch 122:\n",
      "Loss train 0.012920425432734192 valid 0.01382710284934199\n",
      "Epoch 123:\n",
      "Loss train 0.01287281198333949 valid 0.012582941422448368\n",
      "Epoch 124:\n",
      "Loss train 0.01304165982734412 valid 0.012381605118380313\n",
      "Epoch 125:\n",
      "Loss train 0.012867090912535786 valid 0.01304873046424322\n",
      "Epoch 126:\n",
      "Loss train 0.012904040554538369 valid 0.015958776478850408\n",
      "Epoch 127:\n",
      "Loss train 0.012847953537479043 valid 0.011916176270285614\n",
      "Epoch 128:\n",
      "Loss train 0.012863561296835541 valid 0.01337110623840035\n",
      "Epoch 129:\n",
      "Loss train 0.013006478935480117 valid 0.012832125187457329\n",
      "Epoch 130:\n",
      "Loss train 0.012737749445252121 valid 0.012090620120324627\n",
      "Epoch 131:\n",
      "Loss train 0.012690744805149734 valid 0.011845756728562489\n",
      "Epoch 132:\n",
      "Loss train 0.012842862889170647 valid 0.01170014867024323\n",
      "Epoch 133:\n",
      "Loss train 0.01279150031041354 valid 0.01414195356871587\n",
      "Epoch 134:\n",
      "Loss train 0.012807801246643066 valid 0.0131670838178419\n",
      "Epoch 135:\n",
      "Loss train 0.012956717609427869 valid 0.011444802080140721\n",
      "Epoch 136:\n",
      "Loss train 0.012541604264639319 valid 0.014450760338744024\n",
      "Epoch 137:\n",
      "Loss train 0.012793596928939223 valid 0.012268862315457539\n",
      "Epoch 138:\n",
      "Loss train 0.012646786524914204 valid 0.01155651041849911\n",
      "Epoch 139:\n",
      "Loss train 0.012688621548004448 valid 0.013095144371627324\n",
      "Epoch 140:\n",
      "Loss train 0.012725296842865646 valid 0.015207512586303184\n",
      "Epoch 141:\n",
      "Loss train 0.012645624524913728 valid 0.01161349930070386\n",
      "Epoch 142:\n",
      "Loss train 0.012584199281409383 valid 0.013682171789486481\n",
      "Epoch 143:\n",
      "Loss train 0.012745933489874005 valid 0.012839095202822915\n",
      "Epoch 144:\n",
      "Loss train 0.012638324504718185 valid 0.012083566186119605\n",
      "Epoch 145:\n",
      "Loss train 0.01262291581556201 valid 0.01163546193639287\n",
      "Epoch 146:\n",
      "Loss train 0.012655610678717495 valid 0.011575180611090764\n",
      "Epoch 147:\n",
      "Loss train 0.012620246469974518 valid 0.013289861537097061\n",
      "Epoch 148:\n",
      "Loss train 0.012609738094732165 valid 0.011938215426553073\n",
      "Epoch 149:\n",
      "Loss train 0.012629243624396623 valid 0.012508460474986247\n",
      "Epoch 150:\n",
      "Loss train 0.012637836110778154 valid 0.011383080441674995\n",
      "Epoch 151:\n",
      "Loss train 0.012492848904803395 valid 0.011495238221068299\n",
      "Epoch 152:\n",
      "Loss train 0.01264623964484781 valid 0.012725954263103136\n",
      "Epoch 153:\n",
      "Loss train 0.012565662077628076 valid 0.014475782996587593\n",
      "Epoch 154:\n",
      "Loss train 0.012719764898531138 valid 0.011766457752120452\n",
      "Epoch 155:\n",
      "Loss train 0.0124271951187402 valid 0.012066660741395367\n",
      "Epoch 156:\n",
      "Loss train 0.012550137359648944 valid 0.01209656851203516\n",
      "Epoch 157:\n",
      "Loss train 0.012515326659195126 valid 0.012873188535746058\n",
      "Epoch 158:\n",
      "Loss train 0.012465821027755737 valid 0.011708285457230402\n",
      "Epoch 159:\n",
      "Loss train 0.012550656235776841 valid 0.011688406221524937\n",
      "Epoch 160:\n",
      "Loss train 0.01264419349655509 valid 0.012687504638198184\n",
      "Epoch 161:\n",
      "Loss train 0.012450177495367825 valid 0.013090067726240464\n",
      "Epoch 162:\n",
      "Loss train 0.012420380481518806 valid 0.011501895302651723\n",
      "Epoch 163:\n",
      "Loss train 0.012474281514063478 valid 0.014348174512720734\n",
      "Epoch 164:\n",
      "Loss train 0.012403782155364753 valid 0.012023458238330113\n",
      "Epoch 165:\n",
      "Loss train 0.012466719810850919 valid 0.011809178947271721\n",
      "Epoch 166:\n",
      "Loss train 0.012483257655985654 valid 0.013652392861251264\n",
      "Epoch 167:\n",
      "Loss train 0.012309991393238306 valid 0.01254994768295542\n",
      "Epoch 168:\n",
      "Loss train 0.012469555689953268 valid 0.011774716897122733\n",
      "Epoch 169:\n",
      "Loss train 0.012422461758367717 valid 0.011469508765832888\n",
      "Epoch 170:\n",
      "Loss train 0.012348472762852907 valid 0.013581290077270434\n",
      "Epoch 171:\n",
      "Loss train 0.012430585007183253 valid 0.02115646369700904\n",
      "Epoch 172:\n",
      "Loss train 0.012376102028414607 valid 0.011623251712502256\n",
      "Epoch 173:\n",
      "Loss train 0.012350872310809792 valid 0.012044353524201397\n",
      "Epoch 174:\n",
      "Loss train 0.012335553229786456 valid 0.01616180182031842\n",
      "Epoch 175:\n",
      "Loss train 0.012345511275343597 valid 0.012729333867068373\n",
      "Epoch 176:\n",
      "Loss train 0.012375806597061455 valid 0.011674225520092731\n",
      "Epoch 177:\n",
      "Loss train 0.01233618812263012 valid 0.013960878148805972\n",
      "Epoch 178:\n",
      "Loss train 0.01228441858664155 valid 0.011683085667804809\n",
      "Epoch 179:\n",
      "Loss train 0.012217210721224546 valid 0.011878107382688725\n",
      "Epoch 180:\n",
      "Loss train 0.012303322797641158 valid 0.011547405223270567\n",
      "Epoch 181:\n",
      "Loss train 0.012414163977839053 valid 0.011207263034102957\n",
      "Epoch 182:\n",
      "Loss train 0.012265699192881584 valid 0.014667479552276584\n",
      "Epoch 183:\n",
      "Loss train 0.012414589672349393 valid 0.011007610819447698\n",
      "Epoch 184:\n",
      "Loss train 0.012191192118450999 valid 0.011262723508703635\n",
      "Epoch 185:\n",
      "Loss train 0.012247108646668494 valid 0.012450500768524055\n",
      "Epoch 186:\n",
      "Loss train 0.012152942682616413 valid 0.011904955666012433\n",
      "Epoch 187:\n",
      "Loss train 0.012366317170672119 valid 0.011358036089435298\n",
      "Epoch 188:\n",
      "Loss train 0.012109761350788176 valid 0.011883442499330696\n",
      "Epoch 189:\n",
      "Loss train 0.012184843074530364 valid 0.014078572141097544\n",
      "Epoch 190:\n",
      "Loss train 0.012089205325581134 valid 0.01472447355373825\n",
      "Epoch 191:\n",
      "Loss train 0.0121343635590747 valid 0.013546261196991944\n",
      "Epoch 192:\n",
      "Loss train 0.012247784402221441 valid 0.016659077548174295\n",
      "Epoch 193:\n",
      "Loss train 0.012124524186365306 valid 0.011134913124747296\n",
      "Epoch 194:\n",
      "Loss train 0.012275531983934343 valid 0.012043464591294134\n",
      "Epoch 195:\n",
      "Loss train 0.012134745695628226 valid 0.011790747308638503\n",
      "Epoch 196:\n",
      "Loss train 0.012108433110639453 valid 0.011111556973182798\n",
      "Epoch 197:\n",
      "Loss train 0.012260924882255494 valid 0.012250149892385303\n",
      "Epoch 198:\n",
      "Loss train 0.012012326309457422 valid 0.011437763819362682\n",
      "Epoch 199:\n",
      "Loss train 0.012229983200319112 valid 0.011185205826855062\n",
      "Epoch 200:\n",
      "Loss train 0.012287539501674473 valid 0.017446699771945472\n",
      "Epoch 201:\n",
      "Loss train 0.012004806271754206 valid 0.010977338663875307\n",
      "Epoch 202:\n",
      "Loss train 0.012130073086358608 valid 0.012993127384235675\n",
      "Epoch 203:\n",
      "Loss train 0.012048909762874246 valid 0.012342857540298286\n",
      "Epoch 204:\n",
      "Loss train 0.012004992275498807 valid 0.013563320595797962\n",
      "Epoch 205:\n",
      "Loss train 0.012039201213046908 valid 0.010947580984413357\n",
      "Epoch 206:\n",
      "Loss train 0.01210706834308803 valid 0.011539014910732363\n",
      "Epoch 207:\n",
      "Loss train 0.012033569653518497 valid 0.01123586568228526\n",
      "Epoch 208:\n",
      "Loss train 0.012124219226650895 valid 0.013134931507048106\n",
      "Epoch 209:\n",
      "Loss train 0.011948745961301029 valid 0.014230056130672622\n",
      "Epoch 210:\n",
      "Loss train 0.011982276255264879 valid 0.010965419715705159\n",
      "Epoch 211:\n",
      "Loss train 0.012025371422991156 valid 0.014037913575381724\n",
      "Epoch 212:\n",
      "Loss train 0.011928215583786369 valid 0.012351439727636429\n",
      "Epoch 213:\n",
      "Loss train 0.012124115594662727 valid 0.011050801534193208\n",
      "Epoch 214:\n",
      "Loss train 0.012011280086822809 valid 0.011571500918285921\n",
      "Epoch 215:\n",
      "Loss train 0.011843293386511505 valid 0.011169159339112786\n",
      "Epoch 216:\n",
      "Loss train 0.01212846400681883 valid 0.016740651928126033\n",
      "Epoch 217:\n",
      "Loss train 0.011876108455471695 valid 0.016745522602758696\n",
      "Epoch 218:\n",
      "Loss train 0.01199125915300101 valid 0.011778770888672204\n",
      "Epoch 219:\n",
      "Loss train 0.011889510320499539 valid 0.010831508787009544\n",
      "Epoch 220:\n",
      "Loss train 0.011922090698964894 valid 0.011399265698356292\n",
      "Epoch 221:\n",
      "Loss train 0.011988734533078969 valid 0.012288721822794304\n",
      "Epoch 222:\n",
      "Loss train 0.011859083094634116 valid 0.010775589656308404\n",
      "Epoch 223:\n",
      "Loss train 0.01187754341121763 valid 0.011158786311706703\n",
      "Epoch 224:\n",
      "Loss train 0.011807892157696188 valid 0.01714723559914214\n",
      "Epoch 225:\n",
      "Loss train 0.01207810731139034 valid 0.011118870393719964\n",
      "Epoch 226:\n",
      "Loss train 0.011681405412964523 valid 0.012688921774351741\n",
      "Epoch 227:\n",
      "Loss train 0.011835716771893203 valid 0.010823078715940465\n",
      "Epoch 228:\n",
      "Loss train 0.011982060966081917 valid 0.012206329735960869\n",
      "Epoch 229:\n",
      "Loss train 0.011713279633782804 valid 0.011448088005203376\n",
      "Epoch 230:\n",
      "Loss train 0.011898610971868038 valid 0.011190463974562902\n",
      "Epoch 231:\n",
      "Loss train 0.011799636169336736 valid 0.011148888331275949\n",
      "Epoch 232:\n",
      "Loss train 0.011800193138420581 valid 0.01084698414387726\n",
      "Epoch 233:\n",
      "Loss train 0.011852317799814046 valid 0.010792556684180835\n",
      "Epoch 234:\n",
      "Loss train 0.011718577351421117 valid 0.018373310747266624\n",
      "Epoch 235:\n",
      "Loss train 0.011764965849928559 valid 0.011000003101612576\n",
      "Epoch 236:\n",
      "Loss train 0.011710253234021365 valid 0.0135796096466069\n",
      "Epoch 237:\n",
      "Loss train 0.011606466104276479 valid 0.013556619076440474\n",
      "Epoch 238:\n",
      "Loss train 0.011988306488841772 valid 0.012008396887328568\n",
      "Epoch 239:\n",
      "Loss train 0.011592015125788748 valid 0.011983534322841721\n",
      "Epoch 240:\n",
      "Loss train 0.011773808606900275 valid 0.011212882782958818\n",
      "Epoch 241:\n",
      "Loss train 0.011762710793875157 valid 0.010614834270020088\n",
      "Epoch 242:\n",
      "Loss train 0.011803896519355477 valid 0.010863681619638944\n",
      "Epoch 243:\n",
      "Loss train 0.011652193224057555 valid 0.01408688981832298\n",
      "Epoch 244:\n",
      "Loss train 0.011697540359571577 valid 0.01159900144326657\n",
      "Epoch 245:\n",
      "Loss train 0.01164905041269958 valid 0.011282972716508655\n",
      "Epoch 246:\n",
      "Loss train 0.011562199296429753 valid 0.010798375852934713\n",
      "Epoch 247:\n",
      "Loss train 0.011754714072681964 valid 0.013062168221303082\n",
      "Epoch 248:\n",
      "Loss train 0.011619078224524855 valid 0.010812407740659745\n",
      "Epoch 249:\n",
      "Loss train 0.011657441060990096 valid 0.012249895016849276\n",
      "Epoch 250:\n",
      "Loss train 0.011694246450439096 valid 0.01131869744974882\n",
      "Epoch 251:\n",
      "Loss train 0.011561849783174694 valid 0.015202029230868701\n",
      "Epoch 252:\n",
      "Loss train 0.011731079531833529 valid 0.01065069838615246\n",
      "Epoch 253:\n",
      "Loss train 0.011609280443750323 valid 0.011862269483353935\n",
      "Epoch 254:\n",
      "Loss train 0.011566887380555273 valid 0.013397580059092527\n",
      "Epoch 255:\n",
      "Loss train 0.011538075558841229 valid 0.013391081892139311\n",
      "Epoch 256:\n",
      "Loss train 0.011602439285255969 valid 0.011680342440819568\n",
      "Epoch 257:\n",
      "Loss train 0.011530679079703987 valid 0.011212895435882532\n",
      "Epoch 258:\n",
      "Loss train 0.01184903732314706 valid 0.010549806518037328\n",
      "Epoch 259:\n",
      "Loss train 0.01135531611274928 valid 0.010742882110983454\n",
      "Epoch 260:\n",
      "Loss train 0.011520777174271643 valid 0.010533171686353416\n",
      "Epoch 261:\n",
      "Loss train 0.011567220754921436 valid 0.012676947839070236\n",
      "Epoch 262:\n",
      "Loss train 0.011565492954105139 valid 0.012607114226186723\n",
      "Epoch 263:\n",
      "Loss train 0.011522039160132409 valid 0.021618498508999523\n",
      "Epoch 264:\n",
      "Loss train 0.01158674935810268 valid 0.010992406815668524\n",
      "Epoch 265:\n",
      "Loss train 0.01154874873533845 valid 0.011963051624659205\n",
      "Epoch 266:\n",
      "Loss train 0.011443984234705567 valid 0.010417574073285144\n",
      "Epoch 267:\n",
      "Loss train 0.01153169715963304 valid 0.011237039662957955\n",
      "Epoch 268:\n",
      "Loss train 0.011469330287538469 valid 0.013551840146809241\n",
      "Epoch 269:\n",
      "Loss train 0.011466239588335156 valid 0.010683985488493102\n",
      "Epoch 270:\n",
      "Loss train 0.011408911784179509 valid 0.011155043396008897\n",
      "Epoch 271:\n",
      "Loss train 0.011629271738231183 valid 0.011387204531528703\n",
      "Epoch 272:\n",
      "Loss train 0.011405746705830097 valid 0.010304161246247244\n",
      "Epoch 273:\n",
      "Loss train 0.01141284338850528 valid 0.010632497649324483\n",
      "Epoch 274:\n",
      "Loss train 0.011415165667422116 valid 0.010485559037542554\n",
      "Epoch 275:\n",
      "Loss train 0.011473060940392315 valid 0.010584571409515232\n",
      "Epoch 276:\n",
      "Loss train 0.011431881244294345 valid 0.011234500327087955\n",
      "Epoch 277:\n",
      "Loss train 0.011363456272520125 valid 0.011220908674169087\n",
      "Epoch 278:\n",
      "Loss train 0.011471498722210526 valid 0.01075412530050176\n",
      "Epoch 279:\n",
      "Loss train 0.011331291536800563 valid 0.010123148916815589\n",
      "Epoch 280:\n",
      "Loss train 0.011337045994587243 valid 0.018808435493130105\n",
      "Epoch 281:\n",
      "Loss train 0.011300455641932786 valid 0.010710774642256648\n",
      "Epoch 282:\n",
      "Loss train 0.011474407507106661 valid 0.011400004774726778\n",
      "Epoch 283:\n",
      "Loss train 0.011270970416255296 valid 0.01075546652449925\n",
      "Epoch 284:\n",
      "Loss train 0.01135030185431242 valid 0.011803108048110937\n",
      "Epoch 285:\n",
      "Loss train 0.011381929791532457 valid 0.013833141542578494\n",
      "Epoch 286:\n",
      "Loss train 0.011480079811997711 valid 0.010528873093455952\n",
      "Epoch 287:\n",
      "Loss train 0.011320465415716172 valid 0.011168104638141153\n",
      "Epoch 288:\n",
      "Loss train 0.011335315900854766 valid 0.012506787967734663\n",
      "Epoch 289:\n",
      "Loss train 0.011168154834769666 valid 0.010065168219220984\n",
      "Epoch 290:\n",
      "Loss train 0.011370915460400283 valid 0.012627643660902969\n",
      "Epoch 291:\n",
      "Loss train 0.011351334775798022 valid 0.010421220488491189\n",
      "Epoch 292:\n",
      "Loss train 0.011110765029676258 valid 0.011459635889909277\n",
      "Epoch 293:\n",
      "Loss train 0.01133599559031427 valid 0.011731739730248594\n",
      "Epoch 294:\n",
      "Loss train 0.011357088678516447 valid 0.012512899022010497\n",
      "Epoch 295:\n",
      "Loss train 0.011249002595432102 valid 0.011693560663695616\n",
      "Epoch 296:\n",
      "Loss train 0.01121330926194787 valid 0.011049834637790513\n",
      "Epoch 297:\n",
      "Loss train 0.011273058406077325 valid 0.010626884813108533\n",
      "Epoch 298:\n",
      "Loss train 0.011299915798008441 valid 0.0108288811216924\n",
      "Epoch 299:\n",
      "Loss train 0.01117597962450236 valid 0.010208980056524703\n",
      "Epoch 300:\n",
      "Loss train 0.011243387893773615 valid 0.015209773112374092\n",
      "Epoch 301:\n",
      "Loss train 0.011114993561059237 valid 0.010150761721864151\n",
      "Epoch 302:\n",
      "Loss train 0.011314549368806184 valid 0.010209744286054687\n",
      "Epoch 303:\n",
      "Loss train 0.011152218186296522 valid 0.010094009531020912\n",
      "Epoch 304:\n",
      "Loss train 0.011255636478774249 valid 0.014982922014585008\n",
      "Epoch 305:\n",
      "Loss train 0.011085115922614932 valid 0.010634889287807082\n",
      "Epoch 306:\n",
      "Loss train 0.01122360415570438 valid 0.013225097368844446\n",
      "Epoch 307:\n",
      "Loss train 0.011096702704206108 valid 0.010356789839648651\n",
      "Epoch 308:\n",
      "Loss train 0.011169630139134825 valid 0.010940944158131448\n",
      "Epoch 309:\n",
      "Loss train 0.011264557006768882 valid 0.011317373991987255\n",
      "Epoch 310:\n",
      "Loss train 0.011163507641293108 valid 0.010363262932916593\n",
      "Epoch 311:\n",
      "Loss train 0.011331108022481204 valid 0.010129319136066942\n",
      "Epoch 312:\n",
      "Loss train 0.01113017184380442 valid 0.010290699887715622\n",
      "Epoch 313:\n",
      "Loss train 0.01103058177512139 valid 0.011386349787269752\n",
      "Epoch 314:\n",
      "Loss train 0.011141361975111067 valid 0.010555919448179213\n",
      "Epoch 315:\n",
      "Loss train 0.01100982580985874 valid 0.010706044660395098\n",
      "Epoch 316:\n",
      "Loss train 0.01129074777662754 valid 0.011966492250176899\n",
      "Epoch 317:\n",
      "Loss train 0.011031584722921253 valid 0.011312973015623975\n",
      "Epoch 318:\n",
      "Loss train 0.011115443994291126 valid 0.010510361995839003\n",
      "Epoch 319:\n",
      "Loss train 0.011046222127042711 valid 0.009936751170412086\n",
      "Epoch 320:\n",
      "Loss train 0.011016114242374897 valid 0.015268200845152168\n",
      "Epoch 321:\n",
      "Loss train 0.011028014028444886 valid 0.010331803752300292\n",
      "Epoch 322:\n",
      "Loss train 0.011051659668795764 valid 0.0100652422420639\n",
      "Epoch 323:\n",
      "Loss train 0.011022807367146016 valid 0.010016383097684359\n",
      "Epoch 324:\n",
      "Loss train 0.011091353204101324 valid 0.01052887474394143\n",
      "Epoch 325:\n",
      "Loss train 0.010940497224219144 valid 0.010460720999496013\n",
      "Epoch 326:\n",
      "Loss train 0.011054118608124554 valid 0.010656258497010187\n",
      "Epoch 327:\n",
      "Loss train 0.010957665445283055 valid 0.010073766520269586\n",
      "Epoch 328:\n",
      "Loss train 0.011013759708963334 valid 0.011090401002739165\n",
      "Epoch 329:\n",
      "Loss train 0.011141997710801661 valid 0.011643691728633236\n",
      "Epoch 330:\n",
      "Loss train 0.011024644232355059 valid 0.011731824221636893\n",
      "Epoch 331:\n",
      "Loss train 0.011008949606679379 valid 0.011245110842624549\n",
      "Epoch 332:\n",
      "Loss train 0.010954105535522103 valid 0.010421939421322102\n",
      "Epoch 333:\n",
      "Loss train 0.011168042201548815 valid 0.01001298359773528\n",
      "Epoch 334:\n",
      "Loss train 0.010795484631322324 valid 0.010982927554214074\n",
      "Epoch 335:\n",
      "Loss train 0.010972199599258601 valid 0.015701344336170923\n",
      "Epoch 336:\n",
      "Loss train 0.011047185518778861 valid 0.010058118563111559\n",
      "Epoch 337:\n",
      "Loss train 0.010865797986276448 valid 0.010518789492407723\n",
      "Epoch 338:\n",
      "Loss train 0.010921922045759857 valid 0.010830645852789198\n",
      "Epoch 339:\n",
      "Loss train 0.010955035203136504 valid 0.01033551130915248\n",
      "Epoch 340:\n",
      "Loss train 0.010876296103000642 valid 0.010053667516637934\n",
      "Epoch 341:\n",
      "Loss train 0.011121608963236213 valid 0.011728824188043134\n",
      "Epoch 342:\n",
      "Loss train 0.010847073731012642 valid 0.010091734368877408\n",
      "Epoch 343:\n",
      "Loss train 0.010899964393116534 valid 0.01049274208923712\n",
      "Epoch 344:\n",
      "Loss train 0.010911509498953819 valid 0.009754669636862116\n",
      "Epoch 345:\n",
      "Loss train 0.010930132493376732 valid 0.011177903638965734\n",
      "Epoch 346:\n",
      "Loss train 0.010801760353147984 valid 0.010718883271192052\n",
      "Epoch 347:\n",
      "Loss train 0.010974161841906607 valid 0.013428621847580608\n",
      "Epoch 348:\n",
      "Loss train 0.010783386707305909 valid 0.009707872828301135\n",
      "Epoch 349:\n",
      "Loss train 0.010831938169896603 valid 0.011104555744181727\n",
      "Epoch 350:\n",
      "Loss train 0.011037226976826787 valid 0.009986691746331919\n",
      "Epoch 351:\n",
      "Loss train 0.010769708598963916 valid 0.011100426035236048\n",
      "Epoch 352:\n",
      "Loss train 0.01086878420971334 valid 0.011989649095440686\n",
      "Epoch 353:\n",
      "Loss train 0.010861430884338915 valid 0.010111856992208106\n",
      "Epoch 354:\n",
      "Loss train 0.010876264329999686 valid 0.009925622329272962\n",
      "Epoch 355:\n",
      "Loss train 0.010775426591746509 valid 0.012722986738811063\n",
      "Epoch 356:\n",
      "Loss train 0.010736245467327534 valid 0.010900321002566946\n",
      "Epoch 357:\n",
      "Loss train 0.010869345451705158 valid 0.011759553178755405\n",
      "Epoch 358:\n",
      "Loss train 0.010735711635090411 valid 0.0097958072708129\n",
      "Epoch 359:\n",
      "Loss train 0.010857283369638026 valid 0.00968620362359057\n",
      "Epoch 360:\n",
      "Loss train 0.010857827140018345 valid 0.011321512087223451\n",
      "Epoch 361:\n",
      "Loss train 0.010818375758826732 valid 0.010714996927392851\n",
      "Epoch 362:\n",
      "Loss train 0.010841056298464536 valid 0.010104768631042723\n",
      "Epoch 363:\n",
      "Loss train 0.010713073796592653 valid 0.009724264148296987\n",
      "Epoch 364:\n",
      "Loss train 0.010722203969955444 valid 0.010046753871106325\n",
      "Epoch 365:\n",
      "Loss train 0.01072126438934356 valid 0.011347563510545989\n",
      "Epoch 366:\n",
      "Loss train 0.01090846263244748 valid 0.010578856427148285\n",
      "Epoch 367:\n",
      "Loss train 0.010687479279004037 valid 0.010764660891216957\n",
      "Epoch 368:\n",
      "Loss train 0.010704914457164705 valid 0.010127914730595555\n",
      "Epoch 369:\n",
      "Loss train 0.010768023332580924 valid 0.011347341412762356\n",
      "Epoch 370:\n",
      "Loss train 0.010781785712577402 valid 0.010260484811960275\n",
      "Epoch 371:\n",
      "Loss train 0.010710924795828759 valid 0.009832012266705718\n",
      "Epoch 372:\n",
      "Loss train 0.010765520081855356 valid 0.011604743333371345\n",
      "Epoch 373:\n",
      "Loss train 0.01067054625414312 valid 0.009703565301895093\n",
      "Epoch 374:\n",
      "Loss train 0.010767583361826837 valid 0.013144168311137844\n",
      "Epoch 375:\n",
      "Loss train 0.01069467998854816 valid 0.010286565411628984\n",
      "Epoch 376:\n",
      "Loss train 0.010703412929549813 valid 0.011515481949101182\n",
      "Epoch 377:\n",
      "Loss train 0.010739192847162486 valid 0.009744026173808998\n",
      "Epoch 378:\n",
      "Loss train 0.010653899467550219 valid 0.009714623032758905\n",
      "Epoch 379:\n",
      "Loss train 0.010659646724350751 valid 0.010809521948818979\n",
      "Epoch 380:\n",
      "Loss train 0.010633049987256527 valid 0.010352110476771294\n",
      "Epoch 381:\n",
      "Loss train 0.010675541009753943 valid 0.013142243609227073\n",
      "Epoch 382:\n",
      "Loss train 0.010805421749129891 valid 0.01115909423457747\n",
      "Epoch 383:\n",
      "Loss train 0.01058091624919325 valid 0.01119896334474168\n",
      "Epoch 384:\n",
      "Loss train 0.01065932742599398 valid 0.010104956711945104\n",
      "Epoch 385:\n",
      "Loss train 0.010787316161207854 valid 0.010793268034958958\n",
      "Epoch 386:\n",
      "Loss train 0.010679577280767263 valid 0.010450692208029515\n",
      "Epoch 387:\n",
      "Loss train 0.010634944878518581 valid 0.01065500146154599\n",
      "Epoch 388:\n",
      "Loss train 0.010547660253942014 valid 0.009905308491252254\n",
      "Epoch 389:\n",
      "Loss train 0.010689605536870659 valid 0.012403934965412483\n",
      "Epoch 390:\n",
      "Loss train 0.010616332034580409 valid 0.010482966981775626\n",
      "Epoch 391:\n",
      "Loss train 0.010585644688457251 valid 0.010237485601422614\n",
      "Epoch 392:\n",
      "Loss train 0.010732780354097485 valid 0.009847574776598972\n",
      "Epoch 393:\n",
      "Loss train 0.0104729668693617 valid 0.00950180272119977\n",
      "Epoch 394:\n",
      "Loss train 0.010614853469654918 valid 0.009805508479479703\n",
      "Epoch 395:\n",
      "Loss train 0.01069031372666359 valid 0.009981165319675056\n",
      "Epoch 396:\n",
      "Loss train 0.010534372724592686 valid 0.011475832413672527\n",
      "Epoch 397:\n",
      "Loss train 0.010663780235685408 valid 0.010714743757425635\n",
      "Epoch 398:\n",
      "Loss train 0.010533656216226518 valid 0.009691522048323895\n",
      "Epoch 399:\n",
      "Loss train 0.010624108094722032 valid 0.009864695427013308\n",
      "Epoch 400:\n",
      "Loss train 0.010552018394693732 valid 0.012650246743332922\n",
      "Epoch 401:\n",
      "Loss train 0.010684748898260295 valid 0.012476822042803088\n",
      "Epoch 402:\n",
      "Loss train 0.0104530846234411 valid 0.010188535025423362\n",
      "Epoch 403:\n",
      "Loss train 0.01066544448863715 valid 0.009573900012208588\n",
      "Epoch 404:\n",
      "Loss train 0.010555552456527948 valid 0.009771790079481792\n",
      "Epoch 405:\n",
      "Loss train 0.010568022702820598 valid 0.009947435182484332\n",
      "Epoch 406:\n",
      "Loss train 0.010619072721339762 valid 0.011128835184938873\n",
      "Epoch 407:\n",
      "Loss train 0.010621431156992912 valid 0.010013665036207677\n",
      "Epoch 408:\n",
      "Loss train 0.010455247570760547 valid 0.012730273277111997\n",
      "Epoch 409:\n",
      "Loss train 0.010516840740107 valid 0.009555105198866808\n",
      "Epoch 410:\n",
      "Loss train 0.010472935887053609 valid 0.009661788362345682\n",
      "Epoch 411:\n",
      "Loss train 0.010492782867513597 valid 0.009700434086298858\n",
      "Epoch 412:\n",
      "Loss train 0.010531870722770691 valid 0.010111186744240611\n",
      "Epoch 413:\n",
      "Loss train 0.01052439768332988 valid 0.013095858147683857\n",
      "Epoch 414:\n",
      "Loss train 0.010510288638994099 valid 0.010199204653976718\n",
      "Epoch 415:\n",
      "Loss train 0.010447953859344125 valid 0.009995473246879152\n",
      "Epoch 416:\n",
      "Loss train 0.010490246752277017 valid 0.011542288716441066\n",
      "Epoch 417:\n",
      "Loss train 0.010484174985438586 valid 0.010809676822943922\n",
      "Epoch 418:\n",
      "Loss train 0.010562129230238497 valid 0.00996031462372385\n",
      "Epoch 419:\n",
      "Loss train 0.010557292623445392 valid 0.011372238607400784\n",
      "Epoch 420:\n",
      "Loss train 0.01026291498914361 valid 0.00938775672314328\n",
      "Epoch 421:\n",
      "Loss train 0.01051725268829614 valid 0.009969685005694692\n",
      "Epoch 422:\n",
      "Loss train 0.010546111028641463 valid 0.01041862498422996\n",
      "Epoch 423:\n",
      "Loss train 0.010460795789957046 valid 0.010582882705796623\n",
      "Epoch 424:\n",
      "Loss train 0.010403596146032213 valid 0.009456637556033926\n",
      "Epoch 425:\n",
      "Loss train 0.010530748778022826 valid 0.010406725556607118\n",
      "Epoch 426:\n",
      "Loss train 0.010364854786545039 valid 0.009654687687176422\n",
      "Epoch 427:\n",
      "Loss train 0.010375880440697073 valid 0.009144994368389076\n",
      "Epoch 428:\n",
      "Loss train 0.010499250589869917 valid 0.009692017629734243\n",
      "Epoch 429:\n",
      "Loss train 0.010389532530680298 valid 0.010900343519624394\n",
      "Epoch 430:\n",
      "Loss train 0.010356831427663565 valid 0.011092482628778266\n",
      "Epoch 431:\n",
      "Loss train 0.010395888070575893 valid 0.010662593430399978\n",
      "Epoch 432:\n",
      "Loss train 0.010426241929642857 valid 0.010010760952558604\n",
      "Epoch 433:\n",
      "Loss train 0.010307657304219902 valid 0.009348704435493349\n",
      "Epoch 434:\n",
      "Loss train 0.010328970141708851 valid 0.00956832554050294\n",
      "Epoch 435:\n",
      "Loss train 0.010505402283743024 valid 0.01101179884818214\n",
      "Epoch 436:\n",
      "Loss train 0.010347125372849405 valid 0.016023023863792486\n",
      "Epoch 437:\n",
      "Loss train 0.010298551194369792 valid 0.009841898458048746\n",
      "Epoch 438:\n",
      "Loss train 0.01033950197417289 valid 0.009255027154274044\n",
      "Epoch 439:\n",
      "Loss train 0.01031998352985829 valid 0.010670151847369313\n",
      "Epoch 440:\n",
      "Loss train 0.010331300906836987 valid 0.009361670271249458\n",
      "Epoch 441:\n",
      "Loss train 0.010380170777440072 valid 0.00949793632463398\n",
      "Epoch 442:\n",
      "Loss train 0.01033091840799898 valid 0.009692814159877074\n",
      "Epoch 443:\n",
      "Loss train 0.010444842679426074 valid 0.010989198455022092\n",
      "Epoch 444:\n",
      "Loss train 0.010328283994458615 valid 0.009493514919834233\n",
      "Epoch 445:\n",
      "Loss train 0.010321802090853453 valid 0.010173101155634778\n",
      "Epoch 446:\n",
      "Loss train 0.010243586624972522 valid 0.009582779821647871\n",
      "Epoch 447:\n",
      "Loss train 0.010334730205126107 valid 0.010127775933419033\n",
      "Epoch 448:\n",
      "Loss train 0.010328155394643546 valid 0.009231735096731688\n",
      "Epoch 449:\n",
      "Loss train 0.010239013032987713 valid 0.010755799394766839\n",
      "Epoch 450:\n",
      "Loss train 0.010432756340131164 valid 0.0100303736939193\n",
      "Epoch 451:\n",
      "Loss train 0.010261794180609286 valid 0.011512562699360891\n",
      "Epoch 452:\n",
      "Loss train 0.01024508451949805 valid 0.010231248367823626\n",
      "Epoch 453:\n",
      "Loss train 0.010299241743050516 valid 0.009472671274313928\n",
      "Epoch 454:\n",
      "Loss train 0.010506725564599037 valid 0.013855760405113143\n",
      "Epoch 455:\n",
      "Loss train 0.010257694648578763 valid 0.010368158025596505\n",
      "Epoch 456:\n",
      "Loss train 0.010309021698310971 valid 0.009413574569265221\n",
      "Epoch 457:\n",
      "Loss train 0.010217255402356386 valid 0.00952861323147737\n",
      "Epoch 458:\n",
      "Loss train 0.010280239935964346 valid 0.009897117789099585\n",
      "Epoch 459:\n",
      "Loss train 0.01028341772966087 valid 0.009156019525779796\n",
      "Epoch 460:\n",
      "Loss train 0.010194779234007 valid 0.00994182757072501\n",
      "Epoch 461:\n",
      "Loss train 0.010372899662703276 valid 0.009233156356607356\n",
      "Epoch 462:\n",
      "Loss train 0.010220810920000076 valid 0.009294657723880925\n",
      "Epoch 463:\n",
      "Loss train 0.010220976288430392 valid 0.009807590358219613\n",
      "Epoch 464:\n",
      "Loss train 0.010163584667257964 valid 0.009923241852553845\n",
      "Epoch 465:\n",
      "Loss train 0.010282792964950204 valid 0.010254405344158005\n",
      "Epoch 466:\n",
      "Loss train 0.01015996907837689 valid 0.011572620937321842\n",
      "Epoch 467:\n",
      "Loss train 0.0102830866323784 valid 0.013300404309654356\n",
      "Epoch 468:\n",
      "Loss train 0.010192525008693337 valid 0.0095215268215342\n",
      "Epoch 469:\n",
      "Loss train 0.010227811750024558 valid 0.010377042844333351\n",
      "Epoch 470:\n",
      "Loss train 0.010233394736424088 valid 0.013301895740251209\n",
      "Epoch 471:\n",
      "Loss train 0.010270347132347524 valid 0.009340038239445599\n",
      "Epoch 472:\n",
      "Loss train 0.01022713080327958 valid 0.01006555379075447\n",
      "Epoch 473:\n",
      "Loss train 0.010180231308564543 valid 0.009528366862779237\n",
      "Epoch 474:\n",
      "Loss train 0.010193695926107466 valid 0.009288322946590254\n",
      "Epoch 475:\n",
      "Loss train 0.010179125815629958 valid 0.010376629368960581\n",
      "Epoch 476:\n",
      "Loss train 0.010168646023608743 valid 0.00960970699243673\n",
      "Epoch 477:\n",
      "Loss train 0.01027728407178074 valid 0.010293410301169972\n",
      "Epoch 478:\n",
      "Loss train 0.010124187304638326 valid 0.008987357490484082\n",
      "Epoch 479:\n",
      "Loss train 0.01019012311194092 valid 0.009280735636634112\n",
      "Epoch 480:\n",
      "Loss train 0.010211998370476066 valid 0.011217000931221615\n",
      "Epoch 481:\n",
      "Loss train 0.01008524636272341 valid 0.012319204361170786\n",
      "Epoch 482:\n",
      "Loss train 0.010109586720354856 valid 0.00930312679437965\n",
      "Epoch 483:\n",
      "Loss train 0.010261997422203422 valid 0.011117292668601121\n",
      "Epoch 484:\n",
      "Loss train 0.009994759250432254 valid 0.009252414967828375\n",
      "Epoch 485:\n",
      "Loss train 0.010150763276033103 valid 0.009308622311484884\n",
      "Epoch 486:\n",
      "Loss train 0.010142465671524405 valid 0.01055610802544963\n",
      "Epoch 487:\n",
      "Loss train 0.010073862965684384 valid 0.009872468766818502\n",
      "Epoch 488:\n",
      "Loss train 0.010165756106376649 valid 0.009155861359297654\n",
      "Epoch 489:\n",
      "Loss train 0.010078857353888452 valid 0.009323913668033711\n",
      "Epoch 490:\n",
      "Loss train 0.010229485541582107 valid 0.00994678227380924\n",
      "Epoch 491:\n",
      "Loss train 0.010012070322409273 valid 0.010051482843433442\n",
      "Epoch 492:\n",
      "Loss train 0.010053410151973367 valid 0.009578453715071466\n",
      "Epoch 493:\n",
      "Loss train 0.010151334001682699 valid 0.010113381735310671\n",
      "Epoch 494:\n",
      "Loss train 0.010091220756992697 valid 0.008990369153994439\n",
      "Epoch 495:\n",
      "Loss train 0.010142464630305766 valid 0.009229823766431902\n",
      "Epoch 496:\n",
      "Loss train 0.010103312976658344 valid 0.009029534686737133\n",
      "Epoch 497:\n",
      "Loss train 0.01006202882900834 valid 0.009051200891851964\n",
      "Epoch 498:\n",
      "Loss train 0.01021006603911519 valid 0.011223421994511307\n",
      "Epoch 499:\n",
      "Loss train 0.009903327081352472 valid 0.008978649623985577\n",
      "Epoch 500:\n",
      "Loss train 0.01014541647490114 valid 0.009250591111189317\n",
      "Epoch 501:\n",
      "Loss train 0.010091556121129542 valid 0.009014066234259844\n",
      "Epoch 502:\n",
      "Loss train 0.010059756894595921 valid 0.009294425688348448\n",
      "Epoch 503:\n",
      "Loss train 0.009992902509868146 valid 0.0089010978463586\n",
      "Epoch 504:\n",
      "Loss train 0.010060680805705487 valid 0.00999323431927758\n",
      "Epoch 505:\n",
      "Loss train 0.010060841988772154 valid 0.009252148952129825\n",
      "Epoch 506:\n",
      "Loss train 0.010046813262626528 valid 0.009083549784090329\n",
      "Epoch 507:\n",
      "Loss train 0.010130904566496611 valid 0.009567698166610062\n",
      "Epoch 508:\n",
      "Loss train 0.00994776470400393 valid 0.009800364962600942\n",
      "Epoch 509:\n",
      "Loss train 0.010190779389813543 valid 0.00884580703647239\n",
      "Epoch 510:\n",
      "Loss train 0.00979642059095204 valid 0.009654038182758886\n",
      "Epoch 511:\n",
      "Loss train 0.010132340868469328 valid 0.010789465957222081\n",
      "Epoch 512:\n",
      "Loss train 0.010093992920592426 valid 0.009026158042044622\n",
      "Epoch 513:\n",
      "Loss train 0.009902007143944501 valid 0.010283870352289228\n",
      "Epoch 514:\n",
      "Loss train 0.009990524227265268 valid 0.008918633303033685\n",
      "Epoch 515:\n",
      "Loss train 0.009980868739075959 valid 0.009554627958891658\n",
      "Epoch 516:\n",
      "Loss train 0.010035461221821607 valid 0.009263372088304989\n",
      "Epoch 517:\n",
      "Loss train 0.009984770648181438 valid 0.014714652718549428\n",
      "Epoch 518:\n",
      "Loss train 0.01000371704576537 valid 0.008963026640454497\n",
      "Epoch 519:\n",
      "Loss train 0.009992364326491952 valid 0.009340711979801476\n",
      "Epoch 520:\n",
      "Loss train 0.00995971121173352 valid 0.009549346203029494\n",
      "Epoch 521:\n",
      "Loss train 0.010025574623607099 valid 0.01148511581324301\n",
      "Epoch 522:\n",
      "Loss train 0.009970512173138558 valid 0.009573792523480522\n",
      "Epoch 523:\n",
      "Loss train 0.009982294772285969 valid 0.009831984844430312\n",
      "Epoch 524:\n",
      "Loss train 0.009998640554957092 valid 0.009674843053950298\n",
      "Epoch 525:\n",
      "Loss train 0.009888745835982263 valid 0.009787049502890138\n",
      "Epoch 526:\n",
      "Loss train 0.009854730315972119 valid 0.008991907435930145\n",
      "Epoch 527:\n",
      "Loss train 0.009959486804436892 valid 0.009157651314348208\n",
      "Epoch 528:\n",
      "Loss train 0.009996443302370608 valid 0.010709540242528582\n",
      "Epoch 529:\n",
      "Loss train 0.009948041050694884 valid 0.009097809182385024\n",
      "Epoch 530:\n",
      "Loss train 0.009893105633556843 valid 0.009542806262875141\n",
      "Epoch 531:\n",
      "Loss train 0.009945171016268432 valid 0.01279753540486615\n",
      "Epoch 532:\n",
      "Loss train 0.00988564736256376 valid 0.008897464946913848\n",
      "Epoch 533:\n",
      "Loss train 0.00993053803127259 valid 0.009454370810132189\n",
      "Epoch 534:\n",
      "Loss train 0.00994584617484361 valid 0.009208441892123054\n",
      "Epoch 535:\n",
      "Loss train 0.009880841245874763 valid 0.009493169959838142\n",
      "Epoch 536:\n",
      "Loss train 0.009953401238657534 valid 0.012278290350249512\n",
      "Epoch 537:\n",
      "Loss train 0.009972240669187159 valid 0.010721868296721131\n",
      "Epoch 538:\n",
      "Loss train 0.009872788509353995 valid 0.00905571742290678\n",
      "Epoch 539:\n",
      "Loss train 0.009874240807257594 valid 0.01043383796116515\n",
      "Epoch 540:\n",
      "Loss train 0.00988842590712011 valid 0.009758635797209796\n",
      "Epoch 541:\n",
      "Loss train 0.009947196592576802 valid 0.008969964566436022\n",
      "Epoch 542:\n",
      "Loss train 0.009810924154240638 valid 0.00902283863989974\n",
      "Epoch 543:\n",
      "Loss train 0.010066915955860168 valid 0.010268296993361419\n",
      "Epoch 544:\n",
      "Loss train 0.009763706497382372 valid 0.015704551915351323\n",
      "Epoch 545:\n",
      "Loss train 0.009883910805452615 valid 0.009195626190422976\n",
      "Epoch 546:\n",
      "Loss train 0.009848974633030593 valid 0.00900413349419061\n",
      "Epoch 547:\n",
      "Loss train 0.009849153934046626 valid 0.009940409681856617\n",
      "Epoch 548:\n",
      "Loss train 0.009825719987507909 valid 0.011607700973113281\n",
      "Epoch 549:\n",
      "Loss train 0.009983308165334165 valid 0.009544377399115166\n",
      "Epoch 550:\n",
      "Loss train 0.00995973536092788 valid 0.008796314924426075\n",
      "Epoch 551:\n",
      "Loss train 0.009708170057740063 valid 0.010227080023463687\n",
      "Epoch 552:\n",
      "Loss train 0.009928879485931248 valid 0.009204097350263767\n",
      "Epoch 553:\n",
      "Loss train 0.009990559252444655 valid 0.008918425398805257\n",
      "Epoch 554:\n",
      "Loss train 0.009753460233099759 valid 0.008756891665192676\n",
      "Epoch 555:\n",
      "Loss train 0.009872464599087834 valid 0.009005685571572079\n",
      "Epoch 556:\n",
      "Loss train 0.010008396688848734 valid 0.010574882979572747\n",
      "Epoch 557:\n",
      "Loss train 0.00973584006447345 valid 0.010346549656642286\n",
      "Epoch 558:\n",
      "Loss train 0.009747194294352085 valid 0.009995473787258964\n",
      "Epoch 559:\n",
      "Loss train 0.009871950671076775 valid 0.008747812266949408\n",
      "Epoch 560:\n",
      "Loss train 0.009754965903703124 valid 0.008889321931347633\n",
      "Epoch 561:\n",
      "Loss train 0.009933037971612066 valid 0.009133058696135454\n",
      "Epoch 562:\n",
      "Loss train 0.009749935540370644 valid 0.009155794159929176\n",
      "Epoch 563:\n",
      "Loss train 0.009768068362958729 valid 0.008552007900265871\n",
      "Epoch 564:\n",
      "Loss train 0.009895478622987867 valid 0.00927829456062696\n",
      "Epoch 565:\n",
      "Loss train 0.00976754545327276 valid 0.008971891299010753\n",
      "Epoch 566:\n",
      "Loss train 0.009756189114414155 valid 0.00885209618363858\n",
      "Epoch 567:\n",
      "Loss train 0.009842247933615 valid 0.008756622229385957\n",
      "Epoch 568:\n",
      "Loss train 0.009849792534019798 valid 0.010021859183646138\n",
      "Epoch 569:\n",
      "Loss train 0.009718534039799123 valid 0.008875248599797865\n",
      "Epoch 570:\n",
      "Loss train 0.00980630839895457 valid 0.009129280588733177\n",
      "Epoch 571:\n",
      "Loss train 0.009683899217750877 valid 0.014322939501710352\n",
      "Epoch 572:\n",
      "Loss train 0.009894530797377229 valid 0.01038255206909637\n",
      "Epoch 573:\n",
      "Loss train 0.009857063858769834 valid 0.01995385789990668\n",
      "Epoch 574:\n",
      "Loss train 0.009747031717095523 valid 0.00871952802518181\n",
      "Epoch 575:\n",
      "Loss train 0.009731126725208015 valid 0.011275116795681016\n",
      "Epoch 576:\n",
      "Loss train 0.009809715215582401 valid 0.009246815568263296\n",
      "Epoch 577:\n",
      "Loss train 0.00964863115688786 valid 0.009233732420557528\n",
      "Epoch 578:\n",
      "Loss train 0.009799861198291182 valid 0.008711341366938925\n",
      "Epoch 579:\n",
      "Loss train 0.009820401981007308 valid 0.009602327461310421\n",
      "Epoch 580:\n",
      "Loss train 0.009725766958668829 valid 0.00924093807327513\n",
      "Epoch 581:\n",
      "Loss train 0.009799473065882922 valid 0.009338297630141134\n",
      "Epoch 582:\n",
      "Loss train 0.009721128185279667 valid 0.015509229197299777\n",
      "Epoch 583:\n",
      "Loss train 0.009805297458544374 valid 0.009862404305995278\n",
      "Epoch 584:\n",
      "Loss train 0.009692750031594187 valid 0.008954304198069843\n",
      "Epoch 585:\n",
      "Loss train 0.009820210651028902 valid 0.009638677950320711\n",
      "Epoch 586:\n",
      "Loss train 0.009608984327409417 valid 0.00898544676269462\n",
      "Epoch 587:\n",
      "Loss train 0.009809659503400325 valid 0.009044133304319326\n",
      "Epoch 588:\n",
      "Loss train 0.009598823769018053 valid 0.009265105098968463\n",
      "Epoch 589:\n",
      "Loss train 0.009734193761367351 valid 0.011316624176172135\n",
      "Epoch 590:\n",
      "Loss train 0.00974503713613376 valid 0.00993746952670258\n",
      "Epoch 591:\n",
      "Loss train 0.009708479911554605 valid 0.009275937974656229\n",
      "Epoch 592:\n",
      "Loss train 0.009546903250273317 valid 0.008832106856177593\n",
      "Epoch 593:\n",
      "Loss train 0.00979762944066897 valid 0.010051592414731967\n",
      "Epoch 594:\n",
      "Loss train 0.00966112092928961 valid 0.009833988640873408\n",
      "Epoch 595:\n",
      "Loss train 0.009698985750321298 valid 0.008879910400463544\n",
      "Epoch 596:\n",
      "Loss train 0.009610096850432456 valid 0.008851152858339213\n",
      "Epoch 597:\n",
      "Loss train 0.009671152890194207 valid 0.008720662218200337\n",
      "Epoch 598:\n",
      "Loss train 0.009568895563483238 valid 0.008504309582572882\n",
      "Epoch 599:\n",
      "Loss train 0.009826986484695226 valid 0.009540193731279708\n",
      "Epoch 600:\n",
      "Loss train 0.00968841410567984 valid 0.009669995434245043\n",
      "Epoch 601:\n",
      "Loss train 0.00954590947413817 valid 0.009318611214164153\n",
      "Epoch 602:\n",
      "Loss train 0.009726249585859478 valid 0.010183816739751215\n",
      "Epoch 603:\n",
      "Loss train 0.00972774347057566 valid 0.015147719191787761\n",
      "Epoch 604:\n",
      "Loss train 0.009622701273299753 valid 0.009659175443361196\n",
      "Epoch 605:\n",
      "Loss train 0.009695012718439103 valid 0.009461329049712398\n",
      "Epoch 606:\n",
      "Loss train 0.009634344414342196 valid 0.008500554778560333\n",
      "Epoch 607:\n",
      "Loss train 0.009665686617605388 valid 0.009220988500850033\n",
      "Epoch 608:\n",
      "Loss train 0.009597731832414866 valid 0.009920662653590687\n",
      "Epoch 609:\n",
      "Loss train 0.009623958508949726 valid 0.008760240631324303\n",
      "Epoch 610:\n",
      "Loss train 0.009732046874240041 valid 0.009638715020637097\n",
      "Epoch 611:\n",
      "Loss train 0.009499585284851491 valid 0.012628591391537634\n",
      "Epoch 612:\n",
      "Loss train 0.009724107244051993 valid 0.011189736466294791\n",
      "Epoch 613:\n",
      "Loss train 0.009587099629454315 valid 0.008690105873431523\n",
      "Epoch 614:\n",
      "Loss train 0.009663863153662532 valid 0.009403206123426761\n",
      "Epoch 615:\n",
      "Loss train 0.009589956244453787 valid 0.010070131530090762\n",
      "Epoch 616:\n",
      "Loss train 0.009654465425293893 valid 0.008474215063071784\n",
      "Epoch 617:\n",
      "Loss train 0.009571386304683984 valid 0.009505910892276533\n",
      "Epoch 618:\n",
      "Loss train 0.009653969682753087 valid 0.009088051812164967\n",
      "Epoch 619:\n",
      "Loss train 0.009583189419005066 valid 0.008682533423916925\n",
      "Epoch 620:\n",
      "Loss train 0.009516754799988121 valid 0.00903992971722181\n",
      "Epoch 621:\n",
      "Loss train 0.009658958433195948 valid 0.009145092688484254\n",
      "Epoch 622:\n",
      "Loss train 0.009571937447879464 valid 0.009879704270394208\n",
      "Epoch 623:\n",
      "Loss train 0.009515437055379153 valid 0.008708313112978407\n",
      "Epoch 624:\n",
      "Loss train 0.009730589446146042 valid 0.010003244051560344\n",
      "Epoch 625:\n",
      "Loss train 0.009495598840061575 valid 0.008937701075130996\n",
      "Epoch 626:\n",
      "Loss train 0.00967491898406297 valid 0.010345448725203318\n",
      "Epoch 627:\n",
      "Loss train 0.009626486209686846 valid 0.009416359129427718\n",
      "Epoch 628:\n",
      "Loss train 0.009495124500710517 valid 0.00931580894712426\n",
      "Epoch 629:\n",
      "Loss train 0.00955298286722973 valid 0.009898578147251876\n",
      "Epoch 630:\n",
      "Loss train 0.009694330899976194 valid 0.009397421161205389\n",
      "Epoch 631:\n",
      "Loss train 0.009504807010293007 valid 0.010117908952656905\n",
      "Epoch 632:\n",
      "Loss train 0.009578381178434937 valid 0.009290389652697517\n",
      "Epoch 633:\n",
      "Loss train 0.009628646293189377 valid 0.008705636261075728\n",
      "Epoch 634:\n",
      "Loss train 0.009418155725114048 valid 0.008511889330951981\n",
      "Epoch 635:\n",
      "Loss train 0.009539206082932652 valid 0.008638955429634668\n",
      "Epoch 636:\n",
      "Loss train 0.009599480048753321 valid 0.010391547976679107\n",
      "Epoch 637:\n",
      "Loss train 0.009525363910477608 valid 0.00890452152201736\n",
      "Epoch 638:\n",
      "Loss train 0.00947963308589533 valid 0.008687753944892912\n",
      "Epoch 639:\n",
      "Loss train 0.009585836387239397 valid 0.009142337969503459\n",
      "Epoch 640:\n",
      "Loss train 0.009506415245123207 valid 0.008821015102982645\n",
      "Epoch 641:\n",
      "Loss train 0.009473632239270955 valid 0.00858042218057907\n",
      "Epoch 642:\n",
      "Loss train 0.009480048516299576 valid 0.009189348316816875\n",
      "Epoch 643:\n",
      "Loss train 0.009570184911601245 valid 0.009180560997978829\n",
      "Epoch 644:\n",
      "Loss train 0.009557969174347819 valid 0.008260545976882815\n",
      "Epoch 645:\n",
      "Loss train 0.009466931134928018 valid 0.008815980413740375\n",
      "Epoch 646:\n",
      "Loss train 0.009511500291060656 valid 0.00957796099239046\n",
      "Epoch 647:\n",
      "Loss train 0.009420417200308293 valid 0.01009933355194196\n",
      "Epoch 648:\n",
      "Loss train 0.009640523941256106 valid 0.011533052385272193\n",
      "Epoch 649:\n",
      "Loss train 0.009452892218250781 valid 0.008457033378149681\n",
      "Epoch 650:\n",
      "Loss train 0.009503611908294261 valid 0.009751571277427923\n",
      "Epoch 651:\n",
      "Loss train 0.009424353757407517 valid 0.011230352819170982\n",
      "Epoch 652:\n",
      "Loss train 0.009512977019883693 valid 0.009060491480090464\n",
      "Epoch 653:\n",
      "Loss train 0.009427688037510961 valid 0.008861486349472262\n",
      "Epoch 654:\n",
      "Loss train 0.009450874957256019 valid 0.008903788379162052\n",
      "Epoch 655:\n",
      "Loss train 0.009568724249955267 valid 0.008767821453500814\n",
      "Epoch 656:\n",
      "Loss train 0.009345526515506209 valid 0.008791868150556596\n",
      "Epoch 657:\n",
      "Loss train 0.00939241814473644 valid 0.009499966070163724\n",
      "Epoch 658:\n",
      "Loss train 0.009428743231110275 valid 0.008514095189956012\n",
      "Epoch 659:\n",
      "Loss train 0.009520691591780633 valid 0.011038916224148296\n",
      "Epoch 660:\n",
      "Loss train 0.009375966897699982 valid 0.012528430267903448\n",
      "Epoch 661:\n",
      "Loss train 0.00944037550361827 valid 0.012244026581853779\n",
      "Epoch 662:\n",
      "Loss train 0.009491093539167196 valid 0.009251371908780854\n",
      "Epoch 663:\n",
      "Loss train 0.009530126317869873 valid 0.008735796760796591\n",
      "Epoch 664:\n",
      "Loss train 0.009378815790172666 valid 0.009697310340380013\n",
      "Epoch 665:\n",
      "Loss train 0.009407791278325021 valid 0.00975105359803533\n",
      "Epoch 666:\n",
      "Loss train 0.009324664765037596 valid 0.010463841764700778\n",
      "Epoch 667:\n",
      "Loss train 0.009571042634081095 valid 0.009861633506632285\n",
      "Epoch 668:\n",
      "Loss train 0.009412998153828084 valid 0.00877456031630519\n",
      "Epoch 669:\n",
      "Loss train 0.00926593090640381 valid 0.008340841658983843\n",
      "Epoch 670:\n",
      "Loss train 0.0095690319663845 valid 0.008482878349778029\n",
      "Epoch 671:\n",
      "Loss train 0.009313031917437911 valid 0.009974358395592598\n",
      "Epoch 672:\n",
      "Loss train 0.009530701701529323 valid 0.009409154560527802\n",
      "Epoch 673:\n",
      "Loss train 0.009344743093010038 valid 0.008643743405973379\n",
      "Epoch 674:\n",
      "Loss train 0.009364137927535921 valid 0.0086160104031236\n",
      "Epoch 675:\n",
      "Loss train 0.009386931279208512 valid 0.008401644816442797\n",
      "Epoch 676:\n",
      "Loss train 0.009568846738431603 valid 0.008471001466471464\n",
      "Epoch 677:\n",
      "Loss train 0.009375868493691087 valid 0.01031181841152608\n",
      "Epoch 678:\n",
      "Loss train 0.009325100755319 valid 0.009824208094392276\n",
      "Epoch 679:\n",
      "Loss train 0.009375551624689251 valid 0.009985261329074343\n",
      "Epoch 680:\n",
      "Loss train 0.009364297495223581 valid 0.00837203174229398\n",
      "Epoch 681:\n",
      "Loss train 0.009409361404366792 valid 0.009483983831625699\n",
      "Epoch 682:\n",
      "Loss train 0.009399646539241075 valid 0.008503645239810292\n",
      "Epoch 683:\n",
      "Loss train 0.009302821804769337 valid 0.008943133292598831\n",
      "Epoch 684:\n",
      "Loss train 0.009419162609148771 valid 0.00839084538674058\n",
      "Epoch 685:\n",
      "Loss train 0.009317440910730512 valid 0.008638238437121562\n",
      "Epoch 686:\n",
      "Loss train 0.009391332906205207 valid 0.008588307252447455\n",
      "Epoch 687:\n",
      "Loss train 0.009330011460464447 valid 0.008368518128903032\n",
      "Epoch 688:\n",
      "Loss train 0.009467055018991232 valid 0.008849843173019449\n",
      "Epoch 689:\n",
      "Loss train 0.009221788567025215 valid 0.00881192753437823\n",
      "Epoch 690:\n",
      "Loss train 0.009362131266389042 valid 0.008247864236455637\n",
      "Epoch 691:\n",
      "Loss train 0.009369080036878585 valid 0.009635213193758508\n",
      "Epoch 692:\n",
      "Loss train 0.009314678891561925 valid 0.008045908160702053\n",
      "Epoch 693:\n",
      "Loss train 0.009385083384346217 valid 0.01022586846017516\n",
      "Epoch 694:\n",
      "Loss train 0.009280454826075583 valid 0.008108543293681622\n",
      "Epoch 695:\n",
      "Loss train 0.009425278990995139 valid 0.009379263072417442\n",
      "Epoch 696:\n",
      "Loss train 0.009300432518590241 valid 0.010090891436593709\n",
      "Epoch 697:\n",
      "Loss train 0.00933807253325358 valid 0.008591996407592896\n",
      "Epoch 698:\n",
      "Loss train 0.009461824988014997 valid 0.00986254792067879\n",
      "Epoch 699:\n",
      "Loss train 0.009120675841812044 valid 0.008815230157677415\n",
      "Epoch 700:\n",
      "Loss train 0.009300548843573779 valid 0.00850002212712968\n",
      "Epoch 701:\n",
      "Loss train 0.009365757841151208 valid 0.008689613244177958\n",
      "Epoch 702:\n",
      "Loss train 0.00913639000756666 valid 0.0096499382287828\n",
      "Epoch 703:\n",
      "Loss train 0.009315010369755327 valid 0.008516375076666128\n",
      "Epoch 704:\n",
      "Loss train 0.009265295051969588 valid 0.010518809028849059\n",
      "Epoch 705:\n",
      "Loss train 0.009408942787908018 valid 0.009273437019603979\n",
      "Epoch 706:\n",
      "Loss train 0.009270469234790652 valid 0.00857145026142508\n",
      "Epoch 707:\n",
      "Loss train 0.009304928062018008 valid 0.008808930317839981\n",
      "Epoch 708:\n",
      "Loss train 0.00924830678384751 valid 0.008651003403214721\n",
      "Epoch 709:\n",
      "Loss train 0.009235668927431107 valid 0.008255497837297135\n",
      "Epoch 710:\n",
      "Loss train 0.00934533300390467 valid 0.00886447985773057\n",
      "Epoch 711:\n",
      "Loss train 0.009310791911091656 valid 0.00913973819793022\n",
      "Epoch 712:\n",
      "Loss train 0.00921582917543128 valid 0.00900889054649876\n",
      "Epoch 713:\n",
      "Loss train 0.009233485726173968 valid 0.008810005994188212\n",
      "Epoch 714:\n",
      "Loss train 0.009263203768990935 valid 0.008154895219613255\n",
      "Epoch 715:\n",
      "Loss train 0.009164275749586523 valid 0.009846826993422545\n",
      "Epoch 716:\n",
      "Loss train 0.009442284075543284 valid 0.008688417341399978\n",
      "Epoch 717:\n",
      "Loss train 0.009139466932043433 valid 0.008487821702907381\n",
      "Epoch 718:\n",
      "Loss train 0.009358474664390087 valid 0.00837774474948193\n",
      "Epoch 719:\n",
      "Loss train 0.009182191074825824 valid 0.008058789388788422\n",
      "Epoch 720:\n",
      "Loss train 0.00930680222529918 valid 0.010510112061004336\n",
      "Epoch 721:\n",
      "Loss train 0.009162695950828493 valid 0.008173025746897218\n",
      "Epoch 722:\n",
      "Loss train 0.009378837203141302 valid 0.008373834304236568\n",
      "Epoch 723:\n",
      "Loss train 0.009168232669588178 valid 0.010121897231440434\n",
      "Epoch 724:\n",
      "Loss train 0.009201698100194335 valid 0.012274625166537986\n",
      "Epoch 725:\n",
      "Loss train 0.009171949354931713 valid 0.008521201239178378\n",
      "Epoch 726:\n",
      "Loss train 0.009280269103124737 valid 0.010332995099710434\n",
      "Epoch 727:\n",
      "Loss train 0.009319074757862837 valid 0.008342692466594814\n",
      "Epoch 728:\n",
      "Loss train 0.009132038457319141 valid 0.008157279694115132\n",
      "Epoch 729:\n",
      "Loss train 0.009230275072623044 valid 0.008506089595472398\n",
      "Epoch 730:\n",
      "Loss train 0.009225737356115133 valid 0.011444660745161742\n",
      "Epoch 731:\n",
      "Loss train 0.009250578424893319 valid 0.00925317616979277\n",
      "Epoch 732:\n",
      "Loss train 0.00914387769298628 valid 0.014494633598720616\n",
      "Epoch 733:\n",
      "Loss train 0.009228532386012376 valid 0.011001642783497672\n",
      "Epoch 734:\n",
      "Loss train 0.009160796958953141 valid 0.008806346147562052\n",
      "Epoch 735:\n",
      "Loss train 0.009169153744354844 valid 0.00848866861385912\n",
      "Epoch 736:\n",
      "Loss train 0.00921800396963954 valid 0.008814232794762733\n",
      "Epoch 737:\n",
      "Loss train 0.009145331910811365 valid 0.008483935742621612\n",
      "Epoch 738:\n",
      "Loss train 0.009176765799988062 valid 0.00822567427525395\n",
      "Epoch 739:\n",
      "Loss train 0.009323529538698494 valid 0.008424454832188224\n",
      "Epoch 740:\n",
      "Loss train 0.009168589999433607 valid 0.008291834925753352\n",
      "Epoch 741:\n",
      "Loss train 0.009164879919495434 valid 0.009699555609048005\n",
      "Epoch 742:\n",
      "Loss train 0.00925411780597642 valid 0.008674062594251424\n",
      "Epoch 743:\n",
      "Loss train 0.009035575077868998 valid 0.009894327331810483\n",
      "Epoch 744:\n",
      "Loss train 0.009316916667390615 valid 0.009087509119704589\n",
      "Epoch 745:\n",
      "Loss train 0.009135210555512458 valid 0.008220417043380559\n",
      "Epoch 746:\n",
      "Loss train 0.009252662899438291 valid 0.00828831093446656\n",
      "Epoch 747:\n",
      "Loss train 0.009066563772503287 valid 0.00816993727542042\n",
      "Epoch 748:\n",
      "Loss train 0.009230427746661007 valid 0.00913861709105224\n",
      "Epoch 749:\n",
      "Loss train 0.009164638612419366 valid 0.008934261965147423\n",
      "Epoch 750:\n",
      "Loss train 0.009269931052811443 valid 0.008147567158154215\n",
      "Epoch 751:\n",
      "Loss train 0.009044010226149111 valid 0.00950305682962919\n",
      "Epoch 752:\n",
      "Loss train 0.009251399455592037 valid 0.009259385922484136\n",
      "Epoch 753:\n",
      "Loss train 0.009107896384317428 valid 0.008527298981081299\n",
      "Epoch 754:\n",
      "Loss train 0.009202845682390035 valid 0.008607565964295155\n",
      "Epoch 755:\n",
      "Loss train 0.009222880866378546 valid 0.00869648777675948\n",
      "Epoch 756:\n",
      "Loss train 0.00898638070654124 valid 0.008097301397549767\n",
      "Epoch 757:\n",
      "Loss train 0.009219272653572261 valid 0.00824353824435854\n",
      "Epoch 758:\n",
      "Loss train 0.008990532614290715 valid 0.01085058824632867\n",
      "Epoch 759:\n",
      "Loss train 0.009185170320793986 valid 0.010748606351923049\n",
      "Epoch 760:\n",
      "Loss train 0.009118167515844107 valid 0.00897631868935956\n",
      "Epoch 761:\n",
      "Loss train 0.009002588039264082 valid 0.008218944392190253\n",
      "Epoch 762:\n",
      "Loss train 0.009127007228787988 valid 0.010763937271712532\n",
      "Epoch 763:\n",
      "Loss train 0.009141916916240007 valid 0.008286474489068094\n",
      "Epoch 764:\n",
      "Loss train 0.008978055753279477 valid 0.009254462463955074\n",
      "Epoch 765:\n",
      "Loss train 0.00910191174224019 valid 0.009308940498329652\n",
      "Epoch 766:\n",
      "Loss train 0.009177071321755648 valid 0.009673179323776739\n",
      "Epoch 767:\n",
      "Loss train 0.00903889910876751 valid 0.008337370556953548\n",
      "Epoch 768:\n",
      "Loss train 0.009073572403285652 valid 0.00807666059093914\n",
      "Epoch 769:\n",
      "Loss train 0.009256400000769644 valid 0.00857966338036675\n",
      "Epoch 770:\n",
      "Loss train 0.009068228482734413 valid 0.007920605507405282\n",
      "Epoch 771:\n",
      "Loss train 0.009055583957117051 valid 0.008508616567185505\n",
      "Epoch 772:\n",
      "Loss train 0.009065340463537724 valid 0.0084758200201582\n",
      "Epoch 773:\n",
      "Loss train 0.009157089202199131 valid 0.009642226139012375\n",
      "Epoch 774:\n",
      "Loss train 0.009088758955243975 valid 0.00821622873390714\n",
      "Epoch 775:\n",
      "Loss train 0.009064566375687718 valid 0.009690216472945934\n",
      "Epoch 776:\n",
      "Loss train 0.009092684004921466 valid 0.008762850601338088\n",
      "Epoch 777:\n",
      "Loss train 0.009189159627538175 valid 0.008203260516888666\n",
      "Epoch 778:\n",
      "Loss train 0.008871940864250064 valid 0.00811557396867351\n",
      "Epoch 779:\n",
      "Loss train 0.009084756161551922 valid 0.008539796697118424\n",
      "Epoch 780:\n",
      "Loss train 0.009235032737255096 valid 0.008430453791472512\n",
      "Epoch 781:\n",
      "Loss train 0.008893713320605456 valid 0.008113839150905753\n",
      "Epoch 782:\n",
      "Loss train 0.008981207547709346 valid 0.00919385008557386\n",
      "Epoch 783:\n",
      "Loss train 0.009117288052570074 valid 0.007885828533030286\n",
      "Epoch 784:\n",
      "Loss train 0.008955254687927664 valid 0.007984518170064379\n",
      "Epoch 785:\n",
      "Loss train 0.009087506271433085 valid 0.009533800343411796\n",
      "Epoch 786:\n",
      "Loss train 0.008964088823180645 valid 0.010342688322031965\n",
      "Epoch 787:\n",
      "Loss train 0.00899945495929569 valid 0.009178156250190026\n",
      "Epoch 788:\n",
      "Loss train 0.009041032868903131 valid 0.00790330018232058\n",
      "Epoch 789:\n",
      "Loss train 0.00901757124857977 valid 0.011996171726576856\n",
      "Epoch 790:\n",
      "Loss train 0.008969970447942614 valid 0.012951345957011334\n",
      "Epoch 791:\n",
      "Loss train 0.009119790921453387 valid 0.01233289441347296\n",
      "Epoch 792:\n",
      "Loss train 0.00905389383621514 valid 0.008020398672350581\n",
      "Epoch 793:\n",
      "Loss train 0.009008334241807461 valid 0.011809883257077217\n",
      "Epoch 794:\n",
      "Loss train 0.008955078181345015 valid 0.009450615983609106\n",
      "Epoch 795:\n",
      "Loss train 0.009082126087509095 valid 0.008576555932120923\n",
      "Epoch 796:\n",
      "Loss train 0.009033140521962196 valid 0.00807875782183812\n",
      "Epoch 797:\n",
      "Loss train 0.00896577962534502 valid 0.009450643256819043\n",
      "Epoch 798:\n",
      "Loss train 0.008932331045158207 valid 0.00791539989479298\n",
      "Epoch 799:\n",
      "Loss train 0.008974342701025307 valid 0.008627228796439646\n",
      "Epoch 800:\n",
      "Loss train 0.008989594910759478 valid 0.00819317634697665\n",
      "Epoch 801:\n",
      "Loss train 0.008961742006707936 valid 0.008631831375990844\n",
      "Epoch 802:\n",
      "Loss train 0.008947643962688745 valid 0.008405221109527721\n",
      "Epoch 803:\n",
      "Loss train 0.009005434343591332 valid 0.007948352065256858\n",
      "Epoch 804:\n",
      "Loss train 0.008979450239334255 valid 0.007981089463426617\n",
      "Epoch 805:\n",
      "Loss train 0.009127498629502953 valid 0.009101456449357974\n",
      "Epoch 806:\n",
      "Loss train 0.008774356657173485 valid 0.012195102016679407\n",
      "Epoch 807:\n",
      "Loss train 0.009055177347734571 valid 0.008705603474756648\n",
      "Epoch 808:\n",
      "Loss train 0.00900789616536349 valid 0.011957419825563564\n",
      "Epoch 809:\n",
      "Loss train 0.008855437886901199 valid 0.007966737028338814\n",
      "Epoch 810:\n",
      "Loss train 0.008956801198888571 valid 0.00792120178503127\n",
      "Epoch 811:\n",
      "Loss train 0.00902761549130082 valid 0.00829989393460171\n",
      "Epoch 812:\n",
      "Loss train 0.008967693153768778 valid 0.017560532956872807\n",
      "Epoch 813:\n",
      "Loss train 0.008895264918915928 valid 0.008910201927572184\n",
      "Epoch 814:\n",
      "Loss train 0.009023859872482717 valid 0.008594970727383364\n",
      "Epoch 815:\n",
      "Loss train 0.009050872236955911 valid 0.008048007239146667\n",
      "Epoch 816:\n",
      "Loss train 0.00890484721120447 valid 0.01274923552409009\n",
      "Epoch 817:\n",
      "Loss train 0.009017186260316522 valid 0.008004704703068705\n",
      "Epoch 818:\n",
      "Loss train 0.008824023237917573 valid 0.011501264315888455\n",
      "Epoch 819:\n",
      "Loss train 0.008915959625504911 valid 0.008449174922653804\n",
      "Epoch 820:\n",
      "Loss train 0.009066333417780697 valid 0.007834686843692831\n",
      "Epoch 821:\n",
      "Loss train 0.008811376727651804 valid 0.009599854049176548\n",
      "Epoch 822:\n",
      "Loss train 0.009066784728318452 valid 0.00782889525932632\n",
      "Epoch 823:\n",
      "Loss train 0.008854852201417088 valid 0.008000955244226093\n",
      "Epoch 824:\n",
      "Loss train 0.008964871417731047 valid 0.0107669380242408\n",
      "Epoch 825:\n",
      "Loss train 0.008910829530097544 valid 0.00805627069429935\n",
      "Epoch 826:\n",
      "Loss train 0.008894430544693023 valid 0.021050381044557392\n",
      "Epoch 827:\n",
      "Loss train 0.009037423006724566 valid 0.010157063891291333\n",
      "Epoch 828:\n",
      "Loss train 0.008879164834041149 valid 0.01524826299232005\n",
      "Epoch 829:\n",
      "Loss train 0.00883225287683308 valid 0.015190661172529112\n",
      "Epoch 830:\n",
      "Loss train 0.009014839500188828 valid 0.008539408510878295\n",
      "Epoch 831:\n",
      "Loss train 0.008977669386658817 valid 0.00812814519951256\n",
      "Epoch 832:\n",
      "Loss train 0.008764556968118996 valid 0.008073620559846855\n",
      "Epoch 833:\n",
      "Loss train 0.009082664520014078 valid 0.008235893626632998\n",
      "Epoch 834:\n",
      "Loss train 0.008844492522533984 valid 0.012896420084206784\n",
      "Epoch 835:\n",
      "Loss train 0.008926362412981689 valid 0.007915407162675978\n",
      "Epoch 836:\n",
      "Loss train 0.008905579012352973 valid 0.008539600635544688\n",
      "Epoch 837:\n",
      "Loss train 0.008795810812152922 valid 0.0076824379964169675\n",
      "Epoch 838:\n",
      "Loss train 0.00884126016497612 valid 0.008443034148391388\n",
      "Epoch 839:\n",
      "Loss train 0.008933729614596814 valid 0.008231822734155568\n",
      "Epoch 840:\n",
      "Loss train 0.008818487908691167 valid 0.009716242854702187\n",
      "Epoch 841:\n",
      "Loss train 0.008955861638765782 valid 0.008386960442755283\n",
      "Epoch 842:\n",
      "Loss train 0.008891444545704872 valid 0.009396714523355512\n",
      "Epoch 843:\n",
      "Loss train 0.008905191345605998 valid 0.007863928867769066\n",
      "Epoch 844:\n",
      "Loss train 0.00880786898266524 valid 0.008179890105221781\n",
      "Epoch 845:\n",
      "Loss train 0.008831538029015065 valid 0.007993584064132907\n",
      "Epoch 846:\n",
      "Loss train 0.008809885907918215 valid 0.00933817059449507\n",
      "Epoch 847:\n",
      "Loss train 0.008857705740258098 valid 0.009588110806185997\n",
      "Epoch 848:\n",
      "Loss train 0.008927727372851222 valid 0.0077498160411491965\n",
      "Epoch 849:\n",
      "Loss train 0.008825939236208797 valid 0.008975882248264556\n",
      "Epoch 850:\n",
      "Loss train 0.008816103921737522 valid 0.008063622708851364\n",
      "Epoch 851:\n",
      "Loss train 0.008852779346518218 valid 0.008047076204902635\n",
      "Epoch 852:\n",
      "Loss train 0.008871983722317964 valid 0.011567444444514371\n",
      "Epoch 853:\n",
      "Loss train 0.008749779773876072 valid 0.008977109980586219\n",
      "Epoch 854:\n",
      "Loss train 0.008886746855452658 valid 0.008035173236098132\n",
      "Epoch 855:\n",
      "Loss train 0.008869919294957071 valid 0.00987269096858228\n",
      "Epoch 856:\n",
      "Loss train 0.008871262951288372 valid 0.007859798255641473\n",
      "Epoch 857:\n",
      "Loss train 0.008852611006237566 valid 0.008474549458597094\n",
      "Epoch 858:\n",
      "Loss train 0.00875136882532388 valid 0.008073046554101243\n",
      "Epoch 859:\n",
      "Loss train 0.008781463278923183 valid 0.008176678448185903\n",
      "Epoch 860:\n",
      "Loss train 0.008781692046672106 valid 0.008138950041282412\n",
      "Epoch 861:\n",
      "Loss train 0.008859287674538792 valid 0.009033970635120894\n",
      "Epoch 862:\n",
      "Loss train 0.008722845467738808 valid 0.007838857187040013\n",
      "Epoch 863:\n",
      "Loss train 0.008872961163520814 valid 0.007907046724431842\n",
      "Epoch 864:\n",
      "Loss train 0.008829627287108451 valid 0.008073585670734962\n",
      "Epoch 865:\n",
      "Loss train 0.008719492786098272 valid 0.00820136904707731\n",
      "Epoch 866:\n",
      "Loss train 0.008803990868851542 valid 0.009438733506927198\n",
      "Epoch 867:\n",
      "Loss train 0.008846836356911808 valid 0.007850151085810936\n",
      "Epoch 868:\n",
      "Loss train 0.008733934103511275 valid 0.0076900931403259595\n",
      "Epoch 869:\n",
      "Loss train 0.008850616691168397 valid 0.013046071176185161\n",
      "Epoch 870:\n",
      "Loss train 0.008731224035378545 valid 0.008167884298484694\n",
      "Epoch 871:\n",
      "Loss train 0.008784101228695362 valid 0.007928598418000601\n",
      "Epoch 872:\n",
      "Loss train 0.0088904524827376 valid 0.007894280555983617\n",
      "Epoch 873:\n",
      "Loss train 0.008740610927343368 valid 0.00763451966634672\n",
      "Epoch 874:\n",
      "Loss train 0.008753477911930532 valid 0.00847730846084627\n",
      "Epoch 875:\n",
      "Loss train 0.008914658185560257 valid 0.009607585200688665\n",
      "Epoch 876:\n",
      "Loss train 0.008722519927658141 valid 0.007918852077464933\n",
      "Epoch 877:\n",
      "Loss train 0.008767155630514025 valid 0.008619593052905149\n",
      "Epoch 878:\n",
      "Loss train 0.008740899242460728 valid 0.007954807613983308\n",
      "Epoch 879:\n",
      "Loss train 0.00878427558625117 valid 0.01055877684438655\n",
      "Epoch 880:\n",
      "Loss train 0.008745990143623202 valid 0.008165868702834507\n",
      "Epoch 881:\n",
      "Loss train 0.008810279147699475 valid 0.008181582915758639\n",
      "Epoch 882:\n",
      "Loss train 0.008701276706997305 valid 0.007849357758159963\n",
      "Epoch 883:\n",
      "Loss train 0.008787270810455083 valid 0.007664872246318168\n",
      "Epoch 884:\n",
      "Loss train 0.008763629864435643 valid 0.008741899973812951\n",
      "Epoch 885:\n",
      "Loss train 0.00873269160790369 valid 0.007677216403013632\n",
      "Epoch 886:\n",
      "Loss train 0.008844340994022787 valid 0.011048459160992312\n",
      "Epoch 887:\n",
      "Loss train 0.008704180679284036 valid 0.008579103189896208\n",
      "Epoch 888:\n",
      "Loss train 0.008669064794201403 valid 0.011120806210689473\n",
      "Epoch 889:\n",
      "Loss train 0.008802551127504558 valid 0.00853885147064305\n",
      "Epoch 890:\n",
      "Loss train 0.008684083551168441 valid 0.007768830296953557\n",
      "Epoch 891:\n",
      "Loss train 0.008756761963944882 valid 0.00854631050631198\n",
      "Epoch 892:\n",
      "Loss train 0.008771676955278963 valid 0.007966153826439386\n",
      "Epoch 893:\n",
      "Loss train 0.00861366181122139 valid 0.007965338398705758\n",
      "Epoch 894:\n",
      "Loss train 0.008747239688877017 valid 0.008997023612872856\n",
      "Epoch 895:\n",
      "Loss train 0.008779951435048132 valid 0.00794970534346618\n",
      "Epoch 896:\n",
      "Loss train 0.008663297810126097 valid 0.010209314503350348\n",
      "Epoch 897:\n",
      "Loss train 0.008832625799812376 valid 0.007986985753281729\n",
      "Epoch 898:\n",
      "Loss train 0.00856217990675941 valid 0.008984007328199586\n",
      "Epoch 899:\n",
      "Loss train 0.008903585322201252 valid 0.008300060230191815\n",
      "Epoch 900:\n",
      "Loss train 0.008734666586853564 valid 0.010270820504332721\n",
      "Epoch 901:\n",
      "Loss train 0.00873004959197715 valid 0.007871514745856545\n",
      "Epoch 902:\n",
      "Loss train 0.008632131167221814 valid 0.008637044056816395\n",
      "Epoch 903:\n",
      "Loss train 0.00865111861238256 valid 0.009877932025994657\n",
      "Epoch 904:\n",
      "Loss train 0.00875165993347764 valid 0.008042319510849263\n",
      "Epoch 905:\n",
      "Loss train 0.008742697432637215 valid 0.011101469820100846\n",
      "Epoch 906:\n",
      "Loss train 0.008630288385786116 valid 0.0075647521944419635\n",
      "Epoch 907:\n",
      "Loss train 0.00872601741598919 valid 0.007606246440147809\n",
      "Epoch 908:\n",
      "Loss train 0.008572948245797306 valid 0.008277571727961496\n",
      "Epoch 909:\n",
      "Loss train 0.008707681429106742 valid 0.012997918354690835\n",
      "Epoch 910:\n",
      "Loss train 0.008684379719663411 valid 0.008146993908923286\n",
      "Epoch 911:\n",
      "Loss train 0.008656775950454176 valid 0.010126897157623865\n",
      "Epoch 912:\n",
      "Loss train 0.008653421598952264 valid 0.008015109442924422\n",
      "Epoch 913:\n",
      "Loss train 0.008624393349513412 valid 0.007421036300332878\n",
      "Epoch 914:\n",
      "Loss train 0.008698365936987102 valid 0.011675080734820703\n",
      "Epoch 915:\n",
      "Loss train 0.008754761936608702 valid 0.008166751470889393\n",
      "Epoch 916:\n",
      "Loss train 0.008555701564066112 valid 0.01019066278288059\n",
      "Epoch 917:\n",
      "Loss train 0.008629057670012116 valid 0.007525177219712586\n",
      "Epoch 918:\n",
      "Loss train 0.008710284480359405 valid 0.008049082224589665\n",
      "Epoch 919:\n",
      "Loss train 0.008560339917428791 valid 0.009183024812686826\n",
      "Epoch 920:\n",
      "Loss train 0.008605502115096897 valid 0.008585079775104277\n",
      "Epoch 921:\n",
      "Loss train 0.008744302505627275 valid 0.007965166321204926\n",
      "Epoch 922:\n",
      "Loss train 0.008692252356093376 valid 0.00827437367418358\n",
      "Epoch 923:\n",
      "Loss train 0.008527824657969177 valid 0.009191471187549692\n",
      "Epoch 924:\n",
      "Loss train 0.008705653253942729 valid 0.008380635390693966\n",
      "Epoch 925:\n",
      "Loss train 0.008660119203384966 valid 0.007963005239320612\n",
      "Epoch 926:\n",
      "Loss train 0.00866255532298237 valid 0.008523349357748004\n",
      "Epoch 927:\n",
      "Loss train 0.008581740273628385 valid 0.008273680366127897\n",
      "Epoch 928:\n",
      "Loss train 0.00860405722958967 valid 0.007779143909041928\n",
      "Epoch 929:\n",
      "Loss train 0.008650563186500222 valid 0.009836418483399419\n",
      "Epoch 930:\n",
      "Loss train 0.008605345951393246 valid 0.00781315234789981\n",
      "Epoch 931:\n",
      "Loss train 0.008620643520727753 valid 0.008246146797238858\n",
      "Epoch 932:\n",
      "Loss train 0.008573869763873518 valid 0.009413513245030255\n",
      "Epoch 933:\n",
      "Loss train 0.008652080849278718 valid 0.009204362485608727\n",
      "Epoch 934:\n",
      "Loss train 0.008547780504450202 valid 0.009228054082534543\n",
      "Epoch 935:\n",
      "Loss train 0.008633558527100831 valid 0.00765566639623909\n",
      "Epoch 936:\n",
      "Loss train 0.008588422506581992 valid 0.011322661132137857\n",
      "Epoch 937:\n",
      "Loss train 0.008504739049356431 valid 0.00889354503898007\n",
      "Epoch 938:\n",
      "Loss train 0.008723624030593782 valid 0.007837112585798703\n",
      "Epoch 939:\n",
      "Loss train 0.008654963168781251 valid 0.008464141369912227\n",
      "Epoch 940:\n",
      "Loss train 0.008526023632846772 valid 0.008147755459018536\n",
      "Epoch 941:\n",
      "Loss train 0.008603999889455736 valid 0.008613495639629971\n",
      "Epoch 942:\n",
      "Loss train 0.00860478936927393 valid 0.008654714558720291\n",
      "Epoch 943:\n",
      "Loss train 0.008592545946128667 valid 0.008871262661480568\n",
      "Epoch 944:\n",
      "Loss train 0.008728908921126276 valid 0.007706821407446873\n",
      "Epoch 945:\n",
      "Loss train 0.008583941453136503 valid 0.009889794929451934\n",
      "Epoch 946:\n",
      "Loss train 0.008544158764183522 valid 0.007437233645969253\n",
      "Epoch 947:\n",
      "Loss train 0.008553624460939317 valid 0.008652167094269464\n",
      "Epoch 948:\n",
      "Loss train 0.008614513914566488 valid 0.009094158395203524\n",
      "Epoch 949:\n",
      "Loss train 0.008549742829985916 valid 0.008073444384204372\n",
      "Epoch 950:\n",
      "Loss train 0.00862743627326563 valid 0.00746467086785007\n",
      "Epoch 951:\n",
      "Loss train 0.008667652073316276 valid 0.007548808620261268\n",
      "Epoch 952:\n",
      "Loss train 0.00847611017152667 valid 0.0077384944683887395\n",
      "Epoch 953:\n",
      "Loss train 0.0087045072093606 valid 0.00935038352240924\n",
      "Epoch 954:\n",
      "Loss train 0.008546103488188237 valid 0.010264828806850552\n",
      "Epoch 955:\n",
      "Loss train 0.00857476859819144 valid 0.009967510607822702\n",
      "Epoch 956:\n",
      "Loss train 0.008611529289744794 valid 0.008099457033569832\n",
      "Epoch 957:\n",
      "Loss train 0.008507383567746728 valid 0.0092883649313128\n",
      "Epoch 958:\n",
      "Loss train 0.008632995161227883 valid 0.007860320015764904\n",
      "Epoch 959:\n",
      "Loss train 0.008513083539437503 valid 0.008458753958282267\n",
      "Epoch 960:\n",
      "Loss train 0.008730065169278533 valid 0.00824752213959084\n",
      "Epoch 961:\n",
      "Loss train 0.008456175655592233 valid 0.010013244281555644\n",
      "Epoch 962:\n",
      "Loss train 0.00850862517626956 valid 0.008646871192576148\n",
      "Epoch 963:\n",
      "Loss train 0.00848713609809056 valid 0.007711479387538605\n",
      "Epoch 964:\n",
      "Loss train 0.008573971960227937 valid 0.008195921225899227\n",
      "Epoch 965:\n",
      "Loss train 0.008532760054804385 valid 0.010124482903130297\n",
      "Epoch 966:\n",
      "Loss train 0.008658211365342141 valid 0.007730116681649161\n",
      "Epoch 967:\n",
      "Loss train 0.008395143689587712 valid 0.008147063572109528\n",
      "Epoch 968:\n",
      "Loss train 0.008572437097784132 valid 0.007498259321371245\n",
      "Epoch 969:\n",
      "Loss train 0.008622057032305747 valid 0.008360646573341877\n",
      "Epoch 970:\n",
      "Loss train 0.008392062880098819 valid 0.008140694588249604\n",
      "Epoch 971:\n",
      "Loss train 0.00855593754304573 valid 0.008943539893838278\n",
      "Epoch 972:\n",
      "Loss train 0.008518569933716207 valid 0.008798164505241549\n",
      "Epoch 973:\n",
      "Loss train 0.008546858445275574 valid 0.007662462514149052\n",
      "Epoch 974:\n",
      "Loss train 0.00848977749235928 valid 0.007563825292324806\n",
      "Epoch 975:\n",
      "Loss train 0.008482081159949303 valid 0.007803927532272985\n",
      "Epoch 976:\n",
      "Loss train 0.008596264686435462 valid 0.00776087639487878\n",
      "Epoch 977:\n",
      "Loss train 0.008470214575063438 valid 0.008178770345475877\n",
      "Epoch 978:\n",
      "Loss train 0.008562694446649402 valid 0.008906772248303574\n",
      "Epoch 979:\n",
      "Loss train 0.008380575141403824 valid 0.007682628018357504\n",
      "Epoch 980:\n",
      "Loss train 0.008609142381697894 valid 0.007360194096743266\n",
      "Epoch 981:\n",
      "Loss train 0.0084401053651236 valid 0.007751782632445519\n",
      "Epoch 982:\n",
      "Loss train 0.00851468903850764 valid 0.010838841000042109\n",
      "Epoch 983:\n",
      "Loss train 0.008575300042983144 valid 0.009043449276019782\n",
      "Epoch 984:\n",
      "Loss train 0.008394287814386189 valid 0.009148109710188273\n",
      "Epoch 985:\n",
      "Loss train 0.008545467016287148 valid 0.007566333057076798\n",
      "Epoch 986:\n",
      "Loss train 0.008467969270423054 valid 0.010545694277072862\n",
      "Epoch 987:\n",
      "Loss train 0.008490425030700863 valid 0.007577042942508714\n",
      "Epoch 988:\n",
      "Loss train 0.008516395028680563 valid 0.007685732791300137\n",
      "Epoch 989:\n",
      "Loss train 0.008458166874013842 valid 0.00954162674922471\n",
      "Epoch 990:\n",
      "Loss train 0.00848340117605403 valid 0.007758336888723056\n",
      "Epoch 991:\n",
      "Loss train 0.008399024972226471 valid 0.008190426484260974\n",
      "Epoch 992:\n",
      "Loss train 0.008478930862620472 valid 0.007831831364681371\n",
      "Epoch 993:\n",
      "Loss train 0.008394550160039217 valid 0.015259346974080636\n",
      "Epoch 994:\n",
      "Loss train 0.008473979737609626 valid 0.009381434126776842\n",
      "Epoch 995:\n",
      "Loss train 0.008524240780156105 valid 0.008614695118722732\n",
      "Epoch 996:\n",
      "Loss train 0.00838544222805649 valid 0.007714252131171137\n",
      "Epoch 997:\n",
      "Loss train 0.008479405050631612 valid 0.008008097564338394\n",
      "Epoch 998:\n",
      "Loss train 0.008470604641828685 valid 0.007736441229707842\n",
      "Epoch 999:\n",
      "Loss train 0.008573053273838014 valid 0.0113242075054992\n",
      "Epoch 1000:\n",
      "Loss train 0.008229341604746879 valid 0.008873189105283927\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5000\n",
    "learning_rate = 1e-4\n",
    "n_epochs = 1000\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {}:'.format(epoch + 1))\n",
    "\n",
    "    model.train()\n",
    "    avg_loss = train_one_epoch()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_outputs = torch.squeeze(model(torch.from_numpy(valid_features_normalized.astype(\"float32\"))))\n",
    "        valid_loss = loss_function(valid_outputs, torch.from_numpy(valid_fco2).to(torch.device(\"cuda\"))).detach().cpu().item()\n",
    "\n",
    "    print('Loss train {} valid {}'.format(avg_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "636b9fdd-d024-4d43-9e2c-9312402703b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (for consistency check):  0.008873189105283926\n",
      "RMSE:  0.09419760668554125\n",
      "Maximum absolute deviation:  3.685029021252376\n",
      "99.9th percentile of absolute deviation (1000 val's larger):  0.5024679256545325\n",
      "99.99th percentile of absolute deviation (100 val's larger):  1.0437086963047757\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE (for consistency check): \", MSE(valid_fco2, model_output_after_training))\n",
    "print(\"RMSE: \", np.sqrt(MSE(valid_fco2, model_output_after_training)))\n",
    "print(\"Maximum absolute deviation: \", np.max(np.abs(model_output_after_training-valid_fco2)))\n",
    "print(\"99.9th percentile of absolute deviation (1000 val's larger): \", np.percentile(np.abs(model_output_after_training-valid_fco2), q=99.9))\n",
    "print(\"99.99th percentile of absolute deviation (100 val's larger): \", np.percentile(np.abs(model_output_after_training-valid_fco2), q=99.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce220a9a-1e65-43cf-b4e3-a9c3535c03f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Loss train 0.05446089960169047 valid 0.007101428872533365\n",
      "Epoch 2:\n",
      "Loss train 0.0070328404707834125 valid 0.007115673495090755\n",
      "Epoch 3:\n",
      "Loss train 0.007036611973308027 valid 0.007096195535410926\n",
      "Epoch 4:\n",
      "Loss train 0.007033055070787668 valid 0.007072334842601043\n",
      "Epoch 5:\n",
      "Loss train 0.007051302180625499 valid 0.007149934007372583\n",
      "Epoch 6:\n",
      "Loss train 0.007080978397279978 valid 0.007080043501969307\n",
      "Epoch 7:\n",
      "Loss train 0.007045591003261507 valid 0.007253667795009663\n",
      "Epoch 8:\n",
      "Loss train 0.007107173465192318 valid 0.007102049884622727\n",
      "Epoch 9:\n",
      "Loss train 0.0070796957006677985 valid 0.007119826456367083\n",
      "Epoch 10:\n",
      "Loss train 0.007086028037592769 valid 0.007111263037902604\n",
      "Epoch 11:\n",
      "Loss train 0.00707529665902257 valid 0.007098995994801255\n",
      "Epoch 12:\n",
      "Loss train 0.007131665009073913 valid 0.007146409355177554\n",
      "Epoch 13:\n",
      "Loss train 0.007162262815982103 valid 0.007136563775303806\n",
      "Epoch 14:\n",
      "Loss train 0.0071934646880254145 valid 0.007238774247889133\n",
      "Epoch 15:\n",
      "Loss train 0.00718917294871062 valid 0.00720137824230927\n",
      "Epoch 16:\n",
      "Loss train 0.007217600354924798 valid 0.0071247622393654725\n",
      "Epoch 17:\n",
      "Loss train 0.007143388660624623 valid 0.007237534159596183\n",
      "Epoch 18:\n",
      "Loss train 0.007453007353469729 valid 0.0071293494285217755\n",
      "Epoch 19:\n",
      "Loss train 0.007791881295852363 valid 0.007424278322905572\n",
      "Epoch 20:\n",
      "Loss train 0.007895305077545345 valid 0.008497042819409382\n",
      "Epoch 21:\n",
      "Loss train 0.007792686019092799 valid 0.007336865753670734\n",
      "Epoch 22:\n",
      "Loss train 0.008918722127564251 valid 0.009217073582715722\n",
      "Epoch 23:\n",
      "Loss train 0.0073720813589170575 valid 0.007528676302684773\n",
      "Epoch 24:\n",
      "Loss train 0.00802988804411143 valid 0.007215753759022113\n",
      "Epoch 25:\n",
      "Loss train 0.007905979589559138 valid 0.008212810812320939\n",
      "Epoch 26:\n",
      "Loss train 0.007961194710806013 valid 0.007169782080549723\n",
      "Epoch 27:\n",
      "Loss train 0.008213088531047106 valid 0.007702324568956347\n",
      "Epoch 28:\n",
      "Loss train 0.007659227447584271 valid 0.007244909628657058\n",
      "Epoch 29:\n",
      "Loss train 0.008362475517205894 valid 0.007425087456893435\n",
      "Epoch 30:\n",
      "Loss train 0.00847699785605073 valid 0.00723967720289831\n",
      "Epoch 31:\n",
      "Loss train 0.00766993219498545 valid 0.007178575498345808\n",
      "Epoch 32:\n",
      "Loss train 0.007670591943897307 valid 0.00732795836423302\n",
      "Epoch 33:\n",
      "Loss train 0.008047524467110634 valid 0.0074481911008578575\n",
      "Epoch 34:\n",
      "Loss train 0.00804187054745853 valid 0.007217120860577613\n",
      "Epoch 35:\n",
      "Loss train 0.00785272530745715 valid 0.007679874847082039\n",
      "Epoch 36:\n",
      "Loss train 0.008174167186953128 valid 0.007948177764395577\n",
      "Epoch 37:\n",
      "Loss train 0.007911363639868796 valid 0.008071299126674085\n",
      "Epoch 38:\n",
      "Loss train 0.008348680422641338 valid 0.008965034552663375\n",
      "Epoch 39:\n",
      "Loss train 0.008052849764935673 valid 0.00894539293232848\n",
      "Epoch 40:\n",
      "Loss train 0.007646979447454214 valid 0.009761617825770114\n",
      "Epoch 41:\n",
      "Loss train 0.00813699743244797 valid 0.0081375152624025\n",
      "Epoch 42:\n",
      "Loss train 0.007577188708819449 valid 0.00815500537937628\n",
      "Epoch 43:\n",
      "Loss train 0.008257269626483321 valid 0.014109445390058142\n",
      "Epoch 44:\n",
      "Loss train 0.008207952752709389 valid 0.007129201131424282\n",
      "Epoch 45:\n",
      "Loss train 0.007638294515199959 valid 0.008186534802558874\n",
      "Epoch 46:\n",
      "Loss train 0.008266777363605798 valid 0.007887599424662276\n",
      "Epoch 47:\n",
      "Loss train 0.007699792897328734 valid 0.007643838174098476\n",
      "Epoch 48:\n",
      "Loss train 0.008113220143131912 valid 0.0072190095575193096\n",
      "Epoch 49:\n",
      "Loss train 0.00828986514825374 valid 0.007150731101276908\n",
      "Epoch 50:\n",
      "Loss train 0.007960177352651953 valid 0.007408035840832641\n",
      "Epoch 51:\n",
      "Loss train 0.00803836842533201 valid 0.0075084123293443945\n",
      "Epoch 52:\n",
      "Loss train 0.007820240394212306 valid 0.00713537967626206\n",
      "Epoch 53:\n",
      "Loss train 0.008614881723187863 valid 0.00797905832248936\n",
      "Epoch 54:\n",
      "Loss train 0.007433385970070958 valid 0.007658156769444237\n",
      "Epoch 55:\n",
      "Loss train 0.008022755421698093 valid 0.009426857366118013\n",
      "Epoch 56:\n",
      "Loss train 0.007952093728818 valid 0.008181050671309961\n",
      "Epoch 57:\n",
      "Loss train 0.007877388927154243 valid 0.010238886993918641\n",
      "Epoch 58:\n",
      "Loss train 0.009650814891792835 valid 0.007090213217915157\n",
      "Epoch 59:\n",
      "Loss train 0.007230262826196849 valid 0.007142084235353605\n",
      "Epoch 60:\n",
      "Loss train 0.007323197573423385 valid 0.0076569474123475865\n",
      "Epoch 61:\n",
      "Loss train 0.008360154600813985 valid 0.007702780667288595\n",
      "Epoch 62:\n",
      "Loss train 0.007513841963373125 valid 0.007713132317678511\n",
      "Epoch 63:\n",
      "Loss train 0.008720470764674247 valid 0.00715361151458708\n",
      "Epoch 64:\n",
      "Loss train 0.007424847674556077 valid 0.00766289304592162\n",
      "Epoch 65:\n",
      "Loss train 0.007997329486534 valid 0.007547021844082842\n",
      "Epoch 66:\n",
      "Loss train 0.008541487134061753 valid 0.007719062920673217\n",
      "Epoch 67:\n",
      "Loss train 0.007669322127476334 valid 0.007545905329868426\n",
      "Epoch 68:\n",
      "Loss train 0.007801854833960533 valid 0.008550048545741063\n",
      "Epoch 69:\n",
      "Loss train 0.0081380950845778 valid 0.0071761804859395\n",
      "Epoch 70:\n",
      "Loss train 0.007678938219323754 valid 0.007384544859068918\n",
      "Epoch 71:\n",
      "Loss train 0.0083373456960544 valid 0.007218883516870148\n",
      "Epoch 72:\n",
      "Loss train 0.007738223522901535 valid 0.008215569469475154\n",
      "Epoch 73:\n",
      "Loss train 0.008355373335070908 valid 0.007165546520403143\n",
      "Epoch 74:\n",
      "Loss train 0.007821021312847733 valid 0.007085130508416478\n",
      "Epoch 75:\n",
      "Loss train 0.008780003762803973 valid 0.015563684563608382\n",
      "Epoch 76:\n",
      "Loss train 0.0082039484847337 valid 0.007397176763793935\n",
      "Epoch 77:\n",
      "Loss train 0.007343237167224288 valid 0.0071453290632448735\n",
      "Epoch 78:\n",
      "Loss train 0.007767592500895262 valid 0.0072412499568056225\n",
      "Epoch 79:\n",
      "Loss train 0.00777187192812562 valid 0.007277592374407353\n",
      "Epoch 80:\n",
      "Loss train 0.007973667713813484 valid 0.0071194323546615685\n",
      "Epoch 81:\n",
      "Loss train 0.007982392604462802 valid 0.008259860473425931\n",
      "Epoch 82:\n",
      "Loss train 0.00794267597142607 valid 0.007420442643397733\n",
      "Epoch 83:\n",
      "Loss train 0.00826544945128262 valid 0.007371280738658267\n",
      "Epoch 84:\n",
      "Loss train 0.0076831820933148266 valid 0.007634687784672225\n",
      "Epoch 85:\n",
      "Loss train 0.008465075138956309 valid 0.0073720742846497675\n",
      "Epoch 86:\n",
      "Loss train 0.007622439279220999 valid 0.009466248891819148\n",
      "Epoch 87:\n",
      "Loss train 0.007802411653101444 valid 0.008308360607827879\n",
      "Epoch 88:\n",
      "Loss train 0.008262874586507677 valid 0.007435783264702676\n",
      "Epoch 89:\n",
      "Loss train 0.007835715170949697 valid 0.007400480295085206\n",
      "Epoch 90:\n",
      "Loss train 0.007955632284283637 valid 0.007234352145868652\n",
      "Epoch 91:\n",
      "Loss train 0.008101632329635322 valid 0.007306833855033067\n",
      "Epoch 92:\n",
      "Loss train 0.008334538186900317 valid 0.008084604557495777\n",
      "Epoch 93:\n",
      "Loss train 0.007627536924555898 valid 0.008222014329141254\n",
      "Epoch 94:\n",
      "Loss train 0.008236410915851594 valid 0.00942518962880332\n",
      "Epoch 95:\n",
      "Loss train 0.0074736792920157315 valid 0.007112460922710936\n",
      "Epoch 96:\n",
      "Loss train 0.009048774698749184 valid 0.007392578545728977\n",
      "Epoch 97:\n",
      "Loss train 0.007337505030445755 valid 0.00713358491220465\n",
      "Epoch 98:\n",
      "Loss train 0.0077328692236915235 valid 0.008190609044046337\n",
      "Epoch 99:\n",
      "Loss train 0.008301315610297025 valid 0.007168255604238656\n",
      "Epoch 100:\n",
      "Loss train 0.007922422876581549 valid 0.008848644241712722\n",
      "Epoch 101:\n",
      "Loss train 0.008283954160287977 valid 0.0071030941110932265\n",
      "Epoch 102:\n",
      "Loss train 0.007449833168648183 valid 0.007186854876834516\n",
      "Epoch 103:\n",
      "Loss train 0.008427755585871636 valid 0.007295457965264415\n",
      "Epoch 104:\n",
      "Loss train 0.007605232740752399 valid 0.014505003970006671\n",
      "Epoch 105:\n",
      "Loss train 0.008177191144786775 valid 0.007180271803454101\n",
      "Epoch 106:\n",
      "Loss train 0.008017380889505148 valid 0.00788894615249948\n",
      "Epoch 107:\n",
      "Loss train 0.007615516097284853 valid 0.007919443681664745\n",
      "Epoch 108:\n",
      "Loss train 0.008029525531455875 valid 0.007211285236508843\n",
      "Epoch 109:\n",
      "Loss train 0.008259412096813322 valid 0.007399737853993903\n",
      "Epoch 110:\n",
      "Loss train 0.00834546414669603 valid 0.007508806939085187\n",
      "Epoch 111:\n",
      "Loss train 0.007749678865075112 valid 0.01133117820986292\n",
      "Epoch 112:\n",
      "Loss train 0.007843141607008875 valid 0.007282911902751705\n",
      "Epoch 113:\n",
      "Loss train 0.00812426263000816 valid 0.007190404142621891\n",
      "Epoch 114:\n",
      "Loss train 0.008107905923388899 valid 0.007327852913004769\n",
      "Epoch 115:\n",
      "Loss train 0.0075922326650470495 valid 0.0076067586685774555\n",
      "Epoch 116:\n",
      "Loss train 0.0080259880842641 valid 0.009516585901932918\n",
      "Epoch 117:\n",
      "Loss train 0.007854646174237132 valid 0.008529000435296082\n",
      "Epoch 118:\n",
      "Loss train 0.008536007371731102 valid 0.00807863951239425\n",
      "Epoch 119:\n",
      "Loss train 0.0077748515270650385 valid 0.007719617791246966\n",
      "Epoch 120:\n",
      "Loss train 0.007840125989168882 valid 0.007852045423620462\n",
      "Epoch 121:\n",
      "Loss train 0.00799825543537736 valid 0.007362556985214367\n",
      "Epoch 122:\n",
      "Loss train 0.00815160403959453 valid 0.007794309342837018\n",
      "Epoch 123:\n",
      "Loss train 0.00806054193060845 valid 0.008043700663489655\n",
      "Epoch 124:\n",
      "Loss train 0.0074777750950306655 valid 0.00797335300713998\n",
      "Epoch 125:\n",
      "Loss train 0.007943138149566948 valid 0.00912177185590587\n",
      "Epoch 126:\n",
      "Loss train 0.008164076074026526 valid 0.007086184505428514\n",
      "Epoch 127:\n",
      "Loss train 0.008060277118347585 valid 0.008100224140114938\n",
      "Epoch 128:\n",
      "Loss train 0.007755535938777029 valid 0.01132679545544258\n",
      "Epoch 129:\n",
      "Loss train 0.007993435999378563 valid 0.007278081752255328\n",
      "Epoch 130:\n",
      "Loss train 0.008169303149916231 valid 0.007159759768575244\n",
      "Epoch 131:\n",
      "Loss train 0.007713142875581979 valid 0.0074255693336603016\n",
      "Epoch 132:\n",
      "Loss train 0.008355687651783229 valid 0.007433216144248552\n",
      "Epoch 133:\n",
      "Loss train 0.007715497040189803 valid 0.011921927123014549\n",
      "Epoch 134:\n",
      "Loss train 0.007911493149586023 valid 0.0070953133517573375\n",
      "Epoch 135:\n",
      "Loss train 0.008437783760018646 valid 0.0076148460510018015\n",
      "Epoch 136:\n",
      "Loss train 0.008364079599268735 valid 0.007167446245085829\n",
      "Epoch 137:\n",
      "Loss train 0.007326554306782782 valid 0.007228543764291663\n",
      "Epoch 138:\n",
      "Loss train 0.008317934577353298 valid 0.007882635338247354\n",
      "Epoch 139:\n",
      "Loss train 0.00786519691348076 valid 0.00747267282435702\n",
      "Epoch 140:\n",
      "Loss train 0.0081896512536332 valid 0.007558366763948586\n",
      "Epoch 141:\n",
      "Loss train 0.007281174217350781 valid 0.008144500234048157\n",
      "Epoch 142:\n",
      "Loss train 0.008372237230651081 valid 0.008599964835363055\n",
      "Epoch 143:\n",
      "Loss train 0.007649984769523143 valid 0.0073192534627313285\n",
      "Epoch 144:\n",
      "Loss train 0.007974544209428131 valid 0.00707762355437947\n",
      "Epoch 145:\n",
      "Loss train 0.008092061174102128 valid 0.007097906237463664\n",
      "Epoch 146:\n",
      "Loss train 0.007831634492613376 valid 0.007833916380208531\n",
      "Epoch 147:\n",
      "Loss train 0.008104381449520588 valid 0.008118661900769908\n",
      "Epoch 148:\n",
      "Loss train 0.008330625044181942 valid 0.00714845608388383\n",
      "Epoch 149:\n",
      "Loss train 0.00762856250628829 valid 0.00852495895276916\n",
      "Epoch 150:\n",
      "Loss train 0.007860138490796089 valid 0.008150973569008006\n",
      "Epoch 151:\n",
      "Loss train 0.007995780636556447 valid 0.007272827169078172\n",
      "Epoch 152:\n",
      "Loss train 0.00805826422292739 valid 0.007794075886703365\n",
      "Epoch 153:\n",
      "Loss train 0.008067905423231422 valid 0.007232995065430813\n",
      "Epoch 154:\n",
      "Loss train 0.007576037184335292 valid 0.007192987652316193\n",
      "Epoch 155:\n",
      "Loss train 0.008557121944613754 valid 0.007175592476096344\n",
      "Epoch 156:\n",
      "Loss train 0.007501903558149934 valid 0.007830410930700655\n",
      "Epoch 157:\n",
      "Loss train 0.008110754857771098 valid 0.01322131678543426\n",
      "Epoch 158:\n",
      "Loss train 0.007902112901210784 valid 0.007369334657766725\n",
      "Epoch 159:\n",
      "Loss train 0.008130925525911152 valid 0.007688967887606523\n",
      "Epoch 160:\n",
      "Loss train 0.007886163271032275 valid 0.009052276181022221\n",
      "Epoch 161:\n",
      "Loss train 0.007885364484973251 valid 0.00721603678163271\n",
      "Epoch 162:\n",
      "Loss train 0.008130007344298064 valid 0.009000941585930051\n",
      "Epoch 163:\n",
      "Loss train 0.007811563047580421 valid 0.007456337477439423\n",
      "Epoch 164:\n",
      "Loss train 0.007922369432635605 valid 0.007176011443227253\n",
      "Epoch 165:\n",
      "Loss train 0.00802879880182445 valid 0.007489581489979113\n",
      "Epoch 166:\n",
      "Loss train 0.007931215153075755 valid 0.007308958347498577\n",
      "Epoch 167:\n",
      "Loss train 0.008009696458466352 valid 0.008214479427184819\n",
      "Epoch 168:\n",
      "Loss train 0.007875227755866944 valid 0.008319027018032485\n",
      "Epoch 169:\n",
      "Loss train 0.00878929213155061 valid 0.007903387012356652\n",
      "Epoch 170:\n",
      "Loss train 0.007427023723721504 valid 0.007292802620479489\n",
      "Epoch 171:\n",
      "Loss train 0.0077384069748222825 valid 0.010100474185239749\n",
      "Epoch 172:\n",
      "Loss train 0.008194183856248856 valid 0.0073032250253132714\n",
      "Epoch 173:\n",
      "Loss train 0.008911365247331559 valid 0.018382968397165547\n",
      "Epoch 174:\n",
      "Loss train 0.008075776556506754 valid 0.007058760103212109\n",
      "Epoch 175:\n",
      "Loss train 0.00713334959000349 valid 0.007245401952112904\n",
      "Epoch 176:\n",
      "Loss train 0.008022461659274996 valid 0.009140759322434177\n",
      "Epoch 177:\n",
      "Loss train 0.007538958010263741 valid 0.008417990178755276\n",
      "Epoch 178:\n",
      "Loss train 0.008221919387578965 valid 0.007267488560560575\n",
      "Epoch 179:\n",
      "Loss train 0.007879324918612839 valid 0.007168678239905795\n",
      "Epoch 180:\n",
      "Loss train 0.007810135814361274 valid 0.007166677273672349\n",
      "Epoch 181:\n",
      "Loss train 0.00863452454097569 valid 0.007114673164777507\n",
      "Epoch 182:\n",
      "Loss train 0.007414106437936425 valid 0.00852529781624668\n",
      "Epoch 183:\n",
      "Loss train 0.007837582184001803 valid 0.007255208075101302\n",
      "Epoch 184:\n",
      "Loss train 0.008072466836310924 valid 0.007452422540473583\n",
      "Epoch 185:\n",
      "Loss train 0.007963489247485996 valid 0.010862792085270863\n",
      "Epoch 186:\n",
      "Loss train 0.007894241707399488 valid 0.015572120038571665\n",
      "Epoch 187:\n",
      "Loss train 0.008107203771360219 valid 0.007229529464717723\n",
      "Epoch 188:\n",
      "Loss train 0.007880223477259278 valid 0.011491699021922238\n",
      "Epoch 189:\n",
      "Loss train 0.008021902907639742 valid 0.009648530256066453\n",
      "Epoch 190:\n",
      "Loss train 0.008002806100994348 valid 0.007459607252015932\n",
      "Epoch 191:\n",
      "Loss train 0.007779539553448558 valid 0.007525675358146942\n",
      "Epoch 192:\n",
      "Loss train 0.008340870598331093 valid 0.008421802126037208\n",
      "Epoch 193:\n",
      "Loss train 0.0076566619705408815 valid 0.007209282107413243\n",
      "Epoch 194:\n",
      "Loss train 0.00805833527352661 valid 0.007611480999253379\n",
      "Epoch 195:\n",
      "Loss train 0.007948332065716386 valid 0.00750887305377741\n",
      "Epoch 196:\n",
      "Loss train 0.008207470127381385 valid 0.011025281960372547\n",
      "Epoch 197:\n",
      "Loss train 0.007691460852511227 valid 0.007168132227479429\n",
      "Epoch 198:\n",
      "Loss train 0.007836075248196721 valid 0.007477190654294649\n",
      "Epoch 199:\n",
      "Loss train 0.008169191149063408 valid 0.007302588429376933\n",
      "Epoch 200:\n",
      "Loss train 0.008634497397579252 valid 0.013889860151567438\n",
      "Epoch 201:\n",
      "Loss train 0.007547066668048501 valid 0.007076461135488556\n",
      "Epoch 202:\n",
      "Loss train 0.007515782867558301 valid 0.007278520228651017\n",
      "Epoch 203:\n",
      "Loss train 0.0076878330949693915 valid 0.008492250395784481\n",
      "Epoch 204:\n",
      "Loss train 0.00797832832671702 valid 0.008447172363030738\n",
      "Epoch 205:\n",
      "Loss train 0.00799248045310378 valid 0.0086816032647049\n",
      "Epoch 206:\n",
      "Loss train 0.008100417223758996 valid 0.007150535905960944\n",
      "Epoch 207:\n",
      "Loss train 0.007953340718522668 valid 0.007142721328093062\n",
      "Epoch 208:\n",
      "Loss train 0.007901403522118926 valid 0.007258673022677289\n",
      "Epoch 209:\n",
      "Loss train 0.008196153994649649 valid 0.008685568165632058\n",
      "Epoch 210:\n",
      "Loss train 0.00772144156973809 valid 0.007160189702973395\n",
      "Epoch 211:\n",
      "Loss train 0.00854855150450021 valid 0.007390526997224663\n",
      "Epoch 212:\n",
      "Loss train 0.007798722544685006 valid 0.007689652599972504\n",
      "Epoch 213:\n",
      "Loss train 0.007390985111705959 valid 0.007122021701096482\n",
      "Epoch 214:\n",
      "Loss train 0.009006334408186377 valid 0.007122334382871335\n",
      "Epoch 215:\n",
      "Loss train 0.007319597289897502 valid 0.007327693694810604\n",
      "Epoch 216:\n",
      "Loss train 0.008014282155781984 valid 0.007235791162759071\n",
      "Epoch 217:\n",
      "Loss train 0.00784216471016407 valid 0.007372534190751176\n",
      "Epoch 218:\n",
      "Loss train 0.00808469185139984 valid 0.008252666666565576\n",
      "Epoch 219:\n",
      "Loss train 0.007799467612057924 valid 0.007060958037989113\n",
      "Epoch 220:\n",
      "Loss train 0.008219270943664014 valid 0.007363820844757254\n",
      "Epoch 221:\n",
      "Loss train 0.007424607947468758 valid 0.007291313521516941\n",
      "Epoch 222:\n",
      "Loss train 0.008608096120879054 valid 0.007425601210929849\n",
      "Epoch 223:\n",
      "Loss train 0.007394774975255132 valid 0.007209868814986485\n",
      "Epoch 224:\n",
      "Loss train 0.007756382161751389 valid 0.012232307653745677\n",
      "Epoch 225:\n",
      "Loss train 0.008845352856442333 valid 0.007684014287053948\n",
      "Epoch 226:\n",
      "Loss train 0.007483663121238351 valid 0.007244559859924878\n",
      "Epoch 227:\n",
      "Loss train 0.007704796050675213 valid 0.008090775852465214\n",
      "Epoch 228:\n",
      "Loss train 0.008214919376187027 valid 0.007537179028492112\n",
      "Epoch 229:\n",
      "Loss train 0.0076436402089893815 valid 0.007134858483140725\n",
      "Epoch 230:\n",
      "Loss train 0.008578842808492481 valid 0.007066190486686533\n",
      "Epoch 231:\n",
      "Loss train 0.007645325665362179 valid 0.009565329323250295\n",
      "Epoch 232:\n",
      "Loss train 0.007718400163576007 valid 0.009651058296308522\n",
      "Epoch 233:\n",
      "Loss train 0.008105989322066307 valid 0.00745293826590513\n",
      "Epoch 234:\n",
      "Loss train 0.008674570224247873 valid 0.007273911245546461\n",
      "Epoch 235:\n",
      "Loss train 0.007171472981572152 valid 0.007036138236138449\n",
      "Epoch 236:\n",
      "Loss train 0.008365157530643047 valid 0.007281526580873355\n",
      "Epoch 237:\n",
      "Loss train 0.007450447822920978 valid 0.007122167704074415\n",
      "Epoch 238:\n",
      "Loss train 0.008307076152414083 valid 0.007064086250767611\n",
      "Epoch 239:\n",
      "Loss train 0.0077596708294004205 valid 0.007151523671890296\n",
      "Epoch 240:\n",
      "Loss train 0.007709944271482527 valid 0.008145043022978608\n",
      "Epoch 241:\n",
      "Loss train 0.008106149407103658 valid 0.007191377242705533\n",
      "Epoch 242:\n",
      "Loss train 0.00806894849985838 valid 0.009063227044038473\n",
      "Epoch 243:\n",
      "Loss train 0.007653710539452732 valid 0.008042117037277462\n",
      "Epoch 244:\n",
      "Loss train 0.007891607745550573 valid 0.007250079763588962\n",
      "Epoch 245:\n",
      "Loss train 0.008653961322270334 valid 0.007157907708302404\n",
      "Epoch 246:\n",
      "Loss train 0.007391334725543857 valid 0.007348138270715518\n",
      "Epoch 247:\n",
      "Loss train 0.007862542532384395 valid 0.008084365811055918\n",
      "Epoch 248:\n",
      "Loss train 0.008570607788860797 valid 0.007475952593804194\n",
      "Epoch 249:\n",
      "Loss train 0.007827810128219426 valid 0.007582668666719066\n",
      "Epoch 250:\n",
      "Loss train 0.007684837891720236 valid 0.007657023554994128\n",
      "Epoch 251:\n",
      "Loss train 0.007675879867747426 valid 0.007789603332861381\n",
      "Epoch 252:\n",
      "Loss train 0.008113513574935496 valid 0.009141993971659804\n",
      "Epoch 253:\n",
      "Loss train 0.00808674774132669 valid 0.007037769143444602\n",
      "Epoch 254:\n",
      "Loss train 0.007947206259705126 valid 0.007540337929616262\n",
      "Epoch 255:\n",
      "Loss train 0.008273168387822806 valid 0.00769764466530933\n",
      "Epoch 256:\n",
      "Loss train 0.007321046213619411 valid 0.00710060137150205\n",
      "Epoch 257:\n",
      "Loss train 0.008056987770833074 valid 0.007137826239114267\n",
      "Epoch 258:\n",
      "Loss train 0.008132907836697996 valid 0.00876998642739738\n",
      "Epoch 259:\n",
      "Loss train 0.008124815681949258 valid 0.007258097922991655\n",
      "Epoch 260:\n",
      "Loss train 0.00736466262023896 valid 0.007260863144542134\n",
      "Epoch 261:\n",
      "Loss train 0.007994563188403844 valid 0.007130842077524504\n",
      "Epoch 262:\n",
      "Loss train 0.008772596050985158 valid 0.007462072264776337\n",
      "Epoch 263:\n",
      "Loss train 0.0072094778949394825 valid 0.009833668505947388\n",
      "Epoch 264:\n",
      "Loss train 0.007932853219099343 valid 0.007258379530007436\n",
      "Epoch 265:\n",
      "Loss train 0.008061904548667372 valid 0.01004854620150337\n",
      "Epoch 266:\n",
      "Loss train 0.00805802724789828 valid 0.009478862338965808\n",
      "Epoch 267:\n",
      "Loss train 0.00758475829847157 valid 0.007358288122669678\n",
      "Epoch 268:\n",
      "Loss train 0.00835931695997715 valid 0.007703058849137546\n",
      "Epoch 269:\n",
      "Loss train 0.0077283232100307945 valid 0.007148554235862033\n",
      "Epoch 270:\n",
      "Loss train 0.007839181637391448 valid 0.008939159736902685\n",
      "Epoch 271:\n",
      "Loss train 0.008220009142532944 valid 0.00730328112522348\n",
      "Epoch 272:\n",
      "Loss train 0.007573563577607274 valid 0.007867807192335926\n",
      "Epoch 273:\n",
      "Loss train 0.008034659670665861 valid 0.007473077656661116\n",
      "Epoch 274:\n",
      "Loss train 0.008101360495202244 valid 0.008495089356111703\n",
      "Epoch 275:\n",
      "Loss train 0.007745023425668478 valid 0.0071945480282289125\n",
      "Epoch 276:\n",
      "Loss train 0.008043766552582383 valid 0.007831922100292663\n",
      "Epoch 277:\n",
      "Loss train 0.007936398042365908 valid 0.007862663628465077\n",
      "Epoch 278:\n",
      "Loss train 0.008007956184446812 valid 0.009547783114239265\n",
      "Epoch 279:\n",
      "Loss train 0.007841753009706735 valid 0.007099250029946982\n",
      "Epoch 280:\n",
      "Loss train 0.007632698779925704 valid 0.00712570824027438\n",
      "Epoch 281:\n",
      "Loss train 0.008797742426395416 valid 0.007564206699257859\n",
      "Epoch 282:\n",
      "Loss train 0.007271528262645006 valid 0.007038575881041453\n",
      "Epoch 283:\n",
      "Loss train 0.007969857566058636 valid 0.007549733923431335\n",
      "Epoch 284:\n",
      "Loss train 0.007730252793990076 valid 0.0097215770119944\n",
      "Epoch 285:\n",
      "Loss train 0.008541514328680933 valid 0.0071200290794462785\n",
      "Epoch 286:\n",
      "Loss train 0.0073061749525368215 valid 0.007015407915383418\n",
      "Epoch 287:\n",
      "Loss train 0.00856462940108031 valid 0.007185353243225756\n",
      "Epoch 288:\n",
      "Loss train 0.007423064964823425 valid 0.014946485409362676\n",
      "Epoch 289:\n",
      "Loss train 0.008018142930231988 valid 0.007452419665339875\n",
      "Epoch 290:\n",
      "Loss train 0.00830899849999696 valid 0.007058566709113464\n",
      "Epoch 291:\n",
      "Loss train 0.007762284232303501 valid 0.007150763517747692\n",
      "Epoch 292:\n",
      "Loss train 0.008761863624677062 valid 0.007112806140339358\n",
      "Epoch 293:\n",
      "Loss train 0.007445021541789174 valid 0.007717957664353071\n",
      "Epoch 294:\n",
      "Loss train 0.007345375400036574 valid 0.00706684355657641\n",
      "Epoch 295:\n",
      "Loss train 0.008120487327687442 valid 0.007057250356552748\n",
      "Epoch 296:\n",
      "Loss train 0.007905010241083801 valid 0.007268408586955047\n",
      "Epoch 297:\n",
      "Loss train 0.00824468200094998 valid 0.007040986436238932\n",
      "Epoch 298:\n",
      "Loss train 0.0077902738563716415 valid 0.007043067384022512\n",
      "Epoch 299:\n",
      "Loss train 0.007809932557865978 valid 0.007396554109469719\n",
      "Epoch 300:\n",
      "Loss train 0.007999359895475209 valid 0.007269510891312861\n",
      "Epoch 301:\n",
      "Loss train 0.007922249888069928 valid 0.007138268873061055\n",
      "Epoch 302:\n",
      "Loss train 0.007786384816281498 valid 0.007189694954990622\n",
      "Epoch 303:\n",
      "Loss train 0.008411188102327287 valid 0.007322976878066132\n",
      "Epoch 304:\n",
      "Loss train 0.007297161188907921 valid 0.007201076489012262\n",
      "Epoch 305:\n",
      "Loss train 0.008294488852843642 valid 0.007822426498806356\n",
      "Epoch 306:\n",
      "Loss train 0.007817383413203061 valid 0.007943176492343254\n",
      "Epoch 307:\n",
      "Loss train 0.008133892370387912 valid 0.007699297923807033\n",
      "Epoch 308:\n",
      "Loss train 0.0075610114727169275 valid 0.007698080896751615\n",
      "Epoch 309:\n",
      "Loss train 0.008117333184927703 valid 0.0073612648099365165\n",
      "Epoch 310:\n",
      "Loss train 0.007908776137046516 valid 0.008669555109484271\n",
      "Epoch 311:\n",
      "Loss train 0.0077800272079184656 valid 0.009485797243488579\n",
      "Epoch 312:\n",
      "Loss train 0.008027945533394814 valid 0.01054191813705481\n",
      "Epoch 313:\n",
      "Loss train 0.007608851315453648 valid 0.007147811005756357\n",
      "Epoch 314:\n",
      "Loss train 0.008216149425134062 valid 0.0070731553385111725\n",
      "Epoch 315:\n",
      "Loss train 0.008239181418903173 valid 0.007043149095533853\n",
      "Epoch 316:\n",
      "Loss train 0.007608741261065006 valid 0.007073718919831606\n",
      "Epoch 317:\n",
      "Loss train 0.007958069797605277 valid 0.008044284544200857\n",
      "Epoch 318:\n",
      "Loss train 0.008097988804802298 valid 0.009452154704149757\n",
      "Epoch 319:\n",
      "Loss train 0.007545665609650314 valid 0.007223724711913194\n",
      "Epoch 320:\n",
      "Loss train 0.00791102767456323 valid 0.009093053308927181\n",
      "Epoch 321:\n",
      "Loss train 0.008277841717936099 valid 0.009135465085370812\n",
      "Epoch 322:\n",
      "Loss train 0.008049286110326648 valid 0.007093391895572668\n",
      "Epoch 323:\n",
      "Loss train 0.0074589743232354525 valid 0.007544981158785913\n",
      "Epoch 324:\n",
      "Loss train 0.007952726637013257 valid 0.008522910723754504\n",
      "Epoch 325:\n",
      "Loss train 0.007616677251644432 valid 0.007119137335848865\n",
      "Epoch 326:\n",
      "Loss train 0.008153966958634556 valid 0.007075258525061794\n",
      "Epoch 327:\n",
      "Loss train 0.008376500862650573 valid 0.007990493002919876\n",
      "Epoch 328:\n",
      "Loss train 0.007624501399695873 valid 0.01238759360293675\n",
      "Epoch 329:\n",
      "Loss train 0.007690913709811866 valid 0.007228596627967451\n",
      "Epoch 330:\n",
      "Loss train 0.008080771719105541 valid 0.007050627619494323\n",
      "Epoch 331:\n",
      "Loss train 0.00792470515705645 valid 0.007932163131833016\n",
      "Epoch 332:\n",
      "Loss train 0.007742179878987372 valid 0.009144390705997632\n",
      "Epoch 333:\n",
      "Loss train 0.008163421652279795 valid 0.0070646929096142425\n",
      "Epoch 334:\n",
      "Loss train 0.007795163877308369 valid 0.008758257821145537\n",
      "Epoch 335:\n",
      "Loss train 0.007938650203868747 valid 0.00770856541894718\n",
      "Epoch 336:\n",
      "Loss train 0.008657663399353623 valid 0.007461340619601425\n",
      "Epoch 337:\n",
      "Loss train 0.007318349564447999 valid 0.007154288884705647\n",
      "Epoch 338:\n",
      "Loss train 0.007688289973884821 valid 0.008548507431522856\n",
      "Epoch 339:\n",
      "Loss train 0.008002935829572379 valid 0.007483208190950673\n",
      "Epoch 340:\n",
      "Loss train 0.008503660950809717 valid 0.007613478475905812\n",
      "Epoch 341:\n",
      "Loss train 0.007259752177633345 valid 0.007241553897670619\n",
      "Epoch 342:\n",
      "Loss train 0.0080628168489784 valid 0.009949578421608722\n",
      "Epoch 343:\n",
      "Loss train 0.007604079372249543 valid 0.012219058311308668\n",
      "Epoch 344:\n",
      "Loss train 0.008010882595553994 valid 0.00808110003383921\n",
      "Epoch 345:\n",
      "Loss train 0.008071235283277928 valid 0.008450596058420908\n",
      "Epoch 346:\n",
      "Loss train 0.007949638087302447 valid 0.011345660854988828\n",
      "Epoch 347:\n",
      "Loss train 0.007840243810787796 valid 0.007950228553821076\n",
      "Epoch 348:\n",
      "Loss train 0.008504697130993008 valid 0.008331428554640046\n",
      "Epoch 349:\n",
      "Loss train 0.007576388698071241 valid 0.007427775914166054\n",
      "Epoch 350:\n",
      "Loss train 0.007431857222691178 valid 0.007352222405436271\n",
      "Epoch 351:\n",
      "Loss train 0.007979860259220005 valid 0.007174597304033821\n",
      "Epoch 352:\n",
      "Loss train 0.008022376028820872 valid 0.010131502387360762\n",
      "Epoch 353:\n",
      "Loss train 0.008054546816274523 valid 0.007020346757532656\n",
      "Epoch 354:\n",
      "Loss train 0.0077537897415459155 valid 0.007195966073651242\n",
      "Epoch 355:\n",
      "Loss train 0.00778013608418405 valid 0.007691197052368185\n",
      "Epoch 356:\n",
      "Loss train 0.008272585072554649 valid 0.00705986798064773\n",
      "Epoch 357:\n",
      "Loss train 0.008020255449227989 valid 0.009220503182594505\n",
      "Epoch 358:\n",
      "Loss train 0.007376054795458913 valid 0.008221267774202604\n",
      "Epoch 359:\n",
      "Loss train 0.008262525452300907 valid 0.008432345597501036\n",
      "Epoch 360:\n",
      "Loss train 0.007957152440212667 valid 0.008966805114940446\n",
      "Epoch 361:\n",
      "Loss train 0.007789636990055442 valid 0.0072337695599076\n",
      "Epoch 362:\n",
      "Loss train 0.007928874278441071 valid 0.007066307188702941\n",
      "Epoch 363:\n",
      "Loss train 0.007793237920850515 valid 0.0077557024997891725\n",
      "Epoch 364:\n",
      "Loss train 0.007959368419833482 valid 0.007138677087603127\n",
      "Epoch 365:\n",
      "Loss train 0.007855588234961033 valid 0.007150735998918391\n",
      "Epoch 366:\n",
      "Loss train 0.008365529114380479 valid 0.007193309359581079\n",
      "Epoch 367:\n",
      "Loss train 0.007465105927549303 valid 0.007144942939136902\n",
      "Epoch 368:\n",
      "Loss train 0.008212021524086595 valid 0.011467672854172528\n",
      "Epoch 369:\n",
      "Loss train 0.007595692100003362 valid 0.00731974710535872\n",
      "Epoch 370:\n",
      "Loss train 0.008095838106237352 valid 0.007155730767489768\n",
      "Epoch 371:\n",
      "Loss train 0.007795367669314146 valid 0.007008185334174705\n",
      "Epoch 372:\n",
      "Loss train 0.008115354659967124 valid 0.007862231011094711\n",
      "Epoch 373:\n",
      "Loss train 0.007551911198534072 valid 0.008309015595928332\n",
      "Epoch 374:\n",
      "Loss train 0.007890415778383612 valid 0.007183654781091443\n",
      "Epoch 375:\n",
      "Loss train 0.00789951981510967 valid 0.0088287954331046\n",
      "Epoch 376:\n",
      "Loss train 0.008049721270799637 valid 0.008188347561841412\n",
      "Epoch 377:\n",
      "Loss train 0.007759753889404237 valid 0.007027375777411866\n",
      "Epoch 378:\n",
      "Loss train 0.007870322689414025 valid 0.007226404940071593\n",
      "Epoch 379:\n",
      "Loss train 0.007889842242002487 valid 0.009881087073322352\n",
      "Epoch 380:\n",
      "Loss train 0.007872208817861975 valid 0.007613250117554792\n",
      "Epoch 381:\n",
      "Loss train 0.008144889585673809 valid 0.00731064666793483\n",
      "Epoch 382:\n",
      "Loss train 0.007537140380591154 valid 0.007298618409048298\n",
      "Epoch 383:\n",
      "Loss train 0.007984523032791912 valid 0.007538192432848805\n",
      "Epoch 384:\n",
      "Loss train 0.007817263943143188 valid 0.007182333935531078\n",
      "Epoch 385:\n",
      "Loss train 0.008105498012155295 valid 0.007648799165065294\n",
      "Epoch 386:\n",
      "Loss train 0.008305294434539973 valid 0.007015232425864867\n",
      "Epoch 387:\n",
      "Loss train 0.007486076271161437 valid 0.007067717020021909\n",
      "Epoch 388:\n",
      "Loss train 0.008508118577301502 valid 0.007076533750943192\n",
      "Epoch 389:\n",
      "Loss train 0.00729993237182498 valid 0.00713279196662353\n",
      "Epoch 390:\n",
      "Loss train 0.007678179540671408 valid 0.009166418563192492\n",
      "Epoch 391:\n",
      "Loss train 0.007846720162779092 valid 0.007691778002012015\n",
      "Epoch 392:\n",
      "Loss train 0.008255043248645962 valid 0.007200248417559482\n",
      "Epoch 393:\n",
      "Loss train 0.007732046414166689 valid 0.007371275733499286\n",
      "Epoch 394:\n",
      "Loss train 0.008287277328781784 valid 0.011211474866020804\n",
      "Epoch 395:\n",
      "Loss train 0.007495513088069856 valid 0.0071876900561111835\n",
      "Epoch 396:\n",
      "Loss train 0.007788379588164389 valid 0.007409874477239373\n",
      "Epoch 397:\n",
      "Loss train 0.008049796489067375 valid 0.009805539022346408\n",
      "Epoch 398:\n",
      "Loss train 0.007768056909553706 valid 0.0074514347560877894\n",
      "Epoch 399:\n",
      "Loss train 0.008346687676385045 valid 0.0073605586864766625\n",
      "Epoch 400:\n",
      "Loss train 0.007358140591531992 valid 0.007753258146205419\n",
      "Epoch 401:\n",
      "Loss train 0.008129571159370243 valid 0.007808330582078324\n",
      "Epoch 402:\n",
      "Loss train 0.007944983951747418 valid 0.0072886005247165654\n",
      "Epoch 403:\n",
      "Loss train 0.007795849405229091 valid 0.007004774928663951\n",
      "Epoch 404:\n",
      "Loss train 0.008202639361843466 valid 0.00776566181089009\n",
      "Epoch 405:\n",
      "Loss train 0.007453104592859745 valid 0.007568872790267504\n",
      "Epoch 406:\n",
      "Loss train 0.00782827753573656 valid 0.00712594012815076\n",
      "Epoch 407:\n",
      "Loss train 0.008069349969737231 valid 0.008614622015056717\n",
      "Epoch 408:\n",
      "Loss train 0.008042086861096323 valid 0.007214191624612298\n",
      "Epoch 409:\n",
      "Loss train 0.007928248392418027 valid 0.009838544527865499\n",
      "Epoch 410:\n",
      "Loss train 0.007858608532696962 valid 0.007332748954074115\n",
      "Epoch 411:\n",
      "Loss train 0.008366678496822716 valid 0.007063649234102772\n",
      "Epoch 412:\n",
      "Loss train 0.007334065614268184 valid 0.008242492398013996\n",
      "Epoch 413:\n",
      "Loss train 0.007945758318528532 valid 0.008587613543285034\n",
      "Epoch 414:\n",
      "Loss train 0.008015641095116734 valid 0.007059228004878211\n",
      "Epoch 415:\n",
      "Loss train 0.0076354816602542995 valid 0.007041634520212616\n",
      "Epoch 416:\n",
      "Loss train 0.007944033415988087 valid 0.00719626300054357\n",
      "Epoch 417:\n",
      "Loss train 0.00821529574226588 valid 0.007048022889277277\n",
      "Epoch 418:\n",
      "Loss train 0.0073263972857967015 valid 0.00792996455790023\n",
      "Epoch 419:\n",
      "Loss train 0.008067133580334484 valid 0.0071810554677705215\n",
      "Epoch 420:\n",
      "Loss train 0.007970994617789984 valid 0.007293661741318258\n",
      "Epoch 421:\n",
      "Loss train 0.007987070363014936 valid 0.009552316387412763\n",
      "Epoch 422:\n",
      "Loss train 0.007664868361316622 valid 0.007700239623770988\n",
      "Epoch 423:\n",
      "Loss train 0.008183505930937827 valid 0.007259191068076781\n",
      "Epoch 424:\n",
      "Loss train 0.007555661001242697 valid 0.0070812441514516156\n",
      "Epoch 425:\n",
      "Loss train 0.008233337514102459 valid 0.007507604511718618\n",
      "Epoch 426:\n",
      "Loss train 0.0075459844013676045 valid 0.008934350224223904\n",
      "Epoch 427:\n",
      "Loss train 0.008024583216756581 valid 0.009599841453944867\n",
      "Epoch 428:\n",
      "Loss train 0.008384347553364932 valid 0.008406863126358743\n",
      "Epoch 429:\n",
      "Loss train 0.007399307209998369 valid 0.0071326912319898735\n",
      "Epoch 430:\n",
      "Loss train 0.007853855942375958 valid 0.007013590105861438\n",
      "Epoch 431:\n",
      "Loss train 0.007932091108523309 valid 0.007114274833295131\n",
      "Epoch 432:\n",
      "Loss train 0.0078069153940305116 valid 0.007702607481618523\n",
      "Epoch 433:\n",
      "Loss train 0.00758446101564914 valid 0.007121668354034531\n",
      "Epoch 434:\n",
      "Loss train 0.008211573497392237 valid 0.007873978179126833\n",
      "Epoch 435:\n",
      "Loss train 0.007760483473539352 valid 0.007142147427278695\n",
      "Epoch 436:\n",
      "Loss train 0.007897465438582003 valid 0.009877982555324032\n",
      "Epoch 437:\n",
      "Loss train 0.007979945368133486 valid 0.0070742037092478205\n",
      "Epoch 438:\n",
      "Loss train 0.008004706893116236 valid 0.007416872551639062\n",
      "Epoch 439:\n",
      "Loss train 0.007969823195599019 valid 0.007137609571143668\n",
      "Epoch 440:\n",
      "Loss train 0.007578813224099577 valid 0.007518023523219406\n",
      "Epoch 441:\n",
      "Loss train 0.007712814146652818 valid 0.007064845937670219\n",
      "Epoch 442:\n",
      "Loss train 0.008347480092197657 valid 0.00724072290421242\n",
      "Epoch 443:\n",
      "Loss train 0.008009510515257716 valid 0.0093375926768545\n",
      "Epoch 444:\n",
      "Loss train 0.007384127108380199 valid 0.007414698020597874\n",
      "Epoch 445:\n",
      "Loss train 0.008149815481156111 valid 0.007124327623851156\n",
      "Epoch 446:\n",
      "Loss train 0.007713026567362249 valid 0.007008199999965417\n",
      "Epoch 447:\n",
      "Loss train 0.008165920693427324 valid 0.0073247580791534405\n",
      "Epoch 448:\n",
      "Loss train 0.007833146215416491 valid 0.007342861698897381\n",
      "Epoch 449:\n",
      "Loss train 0.00740823115222156 valid 0.007498245018448364\n",
      "Epoch 450:\n",
      "Loss train 0.008612530790269374 valid 0.007251341115486735\n",
      "Epoch 451:\n",
      "Loss train 0.007796133733354509 valid 0.007099996566099941\n",
      "Epoch 452:\n",
      "Loss train 0.007452965429984033 valid 0.008537294971293003\n",
      "Epoch 453:\n",
      "Loss train 0.00810863736551255 valid 0.007017362551523179\n",
      "Epoch 454:\n",
      "Loss train 0.008279905584640802 valid 0.00943573169154475\n",
      "Epoch 455:\n",
      "Loss train 0.0074489499256014825 valid 0.007064343322901666\n",
      "Epoch 456:\n",
      "Loss train 0.007767818318679929 valid 0.007402745013570959\n",
      "Epoch 457:\n",
      "Loss train 0.00806273647584021 valid 0.007464863499347607\n",
      "Epoch 458:\n",
      "Loss train 0.008476502909325064 valid 0.0090197716331858\n",
      "Epoch 459:\n",
      "Loss train 0.007241574693471193 valid 0.007290132447803938\n",
      "Epoch 460:\n",
      "Loss train 0.007748273485340178 valid 0.0077132129596262184\n",
      "Epoch 461:\n",
      "Loss train 0.007995246443897486 valid 0.007336508244819447\n",
      "Epoch 462:\n",
      "Loss train 0.008242324502207339 valid 0.007091661588815774\n",
      "Epoch 463:\n",
      "Loss train 0.007409077095799148 valid 0.007356151285070471\n",
      "Epoch 464:\n",
      "Loss train 0.007709094937890768 valid 0.007198446519732846\n",
      "Epoch 465:\n",
      "Loss train 0.008163385717198253 valid 0.007039141792745821\n",
      "Epoch 466:\n",
      "Loss train 0.007982615055516363 valid 0.009313566556131385\n",
      "Epoch 467:\n",
      "Loss train 0.007789251869544387 valid 0.007197868713660565\n",
      "Epoch 468:\n",
      "Loss train 0.008098293971270322 valid 0.007880795618622273\n",
      "Epoch 469:\n",
      "Loss train 0.007761922059580683 valid 0.009622641313637163\n",
      "Epoch 470:\n",
      "Loss train 0.007717553521506488 valid 0.007278709566480031\n",
      "Epoch 471:\n",
      "Loss train 0.00794236249756068 valid 0.008385110051891126\n",
      "Epoch 472:\n",
      "Loss train 0.00776764907874167 valid 0.007278554380543115\n",
      "Epoch 473:\n",
      "Loss train 0.007829948030412197 valid 0.007637252690164342\n",
      "Epoch 474:\n",
      "Loss train 0.007885349900461733 valid 0.0071795746452456835\n",
      "Epoch 475:\n",
      "Loss train 0.00791577369440347 valid 0.007779043367087117\n",
      "Epoch 476:\n",
      "Loss train 0.0080840671248734 valid 0.008277754700496834\n",
      "Epoch 477:\n",
      "Loss train 0.007547272043302656 valid 0.0076401166642938115\n",
      "Epoch 478:\n",
      "Loss train 0.007894755420275033 valid 0.007015120977746171\n",
      "Epoch 479:\n",
      "Loss train 0.00785421700682491 valid 0.007233860378074949\n",
      "Epoch 480:\n",
      "Loss train 0.007570697441697121 valid 0.007627547329590365\n",
      "Epoch 481:\n",
      "Loss train 0.008234291514381766 valid 0.007121071622678598\n",
      "Epoch 482:\n",
      "Loss train 0.007763704471290112 valid 0.007117914622575678\n",
      "Epoch 483:\n",
      "Loss train 0.007867670673877001 valid 0.007312367894214287\n",
      "Epoch 484:\n",
      "Loss train 0.007809986439533532 valid 0.0070901823030272825\n",
      "Epoch 485:\n",
      "Loss train 0.008000930999405681 valid 0.0069789214463609715\n",
      "Epoch 486:\n",
      "Loss train 0.008244207897223532 valid 0.007043333919864366\n",
      "Epoch 487:\n",
      "Loss train 0.007708241338841617 valid 0.008042729398164581\n",
      "Epoch 488:\n",
      "Loss train 0.007481881589628756 valid 0.00751223357930536\n",
      "Epoch 489:\n",
      "Loss train 0.008541297651827335 valid 0.007292741995605504\n",
      "Epoch 490:\n",
      "Loss train 0.007540886108763516 valid 0.0075526984126932555\n",
      "Epoch 491:\n",
      "Loss train 0.007701812032610178 valid 0.007177742957323232\n",
      "Epoch 492:\n",
      "Loss train 0.007671840018592775 valid 0.007309618412301593\n",
      "Epoch 493:\n",
      "Loss train 0.008057734174653887 valid 0.007699478611863606\n",
      "Epoch 494:\n",
      "Loss train 0.007833262626081704 valid 0.007020052273892563\n",
      "Epoch 495:\n",
      "Loss train 0.007903824485838413 valid 0.007402008452375233\n",
      "Epoch 496:\n",
      "Loss train 0.008751219054684043 valid 0.007080505070752073\n",
      "Epoch 497:\n",
      "Loss train 0.007311223563738168 valid 0.010563065459917052\n",
      "Epoch 498:\n",
      "Loss train 0.007545357490889728 valid 0.007928571689707646\n",
      "Epoch 499:\n",
      "Loss train 0.007956950808875263 valid 0.008125322252979315\n",
      "Epoch 500:\n",
      "Loss train 0.007482919846661389 valid 0.00712790160519212\n",
      "Epoch 501:\n",
      "Loss train 0.008617417686618865 valid 0.007178154774281267\n",
      "Epoch 502:\n",
      "Loss train 0.007317923805676401 valid 0.009248835093838661\n",
      "Epoch 503:\n",
      "Loss train 0.00783601771108806 valid 0.007023315952946433\n",
      "Epoch 504:\n",
      "Loss train 0.00799456296954304 valid 0.0071959227592711325\n",
      "Epoch 505:\n",
      "Loss train 0.00797195702791214 valid 0.0070227233577546755\n",
      "Epoch 506:\n",
      "Loss train 0.007834212905727327 valid 0.007889681793866727\n",
      "Epoch 507:\n",
      "Loss train 0.007801080029457808 valid 0.0075280870258601865\n",
      "Epoch 508:\n",
      "Loss train 0.008022451442666351 valid 0.0072047056787966956\n",
      "Epoch 509:\n",
      "Loss train 0.007616714709438383 valid 0.007042475195835434\n",
      "Epoch 510:\n",
      "Loss train 0.008052053158171474 valid 0.007339364417256647\n",
      "Epoch 511:\n",
      "Loss train 0.008219142132438718 valid 0.007043817807299935\n",
      "Epoch 512:\n",
      "Loss train 0.0073246328579261895 valid 0.008049310751996105\n",
      "Epoch 513:\n",
      "Loss train 0.00795933281071484 valid 0.0073463745555901\n",
      "Epoch 514:\n",
      "Loss train 0.007798084509558976 valid 0.007217388450635604\n",
      "Epoch 515:\n",
      "Loss train 0.00803737973794341 valid 0.007079086195565861\n",
      "Epoch 516:\n",
      "Loss train 0.007660979586653412 valid 0.010760415317339626\n",
      "Epoch 517:\n",
      "Loss train 0.00787954915780574 valid 0.007056288686312405\n",
      "Epoch 518:\n",
      "Loss train 0.00788583661429584 valid 0.007047198886746013\n",
      "Epoch 519:\n",
      "Loss train 0.008057461632415653 valid 0.007161750474150107\n",
      "Epoch 520:\n",
      "Loss train 0.007608422506600618 valid 0.008213159952986035\n",
      "Epoch 521:\n",
      "Loss train 0.008172141374088823 valid 0.007434152692142548\n",
      "Epoch 522:\n",
      "Loss train 0.007609851160086691 valid 0.007265312238111371\n",
      "Epoch 523:\n",
      "Loss train 0.007973990496248006 valid 0.007438443481356017\n",
      "Epoch 524:\n",
      "Loss train 0.0076878095418214795 valid 0.00792814229173229\n",
      "Epoch 525:\n",
      "Loss train 0.007771039451472461 valid 0.007628876203042906\n",
      "Epoch 526:\n",
      "Loss train 0.009194986335933209 valid 0.00796384564696575\n",
      "Epoch 527:\n",
      "Loss train 0.0072196416044607754 valid 0.007097474377256584\n",
      "Epoch 528:\n",
      "Loss train 0.007131880777888 valid 0.007068505762148008\n",
      "Epoch 529:\n",
      "Loss train 0.008098723576404154 valid 0.0074486994392973845\n",
      "Epoch 530:\n",
      "Loss train 0.00804843001998961 valid 0.007073005390102385\n",
      "Epoch 531:\n",
      "Loss train 0.007821861999109387 valid 0.007895218047960355\n",
      "Epoch 532:\n",
      "Loss train 0.00801333102863282 valid 0.010066243941006485\n",
      "Epoch 533:\n",
      "Loss train 0.007553651025518775 valid 0.007247922974073075\n",
      "Epoch 534:\n",
      "Loss train 0.00800711517687887 valid 0.0071759372708700584\n",
      "Epoch 535:\n",
      "Loss train 0.008611042792908847 valid 0.007952959606591753\n",
      "Epoch 536:\n",
      "Loss train 0.007122843055985868 valid 0.0070268187223843995\n",
      "Epoch 537:\n",
      "Loss train 0.007646126747131348 valid 0.007187716240983538\n",
      "Epoch 538:\n",
      "Loss train 0.008200767110101878 valid 0.007905608266797957\n",
      "Epoch 539:\n",
      "Loss train 0.007337056281976402 valid 0.00817556606379423\n",
      "Epoch 540:\n",
      "Loss train 0.008477501412853599 valid 0.0070964661395532935\n",
      "Epoch 541:\n",
      "Loss train 0.007699874541722238 valid 0.0073068692478519175\n",
      "Epoch 542:\n",
      "Loss train 0.007687501981854439 valid 0.006987457015871483\n",
      "Epoch 543:\n",
      "Loss train 0.007880755271762609 valid 0.007125398978592943\n",
      "Epoch 544:\n",
      "Loss train 0.007795378332957625 valid 0.0073723829433249705\n",
      "Epoch 545:\n",
      "Loss train 0.007864556987769902 valid 0.007405394435312926\n",
      "Epoch 546:\n",
      "Loss train 0.007646637209691107 valid 0.011101316821212653\n",
      "Epoch 547:\n",
      "Loss train 0.008285455596633256 valid 0.007892755559151024\n",
      "Epoch 548:\n",
      "Loss train 0.007567492332309484 valid 0.007205177518486197\n",
      "Epoch 549:\n",
      "Loss train 0.008042019871063531 valid 0.007669918907909833\n",
      "Epoch 550:\n",
      "Loss train 0.007700600214302539 valid 0.008322472133059741\n",
      "Epoch 551:\n",
      "Loss train 0.007482516448944807 valid 0.007167652972130999\n",
      "Epoch 552:\n",
      "Loss train 0.008764005932025611 valid 0.007089167000515679\n",
      "Epoch 553:\n",
      "Loss train 0.007208774820901454 valid 0.007033088853965447\n",
      "Epoch 554:\n",
      "Loss train 0.008409903324209154 valid 0.008216750778268288\n",
      "Epoch 555:\n",
      "Loss train 0.0072161365253850815 valid 0.007973245612320967\n",
      "Epoch 556:\n",
      "Loss train 0.008430721680633723 valid 0.007486173204067043\n",
      "Epoch 557:\n",
      "Loss train 0.007415050128474832 valid 0.007912564009565887\n",
      "Epoch 558:\n",
      "Loss train 0.007746805842034518 valid 0.0073278480429354404\n",
      "Epoch 559:\n",
      "Loss train 0.008238427084870636 valid 0.007033526611865092\n",
      "Epoch 560:\n",
      "Loss train 0.007879684758372606 valid 0.007227391180909058\n",
      "Epoch 561:\n",
      "Loss train 0.007486074967309833 valid 0.00790616893931573\n",
      "Epoch 562:\n",
      "Loss train 0.007993409507907927 valid 0.007357663549409737\n",
      "Epoch 563:\n",
      "Loss train 0.007791127231903374 valid 0.007961194633447861\n",
      "Epoch 564:\n",
      "Loss train 0.0077804943174123765 valid 0.007104733298318017\n",
      "Epoch 565:\n",
      "Loss train 0.0076817940827459096 valid 0.007522207721918618\n",
      "Epoch 566:\n",
      "Loss train 0.008093769503757358 valid 0.013378430963305352\n",
      "Epoch 567:\n",
      "Loss train 0.007933506323024631 valid 0.007859642196257479\n",
      "Epoch 568:\n",
      "Loss train 0.0076863668393343685 valid 0.007941589351275704\n",
      "Epoch 569:\n",
      "Loss train 0.008637935584411026 valid 0.00875345678617874\n",
      "Epoch 570:\n",
      "Loss train 0.0072123717283830045 valid 0.006989329281793205\n",
      "Epoch 571:\n",
      "Loss train 0.0072498181229457255 valid 0.007259706750259671\n",
      "Epoch 572:\n",
      "Loss train 0.008426820356398822 valid 0.007297761642294461\n",
      "Epoch 573:\n",
      "Loss train 0.0075472556753084065 valid 0.007084725666660023\n",
      "Epoch 574:\n",
      "Loss train 0.008173450101166964 valid 0.009423340535542777\n",
      "Epoch 575:\n",
      "Loss train 0.007608821075409651 valid 0.006981868559990888\n",
      "Epoch 576:\n",
      "Loss train 0.007994284443557263 valid 0.007075897732200825\n",
      "Epoch 577:\n",
      "Loss train 0.008143380293622613 valid 0.013387337174512412\n",
      "Epoch 578:\n",
      "Loss train 0.007650009524077177 valid 0.007066133596617585\n",
      "Epoch 579:\n",
      "Loss train 0.0073525931360200044 valid 0.0070409413475768885\n",
      "Epoch 580:\n",
      "Loss train 0.007795434612780809 valid 0.008054291595233107\n",
      "Epoch 581:\n",
      "Loss train 0.008175231320783495 valid 0.006982105801047673\n",
      "Epoch 582:\n",
      "Loss train 0.00804035171866417 valid 0.007498504873317066\n",
      "Epoch 583:\n",
      "Loss train 0.0080757915135473 valid 0.0071100284374638325\n",
      "Epoch 584:\n",
      "Loss train 0.007801676872186363 valid 0.007892126932826062\n",
      "Epoch 585:\n",
      "Loss train 0.007607831009663641 valid 0.00710105803032042\n",
      "Epoch 586:\n",
      "Loss train 0.007975568152032793 valid 0.007109715498085107\n",
      "Epoch 587:\n",
      "Loss train 0.007690589143894613 valid 0.008305648885460799\n",
      "Epoch 588:\n",
      "Loss train 0.008030059686861933 valid 0.012739727077589337\n",
      "Epoch 589:\n",
      "Loss train 0.007526995488442481 valid 0.0076211738325968085\n",
      "Epoch 590:\n",
      "Loss train 0.008385100164450704 valid 0.007209607906597037\n",
      "Epoch 591:\n",
      "Loss train 0.007268384885974228 valid 0.008675710977407863\n",
      "Epoch 592:\n",
      "Loss train 0.007587439049966633 valid 0.007290322616695109\n",
      "Epoch 593:\n",
      "Loss train 0.00870155320968479 valid 0.007074839130549776\n",
      "Epoch 594:\n",
      "Loss train 0.007268527620472014 valid 0.007174754106719428\n",
      "Epoch 595:\n",
      "Loss train 0.008773805750533938 valid 0.007341557319237583\n",
      "Epoch 596:\n",
      "Loss train 0.007106197914108634 valid 0.006976758519840144\n",
      "Epoch 597:\n",
      "Loss train 0.007806948465295136 valid 0.007219654442823257\n",
      "Epoch 598:\n",
      "Loss train 0.007637994010001421 valid 0.006975166862750948\n",
      "Epoch 599:\n",
      "Loss train 0.00794205028563738 valid 0.007264302040459175\n",
      "Epoch 600:\n",
      "Loss train 0.00791848797351122 valid 0.007357793574382291\n",
      "Epoch 601:\n",
      "Loss train 0.007791759297251701 valid 0.010870013556124603\n",
      "Epoch 602:\n",
      "Loss train 0.0077068313537165525 valid 0.0070130938240086935\n",
      "Epoch 603:\n",
      "Loss train 0.007941815410740674 valid 0.006974755527936641\n",
      "Epoch 604:\n",
      "Loss train 0.00820806525181979 valid 0.007111180527520611\n",
      "Epoch 605:\n",
      "Loss train 0.007866701520979404 valid 0.010615491778346569\n",
      "Epoch 606:\n",
      "Loss train 0.007482024412602186 valid 0.007264181359318285\n",
      "Epoch 607:\n",
      "Loss train 0.007827963647432625 valid 0.008006972317243932\n",
      "Epoch 608:\n",
      "Loss train 0.008058390938676894 valid 0.007128067199816565\n",
      "Epoch 609:\n",
      "Loss train 0.007522015264257788 valid 0.007196228121147126\n",
      "Epoch 610:\n",
      "Loss train 0.007743337820284068 valid 0.007245542054057661\n",
      "Epoch 611:\n",
      "Loss train 0.0080117939831689 valid 0.007704269471345509\n",
      "Epoch 612:\n",
      "Loss train 0.007830676222220063 valid 0.007068808360135492\n",
      "Epoch 613:\n",
      "Loss train 0.007852582526393235 valid 0.012629289824736297\n",
      "Epoch 614:\n",
      "Loss train 0.007770131248980761 valid 0.007240800292689133\n",
      "Epoch 615:\n",
      "Loss train 0.008149164249189197 valid 0.009746827072325185\n",
      "Epoch 616:\n",
      "Loss train 0.007429660581983626 valid 0.0074900216605678065\n",
      "Epoch 617:\n",
      "Loss train 0.007605108395218849 valid 0.007671583631069114\n",
      "Epoch 618:\n",
      "Loss train 0.008063829322345555 valid 0.007444787275956072\n",
      "Epoch 619:\n",
      "Loss train 0.008082434269599616 valid 0.008205943404722263\n",
      "Epoch 620:\n",
      "Loss train 0.007870654123835266 valid 0.007431191999934399\n",
      "Epoch 621:\n",
      "Loss train 0.007645552703179419 valid 0.007192977810184804\n",
      "Epoch 622:\n",
      "Loss train 0.007585427928715944 valid 0.007760033274848232\n",
      "Epoch 623:\n",
      "Loss train 0.008127149068750441 valid 0.008345094101000301\n",
      "Epoch 624:\n",
      "Loss train 0.007907263543456793 valid 0.007552284740717271\n",
      "Epoch 625:\n",
      "Loss train 0.007968299281783402 valid 0.008251218665505615\n",
      "Epoch 626:\n",
      "Loss train 0.0072209545178338885 valid 0.007056329590491788\n",
      "Epoch 627:\n",
      "Loss train 0.008988439226523042 valid 0.007116018463414762\n",
      "Epoch 628:\n",
      "Loss train 0.007056761425919831 valid 0.007045431329421055\n",
      "Epoch 629:\n",
      "Loss train 0.007496163519099355 valid 0.008599682080733936\n",
      "Epoch 630:\n",
      "Loss train 0.008129300507716835 valid 0.007031261740794621\n",
      "Epoch 631:\n",
      "Loss train 0.007716216915287078 valid 0.007095419500998131\n",
      "Epoch 632:\n",
      "Loss train 0.007860123170539737 valid 0.006925856324050584\n",
      "Epoch 633:\n",
      "Loss train 0.008307194719091058 valid 0.0078444394347073\n",
      "Epoch 634:\n",
      "Loss train 0.0072567903529852625 valid 0.009042427895307235\n",
      "Epoch 635:\n",
      "Loss train 0.007716606934554875 valid 0.007671403986973261\n",
      "Epoch 636:\n",
      "Loss train 0.00798323045950383 valid 0.008927679261334575\n",
      "Epoch 637:\n",
      "Loss train 0.007797222114168108 valid 0.0069347364087496135\n",
      "Epoch 638:\n",
      "Loss train 0.007647347403690219 valid 0.007248745628939964\n",
      "Epoch 639:\n",
      "Loss train 0.007848597052507102 valid 0.007103055438328974\n",
      "Epoch 640:\n",
      "Loss train 0.008068670965731144 valid 0.007634830613892118\n",
      "Epoch 641:\n",
      "Loss train 0.0078948770230636 valid 0.007021590430172696\n",
      "Epoch 642:\n",
      "Loss train 0.007797023323364556 valid 0.007076073861626338\n",
      "Epoch 643:\n",
      "Loss train 0.007597634205594659 valid 0.008436999138617379\n",
      "Epoch 644:\n",
      "Loss train 0.007968181618489325 valid 0.007144964523717717\n",
      "Epoch 645:\n",
      "Loss train 0.008045580037869513 valid 0.009079984128673889\n",
      "Epoch 646:\n",
      "Loss train 0.007687191967852414 valid 0.006995037741154841\n",
      "Epoch 647:\n",
      "Loss train 0.00763429744169116 valid 0.007890114406084045\n",
      "Epoch 648:\n",
      "Loss train 0.008139729425311088 valid 0.00911104968972754\n",
      "Epoch 649:\n",
      "Loss train 0.007623555022291839 valid 0.007092329382791862\n",
      "Epoch 650:\n",
      "Loss train 0.007957356055267156 valid 0.008703154494326446\n",
      "Epoch 651:\n",
      "Loss train 0.007770667583681643 valid 0.007207764239253037\n",
      "Epoch 652:\n",
      "Loss train 0.007698105396702886 valid 0.008743459960500934\n",
      "Epoch 653:\n",
      "Loss train 0.007806338705122471 valid 0.007646428938237808\n",
      "Epoch 654:\n",
      "Loss train 0.00781193015165627 valid 0.008827743762210485\n",
      "Epoch 655:\n",
      "Loss train 0.008103588712401688 valid 0.009398626779300681\n",
      "Epoch 656:\n",
      "Loss train 0.007269139611162245 valid 0.007155366181525814\n",
      "Epoch 657:\n",
      "Loss train 0.008333188728429378 valid 0.007165485524244948\n",
      "Epoch 658:\n",
      "Loss train 0.007370191779918969 valid 0.0078722415351724\n",
      "Epoch 659:\n",
      "Loss train 0.008000639979727567 valid 0.007042203173404732\n",
      "Epoch 660:\n",
      "Loss train 0.007732674987055361 valid 0.008548771374827993\n",
      "Epoch 661:\n",
      "Loss train 0.008161938940174878 valid 0.006939515921564106\n",
      "Epoch 662:\n",
      "Loss train 0.007698745164088905 valid 0.007096206211745286\n",
      "Epoch 663:\n",
      "Loss train 0.007587142093107104 valid 0.015119662980987172\n",
      "Epoch 664:\n",
      "Loss train 0.008125528628006577 valid 0.007039980143414317\n",
      "Epoch 665:\n",
      "Loss train 0.007766503789462149 valid 0.007243327027334429\n",
      "Epoch 666:\n",
      "Loss train 0.007779733496718109 valid 0.007142959464304507\n",
      "Epoch 667:\n",
      "Loss train 0.007620232463814318 valid 0.007693205175210046\n",
      "Epoch 668:\n",
      "Loss train 0.008640192835591733 valid 0.007441493159814613\n",
      "Epoch 669:\n",
      "Loss train 0.007383557693101466 valid 0.008629006969824505\n",
      "Epoch 670:\n",
      "Loss train 0.007847198671661317 valid 0.007241315261169959\n",
      "Epoch 671:\n",
      "Loss train 0.007391594885848463 valid 0.006920117621518854\n",
      "Epoch 672:\n",
      "Loss train 0.008502780483104289 valid 0.006993718684773688\n",
      "Epoch 673:\n",
      "Loss train 0.007422535880468786 valid 0.0074044238921238455\n",
      "Epoch 674:\n",
      "Loss train 0.008061486459337176 valid 0.00881458334738395\n",
      "Epoch 675:\n",
      "Loss train 0.007329162838868797 valid 0.007306347568856764\n",
      "Epoch 676:\n",
      "Loss train 0.008079932467080653 valid 0.00745933578142304\n",
      "Epoch 677:\n",
      "Loss train 0.00750849696341902 valid 0.006902586163144373\n",
      "Epoch 678:\n",
      "Loss train 0.008108176845125855 valid 0.007099960106523913\n",
      "Epoch 679:\n",
      "Loss train 0.007846575207076966 valid 0.009604845276258052\n",
      "Epoch 680:\n",
      "Loss train 0.007850058376789093 valid 0.007588122838040569\n",
      "Epoch 681:\n",
      "Loss train 0.0077278692834079265 valid 0.007190791816970664\n",
      "Epoch 682:\n",
      "Loss train 0.007647912972606719 valid 0.007260053798463854\n",
      "Epoch 683:\n",
      "Loss train 0.007767148590646684 valid 0.0073982614218126245\n",
      "Epoch 684:\n",
      "Loss train 0.007992628053762019 valid 0.0070614133183970655\n",
      "Epoch 685:\n",
      "Loss train 0.008085281988605857 valid 0.008033104802565458\n",
      "Epoch 686:\n",
      "Loss train 0.00788478693459183 valid 0.0074956638394670285\n",
      "Epoch 687:\n",
      "Loss train 0.00777148730121553 valid 0.007028418620753642\n",
      "Epoch 688:\n",
      "Loss train 0.00735443796031177 valid 0.009807093127117543\n",
      "Epoch 689:\n",
      "Loss train 0.007804689956828952 valid 0.0072486577435315344\n",
      "Epoch 690:\n",
      "Loss train 0.007773135267198086 valid 0.007005556799097097\n",
      "Epoch 691:\n",
      "Loss train 0.008345420164987445 valid 0.008375950522473825\n",
      "Epoch 692:\n",
      "Loss train 0.007625083308666945 valid 0.007112047443354771\n",
      "Epoch 693:\n",
      "Loss train 0.007554669664241374 valid 0.007781224125812858\n",
      "Epoch 694:\n",
      "Loss train 0.008202609000727534 valid 0.007003288702570389\n",
      "Epoch 695:\n",
      "Loss train 0.007702987836673856 valid 0.006984673668128672\n",
      "Epoch 696:\n",
      "Loss train 0.007311547081917525 valid 0.013487335534439123\n",
      "Epoch 697:\n",
      "Loss train 0.00826881745364517 valid 0.007282104400236599\n",
      "Epoch 698:\n",
      "Loss train 0.00756348691880703 valid 0.007028968266797274\n",
      "Epoch 699:\n",
      "Loss train 0.008050207211636008 valid 0.008796122865731629\n",
      "Epoch 700:\n",
      "Loss train 0.0075852024275809525 valid 0.007172355384318087\n",
      "Epoch 701:\n",
      "Loss train 0.007944486783817411 valid 0.007693183653469842\n",
      "Epoch 702:\n",
      "Loss train 0.00783906304743141 valid 0.007856836460578831\n",
      "Epoch 703:\n",
      "Loss train 0.007436127844266593 valid 0.007192944458599652\n",
      "Epoch 704:\n",
      "Loss train 0.008193686455488205 valid 0.0069779008273157216\n",
      "Epoch 705:\n",
      "Loss train 0.007493823999539018 valid 0.01235832778643635\n",
      "Epoch 706:\n",
      "Loss train 0.008187305880710483 valid 0.007472480837926194\n",
      "Epoch 707:\n",
      "Loss train 0.007543065780773759 valid 0.010185094008238018\n",
      "Epoch 708:\n",
      "Loss train 0.007915932051837445 valid 0.008675277439307646\n",
      "Epoch 709:\n",
      "Loss train 0.008025760287418962 valid 0.007233518544397722\n",
      "Epoch 710:\n",
      "Loss train 0.007776852273382246 valid 0.029325005787411897\n",
      "Epoch 711:\n",
      "Loss train 0.008439767714589834 valid 0.007101990023063404\n",
      "Epoch 712:\n",
      "Loss train 0.007135303923860192 valid 0.007086991968995985\n",
      "Epoch 713:\n",
      "Loss train 0.00792564614675939 valid 0.008953569128969757\n",
      "Epoch 714:\n",
      "Loss train 0.007432585265487433 valid 0.009078017339200854\n",
      "Epoch 715:\n",
      "Loss train 0.007889485540799797 valid 0.007466092997516318\n",
      "Epoch 716:\n",
      "Loss train 0.007517293216660619 valid 0.007187839919270611\n",
      "Epoch 717:\n",
      "Loss train 0.008272172282449902 valid 0.006960743037081733\n",
      "Epoch 718:\n",
      "Loss train 0.007279105861671269 valid 0.0069818511443948\n",
      "Epoch 719:\n",
      "Loss train 0.008338165418244899 valid 0.0072121653375505955\n",
      "Epoch 720:\n",
      "Loss train 0.007352529424242676 valid 0.00879203207470291\n",
      "Epoch 721:\n",
      "Loss train 0.007799275377765298 valid 0.007746694954819821\n",
      "Epoch 722:\n",
      "Loss train 0.007893308191560209 valid 0.008803828968989688\n",
      "Epoch 723:\n",
      "Loss train 0.00801443612203002 valid 0.007671044041493534\n",
      "Epoch 724:\n",
      "Loss train 0.00799618047196418 valid 0.007136051095794428\n",
      "Epoch 725:\n",
      "Loss train 0.007480630772188306 valid 0.007806472930514643\n",
      "Epoch 726:\n",
      "Loss train 0.008043768899515271 valid 0.00788032620300702\n",
      "Epoch 727:\n",
      "Loss train 0.007570352060720324 valid 0.0070361627048447445\n",
      "Epoch 728:\n",
      "Loss train 0.007965851803310215 valid 0.006998212402818482\n",
      "Epoch 729:\n",
      "Loss train 0.007521334984339774 valid 0.008307615465110583\n",
      "Epoch 730:\n",
      "Loss train 0.008042225916869938 valid 0.007233637828353459\n",
      "Epoch 731:\n",
      "Loss train 0.007838996737264096 valid 0.007801131138830962\n",
      "Epoch 732:\n",
      "Loss train 0.007605404164642096 valid 0.007070249803316147\n",
      "Epoch 733:\n",
      "Loss train 0.007976005100645125 valid 0.008636553750153998\n",
      "Epoch 734:\n",
      "Loss train 0.0076004579709842805 valid 0.007474050014115282\n",
      "Epoch 735:\n",
      "Loss train 0.008009398058056831 valid 0.0074047311950753356\n",
      "Epoch 736:\n",
      "Loss train 0.007789101810194552 valid 0.007790965403797241\n",
      "Epoch 737:\n",
      "Loss train 0.007463660254143178 valid 0.007178061590939088\n",
      "Epoch 738:\n",
      "Loss train 0.009174855365417898 valid 0.006963699571631406\n",
      "Epoch 739:\n",
      "Loss train 0.007261613537557423 valid 0.007214512197497482\n",
      "Epoch 740:\n",
      "Loss train 0.007172237243503332 valid 0.007479004539764167\n",
      "Epoch 741:\n",
      "Loss train 0.007878402159549295 valid 0.007109870044603505\n",
      "Epoch 742:\n",
      "Loss train 0.007919820765964687 valid 0.006922219077830818\n",
      "Epoch 743:\n",
      "Loss train 0.007714700186625123 valid 0.006922781887254721\n",
      "Epoch 744:\n",
      "Loss train 0.007705941791646183 valid 0.009463727753084895\n",
      "Epoch 745:\n",
      "Loss train 0.007628147890791297 valid 0.0072085445669828095\n",
      "Epoch 746:\n",
      "Loss train 0.007868504435755312 valid 0.007071361259917758\n",
      "Epoch 747:\n",
      "Loss train 0.007872257535345852 valid 0.006931731351465113\n",
      "Epoch 748:\n",
      "Loss train 0.007826061714440584 valid 0.007798484737873976\n",
      "Epoch 749:\n",
      "Loss train 0.007412322983145714 valid 0.007295793690471878\n",
      "Epoch 750:\n",
      "Loss train 0.008109300052747131 valid 0.007025922353714135\n",
      "Epoch 751:\n",
      "Loss train 0.007645492851734162 valid 0.0077983855971440075\n",
      "Epoch 752:\n",
      "Loss train 0.008049194859340787 valid 0.007632324400843072\n",
      "Epoch 753:\n",
      "Loss train 0.007591428514569998 valid 0.007750986319619842\n",
      "Epoch 754:\n",
      "Loss train 0.00822450845502317 valid 0.007014035840240791\n",
      "Epoch 755:\n",
      "Loss train 0.007392543763853609 valid 0.007276066182814098\n",
      "Epoch 756:\n",
      "Loss train 0.008175201346166432 valid 0.007704359536521117\n",
      "Epoch 757:\n",
      "Loss train 0.007554692230187356 valid 0.008137685398414788\n",
      "Epoch 758:\n",
      "Loss train 0.007658666144125164 valid 0.007159918374273599\n",
      "Epoch 759:\n",
      "Loss train 0.007821943201124669 valid 0.007072958738438265\n",
      "Epoch 760:\n",
      "Loss train 0.007669825698249042 valid 0.007071330040546829\n",
      "Epoch 761:\n",
      "Loss train 0.007800536062568426 valid 0.007729440947572557\n",
      "Epoch 762:\n",
      "Loss train 0.0078029446816071865 valid 0.007224913412103803\n",
      "Epoch 763:\n",
      "Loss train 0.00786260211840272 valid 0.007040049432589264\n",
      "Epoch 764:\n",
      "Loss train 0.0075307411700487135 valid 0.008063393649876202\n",
      "Epoch 765:\n",
      "Loss train 0.008168232403695583 valid 0.007350514114181695\n",
      "Epoch 766:\n",
      "Loss train 0.007544780764728785 valid 0.007132142078941842\n",
      "Epoch 767:\n",
      "Loss train 0.008169438000768423 valid 0.00798957787942182\n",
      "Epoch 768:\n",
      "Loss train 0.0075727032916620375 valid 0.007110021224997622\n",
      "Epoch 769:\n",
      "Loss train 0.007704603122547269 valid 0.00688301126314626\n",
      "Epoch 770:\n",
      "Loss train 0.008336437712423504 valid 0.007687373015213951\n",
      "Epoch 771:\n",
      "Loss train 0.007316044927574694 valid 0.008275085045571881\n",
      "Epoch 772:\n",
      "Loss train 0.007686075316742063 valid 0.01332995688230348\n",
      "Epoch 773:\n",
      "Loss train 0.007954737516120075 valid 0.006994561382789835\n",
      "Epoch 774:\n",
      "Loss train 0.00766021762508899 valid 0.009840057778302363\n",
      "Epoch 775:\n",
      "Loss train 0.0075329401763156055 valid 0.007115859705201009\n",
      "Epoch 776:\n",
      "Loss train 0.008309656782075762 valid 0.010197965675793704\n",
      "Epoch 777:\n",
      "Loss train 0.007746604327112436 valid 0.00686880849493112\n",
      "Epoch 778:\n",
      "Loss train 0.007515949015505612 valid 0.006940553214746727\n",
      "Epoch 779:\n",
      "Loss train 0.007976595754735171 valid 0.00700331238856488\n",
      "Epoch 780:\n",
      "Loss train 0.007428786894306541 valid 0.007678768944992398\n",
      "Epoch 781:\n",
      "Loss train 0.00809892827179283 valid 0.007240711149519945\n",
      "Epoch 782:\n",
      "Loss train 0.0077042127633467315 valid 0.006868334282425676\n",
      "Epoch 783:\n",
      "Loss train 0.007936849724501371 valid 0.007107700005911192\n",
      "Epoch 784:\n",
      "Loss train 0.0077387625351548195 valid 0.007172603388303924\n",
      "Epoch 785:\n",
      "Loss train 0.007504344349727034 valid 0.011713657807253577\n",
      "Epoch 786:\n",
      "Loss train 0.008799201040528715 valid 0.007164303772813871\n",
      "Epoch 787:\n",
      "Loss train 0.007218099636957049 valid 0.007046885740237525\n",
      "Epoch 788:\n",
      "Loss train 0.008203102699480951 valid 0.006963485515496464\n",
      "Epoch 789:\n",
      "Loss train 0.007072799233719706 valid 0.007223857829449937\n",
      "Epoch 790:\n",
      "Loss train 0.007937427368015051 valid 0.007354741793913294\n",
      "Epoch 791:\n",
      "Loss train 0.0076597242103889586 valid 0.00826409567454033\n",
      "Epoch 792:\n",
      "Loss train 0.00767777412198484 valid 0.007799629462696825\n",
      "Epoch 793:\n",
      "Loss train 0.0082326175365597 valid 0.0068945821549116215\n",
      "Epoch 794:\n",
      "Loss train 0.007739826920442283 valid 0.012329747414764326\n",
      "Epoch 795:\n",
      "Loss train 0.007566981799900532 valid 0.007753871245610762\n",
      "Epoch 796:\n",
      "Loss train 0.007743254909291864 valid 0.009051047432366554\n",
      "Epoch 797:\n",
      "Loss train 0.007724132481962442 valid 0.007063692577746724\n",
      "Epoch 798:\n",
      "Loss train 0.0076120366575196385 valid 0.007033632345432226\n",
      "Epoch 799:\n",
      "Loss train 0.008041869164444507 valid 0.00704014858921734\n",
      "Epoch 800:\n",
      "Loss train 0.007672604084946215 valid 0.007856415018979317\n",
      "Epoch 801:\n",
      "Loss train 0.007859091800637543 valid 0.006877988288247785\n",
      "Epoch 802:\n",
      "Loss train 0.007849431689828634 valid 0.0069068438012938336\n",
      "Epoch 803:\n",
      "Loss train 0.007582281995564699 valid 0.0071849471798218\n",
      "Epoch 804:\n",
      "Loss train 0.007724175681360066 valid 0.00795699069112174\n",
      "Epoch 805:\n",
      "Loss train 0.00777915496379137 valid 0.007107709086719569\n",
      "Epoch 806:\n",
      "Loss train 0.007757633873261512 valid 0.006953505501001937\n",
      "Epoch 807:\n",
      "Loss train 0.007763664592057466 valid 0.007033447139219879\n",
      "Epoch 808:\n",
      "Loss train 0.007832202524878085 valid 0.007367233514319718\n",
      "Epoch 809:\n",
      "Loss train 0.0077128768712282185 valid 0.01041156201589953\n",
      "Epoch 810:\n",
      "Loss train 0.007725352668203413 valid 0.006922709794810187\n",
      "Epoch 811:\n",
      "Loss train 0.007736030910164118 valid 0.009792625041407878\n",
      "Epoch 812:\n",
      "Loss train 0.007849017125554383 valid 0.006886268923454039\n",
      "Epoch 813:\n",
      "Loss train 0.007886115252040327 valid 0.006944145527615626\n",
      "Epoch 814:\n",
      "Loss train 0.00798023507464677 valid 0.00705248204307932\n",
      "Epoch 815:\n",
      "Loss train 0.007689090729691088 valid 0.0076552424416792865\n",
      "Epoch 816:\n",
      "Loss train 0.007540143872611225 valid 0.007215534520253322\n",
      "Epoch 817:\n",
      "Loss train 0.008032890702597797 valid 0.007098619289749442\n",
      "Epoch 818:\n",
      "Loss train 0.007556321807205677 valid 0.007358049764876705\n",
      "Epoch 819:\n",
      "Loss train 0.007910804064013064 valid 0.010599855443389887\n",
      "Epoch 820:\n",
      "Loss train 0.007580640390515327 valid 0.007150828570654171\n",
      "Epoch 821:\n",
      "Loss train 0.007865894348360597 valid 0.009067960037488242\n",
      "Epoch 822:\n",
      "Loss train 0.007660623379051685 valid 0.007154731424804382\n",
      "Epoch 823:\n",
      "Loss train 0.007876187046058476 valid 0.006988611769638334\n",
      "Epoch 824:\n",
      "Loss train 0.0077338584139943125 valid 0.006861006744489142\n",
      "Epoch 825:\n",
      "Loss train 0.00784749330021441 valid 0.007006737351219298\n",
      "Epoch 826:\n",
      "Loss train 0.007863594475202263 valid 0.007082574625236107\n",
      "Epoch 827:\n",
      "Loss train 0.007578770965337754 valid 0.007676115465244545\n",
      "Epoch 828:\n",
      "Loss train 0.007907785177230835 valid 0.009563073354110696\n",
      "Epoch 829:\n",
      "Loss train 0.007414504634216428 valid 0.007836332050475178\n",
      "Epoch 830:\n",
      "Loss train 0.008601948372088372 valid 0.007208040493151043\n",
      "Epoch 831:\n",
      "Loss train 0.0072316683130338785 valid 0.00800085222017668\n",
      "Epoch 832:\n",
      "Loss train 0.00809764941688627 valid 0.007581017260273203\n",
      "Epoch 833:\n",
      "Loss train 0.007726628454402089 valid 0.008865183782323122\n",
      "Epoch 834:\n",
      "Loss train 0.007449546772986651 valid 0.006853255360563613\n",
      "Epoch 835:\n",
      "Loss train 0.0077469916874542835 valid 0.007814537140309755\n",
      "Epoch 836:\n",
      "Loss train 0.007649665237404406 valid 0.007681778477315449\n",
      "Epoch 837:\n",
      "Loss train 0.007722921329550445 valid 0.007920945785477907\n",
      "Epoch 838:\n",
      "Loss train 0.007753828330896795 valid 0.0069583827489416265\n",
      "Epoch 839:\n",
      "Loss train 0.008391725728288293 valid 0.007188973484740825\n",
      "Epoch 840:\n",
      "Loss train 0.007444822085089981 valid 0.007433108103580651\n",
      "Epoch 841:\n",
      "Loss train 0.007530175368301571 valid 0.006974377935224707\n",
      "Epoch 842:\n",
      "Loss train 0.007809022911824286 valid 0.007439674837444509\n",
      "Epoch 843:\n",
      "Loss train 0.008136576092801988 valid 0.01166827090824585\n",
      "Epoch 844:\n",
      "Loss train 0.007656052480451763 valid 0.006859718823689865\n",
      "Epoch 845:\n",
      "Loss train 0.007743636234663427 valid 0.00693556665950933\n",
      "Epoch 846:\n",
      "Loss train 0.007844802723266184 valid 0.007231617317974557\n",
      "Epoch 847:\n",
      "Loss train 0.007523997616954148 valid 0.008107576694536593\n",
      "Epoch 848:\n",
      "Loss train 0.007870803512632847 valid 0.007439206752368804\n",
      "Epoch 849:\n",
      "Loss train 0.00777214172296226 valid 0.00851352086412194\n",
      "Epoch 850:\n",
      "Loss train 0.007929368074983359 valid 0.008479046966788984\n",
      "Epoch 851:\n",
      "Loss train 0.007657209318131209 valid 0.007933473248798504\n",
      "Epoch 852:\n",
      "Loss train 0.007676198175176978 valid 0.010018000416618785\n",
      "Epoch 853:\n",
      "Loss train 0.007892311015166343 valid 0.00773337302278468\n",
      "Epoch 854:\n",
      "Loss train 0.00752763042692095 valid 0.00719743061733171\n",
      "Epoch 855:\n",
      "Loss train 0.007904613646678627 valid 0.0074388868764486415\n",
      "Epoch 856:\n",
      "Loss train 0.007543291058391332 valid 0.007490663752807796\n",
      "Epoch 857:\n",
      "Loss train 0.007709822291508317 valid 0.007325400123396659\n",
      "Epoch 858:\n",
      "Loss train 0.00798268145415932 valid 0.007088550664584223\n",
      "Epoch 859:\n",
      "Loss train 0.007613866562023759 valid 0.007242022120138822\n",
      "Epoch 860:\n",
      "Loss train 0.007948121288791299 valid 0.008239602920935351\n",
      "Epoch 861:\n",
      "Loss train 0.007727701179683208 valid 0.008129498359070208\n",
      "Epoch 862:\n",
      "Loss train 0.007715335437096655 valid 0.007052144241258438\n",
      "Epoch 863:\n",
      "Loss train 0.00771967560518533 valid 0.007922865777089647\n",
      "Epoch 864:\n",
      "Loss train 0.008684328426606953 valid 0.007371491496817365\n",
      "Epoch 865:\n",
      "Loss train 0.007053494746796787 valid 0.007059554966384656\n",
      "Epoch 866:\n",
      "Loss train 0.007975002052262425 valid 0.007295470097171181\n",
      "Epoch 867:\n",
      "Loss train 0.007398690595291555 valid 0.00695683555408306\n",
      "Epoch 868:\n",
      "Loss train 0.007796300956979394 valid 0.006940791668225738\n",
      "Epoch 869:\n",
      "Loss train 0.007739616981707513 valid 0.006844102435329601\n",
      "Epoch 870:\n",
      "Loss train 0.007778577278368175 valid 0.007036432477870546\n",
      "Epoch 871:\n",
      "Loss train 0.007851862306706607 valid 0.007440891868950181\n",
      "Epoch 872:\n",
      "Loss train 0.007425058567896485 valid 0.007580585414252497\n",
      "Epoch 873:\n",
      "Loss train 0.008060997161082923 valid 0.007294955764284007\n",
      "Epoch 874:\n",
      "Loss train 0.0073172193672508 valid 0.01202144232759876\n",
      "Epoch 875:\n",
      "Loss train 0.008017952009104191 valid 0.008499180850606521\n",
      "Epoch 876:\n",
      "Loss train 0.007908253064379096 valid 0.007031494052302537\n",
      "Epoch 877:\n",
      "Loss train 0.007713280273601413 valid 0.007048650345360255\n",
      "Epoch 878:\n",
      "Loss train 0.008055910891853273 valid 0.006850880229214339\n",
      "Epoch 879:\n",
      "Loss train 0.00769950223620981 valid 0.008893548054157535\n",
      "Epoch 880:\n",
      "Loss train 0.007494188696146011 valid 0.007155386079103492\n",
      "Epoch 881:\n",
      "Loss train 0.008044829657301307 valid 0.009087054357539738\n",
      "Epoch 882:\n",
      "Loss train 0.0073001371463760735 valid 0.0068218344390688175\n",
      "Epoch 883:\n",
      "Loss train 0.007908189296722412 valid 0.007246067667057618\n",
      "Epoch 884:\n",
      "Loss train 0.007787858759984374 valid 0.007099960832419321\n",
      "Epoch 885:\n",
      "Loss train 0.00900603630580008 valid 0.007102533817184261\n",
      "Epoch 886:\n",
      "Loss train 0.006979989139363169 valid 0.006947864690679559\n",
      "Epoch 887:\n",
      "Loss train 0.007337622023187578 valid 0.007031829314161938\n",
      "Epoch 888:\n",
      "Loss train 0.007698865132406354 valid 0.007015660779087886\n",
      "Epoch 889:\n",
      "Loss train 0.007621406796388328 valid 0.007139329229325973\n",
      "Epoch 890:\n",
      "Loss train 0.007818075246177614 valid 0.008327492033463058\n",
      "Epoch 891:\n",
      "Loss train 0.00797508392482996 valid 0.007269387191744579\n",
      "Epoch 892:\n",
      "Loss train 0.007443731492385268 valid 0.009483355113330745\n",
      "Epoch 893:\n",
      "Loss train 0.007968870694749058 valid 0.006958990581145537\n",
      "Epoch 894:\n",
      "Loss train 0.007787650427781045 valid 0.008091661905271311\n",
      "Epoch 895:\n",
      "Loss train 0.007582580903545022 valid 0.006945543234648671\n",
      "Epoch 896:\n",
      "Loss train 0.007536991308443248 valid 0.007231266089780561\n",
      "Epoch 897:\n",
      "Loss train 0.007986347624100745 valid 0.007040765903321967\n",
      "Epoch 898:\n",
      "Loss train 0.007746424432843924 valid 0.006962468607737071\n",
      "Epoch 899:\n",
      "Loss train 0.007882513636723161 valid 0.006951902464004325\n",
      "Epoch 900:\n",
      "Loss train 0.007938379305414855 valid 0.008171993029867992\n",
      "Epoch 901:\n",
      "Loss train 0.007694548694416881 valid 0.007733625054656453\n",
      "Epoch 902:\n",
      "Loss train 0.007472933102399111 valid 0.00788325464183962\n",
      "Epoch 903:\n",
      "Loss train 0.0076694183750078086 valid 0.009993666324029642\n",
      "Epoch 904:\n",
      "Loss train 0.007815825697034597 valid 0.007635799700446949\n",
      "Epoch 905:\n",
      "Loss train 0.007930754953995346 valid 0.00834016299369655\n",
      "Epoch 906:\n",
      "Loss train 0.007645491701550782 valid 0.006957583221557787\n",
      "Epoch 907:\n",
      "Loss train 0.0075711913220584395 valid 0.010722477994157853\n",
      "Epoch 908:\n",
      "Loss train 0.007881046663969755 valid 0.007011846294747817\n",
      "Epoch 909:\n",
      "Loss train 0.007719584424048662 valid 0.008777336091252285\n",
      "Epoch 910:\n",
      "Loss train 0.008063253108412027 valid 0.007551034328527904\n",
      "Epoch 911:\n",
      "Loss train 0.0075925681414082645 valid 0.007067482358787135\n",
      "Epoch 912:\n",
      "Loss train 0.007846932783722877 valid 0.009152879205740603\n",
      "Epoch 913:\n",
      "Loss train 0.007496993886306882 valid 0.007752449970649798\n",
      "Epoch 914:\n",
      "Loss train 0.007843279936350882 valid 0.008777940159787641\n",
      "Epoch 915:\n",
      "Loss train 0.007497853278182447 valid 0.007339702070549774\n",
      "Epoch 916:\n",
      "Loss train 0.008503975016064941 valid 0.006933431169645963\n",
      "Epoch 917:\n",
      "Loss train 0.007045557978563011 valid 0.006962668333619585\n",
      "Epoch 918:\n",
      "Loss train 0.007785318549722433 valid 0.007483064713283473\n",
      "Epoch 919:\n",
      "Loss train 0.008154843407683075 valid 0.007740879107098389\n",
      "Epoch 920:\n",
      "Loss train 0.007230087732896209 valid 0.00696182018121562\n",
      "Epoch 921:\n",
      "Loss train 0.007943788804113864 valid 0.007402307233970316\n",
      "Epoch 922:\n",
      "Loss train 0.007515061856247484 valid 0.00709340198115669\n",
      "Epoch 923:\n",
      "Loss train 0.008336956952698529 valid 0.007260514099418185\n",
      "Epoch 924:\n",
      "Loss train 0.0072728243423625825 valid 0.007112872895933955\n",
      "Epoch 925:\n",
      "Loss train 0.008102538245730102 valid 0.006853603590218015\n",
      "Epoch 926:\n",
      "Loss train 0.007402030127122998 valid 0.007387838300436198\n",
      "Epoch 927:\n",
      "Loss train 0.007727130586281419 valid 0.0073699470858741515\n",
      "Epoch 928:\n",
      "Loss train 0.007721232064068317 valid 0.009618845523362727\n",
      "Epoch 929:\n",
      "Loss train 0.007978689218871295 valid 0.0069619742580457255\n",
      "Epoch 930:\n",
      "Loss train 0.00754998488817364 valid 0.006934341698773013\n",
      "Epoch 931:\n",
      "Loss train 0.00802460384555161 valid 0.007600060891561572\n",
      "Epoch 932:\n",
      "Loss train 0.0073504525888711215 valid 0.008369351661137716\n",
      "Epoch 933:\n",
      "Loss train 0.008251555501483382 valid 0.007132472076568941\n",
      "Epoch 934:\n",
      "Loss train 0.00744761873036623 valid 0.00906683978400413\n",
      "Epoch 935:\n",
      "Loss train 0.007611117181368172 valid 0.008916107977913961\n",
      "Epoch 936:\n",
      "Loss train 0.00771067320369184 valid 0.006951411018462197\n",
      "Epoch 937:\n",
      "Loss train 0.007692586099728942 valid 0.007001861367396678\n",
      "Epoch 938:\n",
      "Loss train 0.008059664350003003 valid 0.00717872274254536\n",
      "Epoch 939:\n",
      "Loss train 0.007365367268212139 valid 0.008023706726282607\n",
      "Epoch 940:\n",
      "Loss train 0.007761416807770729 valid 0.007009180305613351\n",
      "Epoch 941:\n",
      "Loss train 0.007922454020008445 valid 0.007151901164955698\n",
      "Epoch 942:\n",
      "Loss train 0.007755179023370147 valid 0.0070179781141144195\n",
      "Epoch 943:\n",
      "Loss train 0.007782982564531266 valid 0.007215853193360047\n",
      "Epoch 944:\n",
      "Loss train 0.007514958344399929 valid 0.007291883991243964\n",
      "Epoch 945:\n",
      "Loss train 0.008500881157815457 valid 0.007496382558002486\n",
      "Epoch 946:\n",
      "Loss train 0.00697655011434108 valid 0.007308775347944485\n",
      "Epoch 947:\n",
      "Loss train 0.00799389362335205 valid 0.007267345603929006\n",
      "Epoch 948:\n",
      "Loss train 0.007652907292358577 valid 0.007439058948924522\n",
      "Epoch 949:\n",
      "Loss train 0.007875616941601039 valid 0.0070348339339986144\n",
      "Epoch 950:\n",
      "Loss train 0.007644594637677074 valid 0.008102779442320724\n",
      "Epoch 951:\n",
      "Loss train 0.0075784522574394945 valid 0.007847261478958653\n",
      "Epoch 952:\n",
      "Loss train 0.007881411472335459 valid 0.020668376660985734\n",
      "Epoch 953:\n",
      "Loss train 0.007696217768825591 valid 0.007533946591734715\n",
      "Epoch 954:\n",
      "Loss train 0.007610413683578372 valid 0.0087750948617022\n",
      "Epoch 955:\n",
      "Loss train 0.008223500377498567 valid 0.006896361688102768\n",
      "Epoch 956:\n",
      "Loss train 0.007172514195553958 valid 0.010459089766807199\n",
      "Epoch 957:\n",
      "Loss train 0.008165083532221616 valid 0.009022933279238697\n",
      "Epoch 958:\n",
      "Loss train 0.0073171094851568344 valid 0.006915293670610646\n",
      "Epoch 959:\n",
      "Loss train 0.007923715217038989 valid 0.007038433962499359\n",
      "Epoch 960:\n",
      "Loss train 0.00754700428340584 valid 0.0072006839741353135\n",
      "Epoch 961:\n",
      "Loss train 0.00823149665724486 valid 0.006941877388594902\n",
      "Epoch 962:\n",
      "Loss train 0.007242950522340834 valid 0.006959813676727524\n",
      "Epoch 963:\n",
      "Loss train 0.007703234860673547 valid 0.008390692238617324\n",
      "Epoch 964:\n",
      "Loss train 0.008095481181517244 valid 0.00829143304639523\n",
      "Epoch 965:\n",
      "Loss train 0.007436131271533668 valid 0.009903287492589384\n",
      "Epoch 966:\n",
      "Loss train 0.007838383903726936 valid 0.006906761969109269\n",
      "Epoch 967:\n",
      "Loss train 0.007826216910034418 valid 0.007952984220191183\n",
      "Epoch 968:\n",
      "Loss train 0.007755836583673954 valid 0.007310553849590476\n",
      "Epoch 969:\n",
      "Loss train 0.007597597884014249 valid 0.006915990101630039\n",
      "Epoch 970:\n",
      "Loss train 0.00897824103012681 valid 0.006980798154057882\n",
      "Epoch 971:\n",
      "Loss train 0.006962434900924563 valid 0.008288132125673133\n",
      "Epoch 972:\n",
      "Loss train 0.007383124972693622 valid 0.00695772851089876\n",
      "Epoch 973:\n",
      "Loss train 0.008397009535692633 valid 0.00871977636605658\n",
      "Epoch 974:\n",
      "Loss train 0.006982428273186087 valid 0.007034400947281425\n",
      "Epoch 975:\n",
      "Loss train 0.008266679849475623 valid 0.008812759098088195\n",
      "Epoch 976:\n",
      "Loss train 0.007172114048153162 valid 0.007076098561448265\n",
      "Epoch 977:\n",
      "Loss train 0.007698350963182748 valid 0.008193780200763676\n",
      "Epoch 978:\n",
      "Loss train 0.007824037061072885 valid 0.007161641123593628\n",
      "Epoch 979:\n",
      "Loss train 0.007996122841723264 valid 0.007679381987375978\n",
      "Epoch 980:\n",
      "Loss train 0.0072770373895764354 valid 0.0068563437556161365\n",
      "Epoch 981:\n",
      "Loss train 0.008062582276761533 valid 0.0077664894700760975\n",
      "Epoch 982:\n",
      "Loss train 0.0077389686042442915 valid 0.00691954022434615\n",
      "Epoch 983:\n",
      "Loss train 0.007598075103014707 valid 0.006879197599430675\n",
      "Epoch 984:\n",
      "Loss train 0.007489437335170806 valid 0.008011074496400959\n",
      "Epoch 985:\n",
      "Loss train 0.007819908605888486 valid 0.007168065002133655\n",
      "Epoch 986:\n",
      "Loss train 0.007689905478619039 valid 0.0074752036925398715\n",
      "Epoch 987:\n",
      "Loss train 0.007838167184963822 valid 0.00721629558557592\n",
      "Epoch 988:\n",
      "Loss train 0.007758882418274879 valid 0.007198137382484295\n",
      "Epoch 989:\n",
      "Loss train 0.007753986553288996 valid 0.007115454956048685\n",
      "Epoch 990:\n",
      "Loss train 0.007416733014397323 valid 0.007695469096625611\n",
      "Epoch 991:\n",
      "Loss train 0.007659046198241413 valid 0.010715451425340057\n",
      "Epoch 992:\n",
      "Loss train 0.008886443004012107 valid 0.006995603406094267\n",
      "Epoch 993:\n",
      "Loss train 0.007045875093899668 valid 0.006943969343877545\n",
      "Epoch 994:\n",
      "Loss train 0.007865533861331642 valid 0.0070899013975626495\n",
      "Epoch 995:\n",
      "Loss train 0.007613045359030366 valid 0.007391937939349281\n",
      "Epoch 996:\n",
      "Loss train 0.008247667173855007 valid 0.015018763887484313\n",
      "Epoch 997:\n",
      "Loss train 0.007628458691760898 valid 0.007272468665103189\n",
      "Epoch 998:\n",
      "Loss train 0.007177792815491557 valid 0.007363243712659895\n",
      "Epoch 999:\n",
      "Loss train 0.00782706351019442 valid 0.007673062592853838\n",
      "Epoch 1000:\n",
      "Loss train 0.007823175247758628 valid 0.006855955767894009\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50_000\n",
    "learning_rate = 1e-4\n",
    "n_epochs = 1000\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {}:'.format(epoch + 1))\n",
    "\n",
    "    model.train()\n",
    "    avg_loss = train_one_epoch()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_outputs = torch.squeeze(model(torch.from_numpy(valid_features_normalized.astype(\"float32\"))))\n",
    "        valid_loss = loss_function(valid_outputs, torch.from_numpy(valid_fco2).to(torch.device(\"cuda\"))).detach().cpu().item()\n",
    "\n",
    "    print('Loss train {} valid {}'.format(avg_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d639505-8278-41f6-b3b8-155177efdb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (for consistency check):  0.006855955767894008\n",
      "RMSE:  0.0828006990785344\n",
      "Maximum absolute deviation:  3.519013396252376\n",
      "99.9th percentile of absolute deviation (1000 val's larger):  0.45540268991040084\n",
      "99.99th percentile of absolute deviation (100 val's larger):  0.9623863943684487\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE (for consistency check): \", MSE(valid_fco2, model_output_after_training))\n",
    "print(\"RMSE: \", np.sqrt(MSE(valid_fco2, model_output_after_training)))\n",
    "print(\"Maximum absolute deviation: \", np.max(np.abs(model_output_after_training-valid_fco2)))\n",
    "print(\"99.9th percentile of absolute deviation (1000 val's larger): \", np.percentile(np.abs(model_output_after_training-valid_fco2), q=99.9))\n",
    "print(\"99.99th percentile of absolute deviation (100 val's larger): \", np.percentile(np.abs(model_output_after_training-valid_fco2), q=99.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5eea0378-bdd5-4fd1-9730-c62001bee81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Loss train 0.009078334085643291 valid 0.0072811123888851656\n",
      "Epoch 2:\n",
      "Loss train 0.007454729871824384 valid 0.0071899387067103394\n",
      "Epoch 3:\n",
      "Loss train 0.00696673528291285 valid 0.006907209559081659\n",
      "Epoch 4:\n",
      "Loss train 0.006815510056912899 valid 0.00683514597905401\n",
      "Epoch 5:\n",
      "Loss train 0.006749142706394195 valid 0.0067843893820044385\n",
      "Epoch 6:\n",
      "Loss train 0.006730653578415513 valid 0.006775729838593238\n",
      "Epoch 7:\n",
      "Loss train 0.006723313871771097 valid 0.00677013003957253\n",
      "Epoch 8:\n",
      "Loss train 0.006719994731247425 valid 0.006768491964312388\n",
      "Epoch 9:\n",
      "Loss train 0.006718560680747032 valid 0.006768577919728333\n",
      "Epoch 10:\n",
      "Loss train 0.006718285055831075 valid 0.006768457834581058\n",
      "Epoch 11:\n",
      "Loss train 0.006717982003465295 valid 0.006767944851430694\n",
      "Epoch 12:\n",
      "Loss train 0.006717538507655263 valid 0.006767263421786092\n",
      "Epoch 13:\n",
      "Loss train 0.006717606633901596 valid 0.006767349122552413\n",
      "Epoch 14:\n",
      "Loss train 0.0067174427676945925 valid 0.006767244989098872\n",
      "Epoch 15:\n",
      "Loss train 0.006717734411358834 valid 0.006767698140260528\n",
      "Epoch 16:\n",
      "Loss train 0.006717836018651724 valid 0.0067675127035180995\n",
      "Epoch 17:\n",
      "Loss train 0.006717908848077059 valid 0.0067685905271406025\n",
      "Epoch 18:\n",
      "Loss train 0.006718275044113398 valid 0.006767728716856406\n",
      "Epoch 19:\n",
      "Loss train 0.006718102376908064 valid 0.006768930061761754\n",
      "Epoch 20:\n",
      "Loss train 0.006718726688995957 valid 0.0067674210175794605\n",
      "Epoch 21:\n",
      "Loss train 0.006717949965968728 valid 0.006768387765911583\n",
      "Epoch 22:\n",
      "Loss train 0.006718416418880224 valid 0.006767640213952316\n",
      "Epoch 23:\n",
      "Loss train 0.006718320632353425 valid 0.006768937962978642\n",
      "Epoch 24:\n",
      "Loss train 0.0067179663572460415 valid 0.0067673499439479656\n",
      "Epoch 25:\n",
      "Loss train 0.006717671127989888 valid 0.006767919678497486\n",
      "Epoch 26:\n",
      "Loss train 0.006718088779598475 valid 0.006767356266564038\n",
      "Epoch 27:\n",
      "Loss train 0.006718042120337486 valid 0.006768991769987329\n",
      "Epoch 28:\n",
      "Loss train 0.006719019170850516 valid 0.00676888385172901\n",
      "Epoch 29:\n",
      "Loss train 0.006718179397284985 valid 0.006767471602170031\n",
      "Epoch 30:\n",
      "Loss train 0.006718399422243238 valid 0.006768348267632654\n",
      "Epoch 31:\n",
      "Loss train 0.006718407804146409 valid 0.006768756687439729\n",
      "Epoch 32:\n",
      "Loss train 0.006718442076817155 valid 0.006769678143068167\n",
      "Epoch 33:\n",
      "Loss train 0.006719152769073844 valid 0.006770549725455706\n",
      "Epoch 34:\n",
      "Loss train 0.00671909861266613 valid 0.006768463277258073\n",
      "Epoch 35:\n",
      "Loss train 0.00671855709515512 valid 0.006767281058633624\n",
      "Epoch 36:\n",
      "Loss train 0.006718198442831636 valid 0.006767718016697796\n",
      "Epoch 37:\n",
      "Loss train 0.006718373997136951 valid 0.006767864102602644\n",
      "Epoch 38:\n",
      "Loss train 0.006718848645687103 valid 0.006768422800341857\n",
      "Epoch 39:\n",
      "Loss train 0.006718578888103366 valid 0.006768075013736797\n",
      "Epoch 40:\n",
      "Loss train 0.006718711601570249 valid 0.0067680493910567945\n",
      "Epoch 41:\n",
      "Loss train 0.006718040769919753 valid 0.006767518704816478\n",
      "Epoch 42:\n",
      "Loss train 0.006718237465247512 valid 0.0067692418552370865\n",
      "Epoch 43:\n",
      "Loss train 0.006718385173007846 valid 0.006768073129701294\n",
      "Epoch 44:\n",
      "Loss train 0.006717792572453618 valid 0.006767795675048228\n",
      "Epoch 45:\n",
      "Loss train 0.006718119326978922 valid 0.006767861031905564\n",
      "Epoch 46:\n",
      "Loss train 0.0067181230522692205 valid 0.0067677022490128\n",
      "Epoch 47:\n",
      "Loss train 0.006718101073056459 valid 0.006767591331959339\n",
      "Epoch 48:\n",
      "Loss train 0.006717651709914207 valid 0.0067672935065239345\n",
      "Epoch 49:\n",
      "Loss train 0.006718138884752989 valid 0.006769184634048285\n",
      "Epoch 50:\n",
      "Loss train 0.006718394346535206 valid 0.0067677969033036675\n",
      "Epoch 51:\n",
      "Loss train 0.006718359235674143 valid 0.006767702270741375\n",
      "Epoch 52:\n",
      "Loss train 0.006719146482646465 valid 0.006769343270278876\n",
      "Epoch 53:\n",
      "Loss train 0.006718956213444471 valid 0.006767447039989899\n",
      "Epoch 54:\n",
      "Loss train 0.0067187229637056586 valid 0.006768590377837064\n",
      "Epoch 55:\n",
      "Loss train 0.006719279475510121 valid 0.006771111463493599\n",
      "Epoch 56:\n",
      "Loss train 0.0067190323024988174 valid 0.006767841570586289\n",
      "Epoch 57:\n",
      "Loss train 0.0067187577951699495 valid 0.006769503124571898\n",
      "Epoch 58:\n",
      "Loss train 0.006719589605927468 valid 0.00676939905792919\n",
      "Epoch 59:\n",
      "Loss train 0.00671866643242538 valid 0.0067682027557013115\n",
      "Epoch 60:\n",
      "Loss train 0.006718665780499578 valid 0.006769458807114107\n",
      "Epoch 61:\n",
      "Loss train 0.0067203829530626535 valid 0.006773772338067048\n",
      "Epoch 62:\n",
      "Loss train 0.006720854388549924 valid 0.00676914085700969\n",
      "Epoch 63:\n",
      "Loss train 0.006718923291191459 valid 0.006769001822378185\n",
      "Epoch 64:\n",
      "Loss train 0.006718974374234676 valid 0.006768117685572751\n",
      "Epoch 65:\n",
      "Loss train 0.006717931013554335 valid 0.006768858714728634\n",
      "Epoch 66:\n",
      "Loss train 0.006719098286703229 valid 0.006767984732988146\n",
      "Epoch 67:\n",
      "Loss train 0.006719525577500463 valid 0.006768998948134096\n",
      "Epoch 68:\n",
      "Loss train 0.006718307873234153 valid 0.006767554561508155\n",
      "Epoch 69:\n",
      "Loss train 0.00671811900101602 valid 0.0067694177764998234\n",
      "Epoch 70:\n",
      "Loss train 0.006718631228432059 valid 0.006768311275208558\n",
      "Epoch 71:\n",
      "Loss train 0.006718557747080922 valid 0.00676778928671262\n",
      "Epoch 72:\n",
      "Loss train 0.006717975344508886 valid 0.006767308605802003\n",
      "Epoch 73:\n",
      "Loss train 0.006718551460653544 valid 0.006768696500533743\n",
      "Epoch 74:\n",
      "Loss train 0.0067187070846557615 valid 0.006767442134852937\n",
      "Epoch 75:\n",
      "Loss train 0.006718637328594923 valid 0.006768716658308665\n",
      "Epoch 76:\n",
      "Loss train 0.006718979775905609 valid 0.00676779756649557\n",
      "Epoch 77:\n",
      "Loss train 0.006719588721171022 valid 0.006770576178467293\n",
      "Epoch 78:\n",
      "Loss train 0.006721322797238827 valid 0.0067749098465950276\n",
      "Epoch 79:\n",
      "Loss train 0.006719861691817641 valid 0.006771399093325953\n",
      "Epoch 80:\n",
      "Loss train 0.0067203610204160215 valid 0.006771621863279144\n",
      "Epoch 81:\n",
      "Loss train 0.006722422037273646 valid 0.006774351674030094\n",
      "Epoch 82:\n",
      "Loss train 0.006720593757927417 valid 0.006769273709602339\n",
      "Epoch 83:\n",
      "Loss train 0.0067194236908108 valid 0.006770323285454576\n",
      "Epoch 84:\n",
      "Loss train 0.006721087638288737 valid 0.006769961024707861\n",
      "Epoch 85:\n",
      "Loss train 0.006719802739098668 valid 0.006769952576000921\n",
      "Epoch 86:\n",
      "Loss train 0.006719729583710432 valid 0.006769084193899724\n",
      "Epoch 87:\n",
      "Loss train 0.006719819316640496 valid 0.006767336363428516\n",
      "Epoch 88:\n",
      "Loss train 0.006718635093420744 valid 0.00676832862068486\n",
      "Epoch 89:\n",
      "Loss train 0.006719001941382885 valid 0.0067698204460000435\n",
      "Epoch 90:\n",
      "Loss train 0.006720467982813716 valid 0.006773517666257408\n",
      "Epoch 91:\n",
      "Loss train 0.006722810212522745 valid 0.00677077822433239\n",
      "Epoch 92:\n",
      "Loss train 0.006720349984243512 valid 0.006769985021317303\n",
      "Epoch 93:\n",
      "Loss train 0.006720061041414737 valid 0.006768796135109639\n",
      "Epoch 94:\n",
      "Loss train 0.006719160964712501 valid 0.006767916894951821\n",
      "Epoch 95:\n",
      "Loss train 0.006719565438106656 valid 0.006771912233143668\n",
      "Epoch 96:\n",
      "Loss train 0.006721590179949999 valid 0.0067702825454211595\n",
      "Epoch 97:\n",
      "Loss train 0.006719285761937499 valid 0.006776461578280565\n",
      "Epoch 98:\n",
      "Loss train 0.0067222130484879015 valid 0.006775063114439748\n",
      "Epoch 99:\n",
      "Loss train 0.006719724228605628 valid 0.0067705488529676\n",
      "Epoch 100:\n",
      "Loss train 0.006721198977902532 valid 0.006770986833855364\n",
      "Epoch 101:\n",
      "Loss train 0.006720273895189166 valid 0.006774471673444193\n",
      "Epoch 102:\n",
      "Loss train 0.006718768365681171 valid 0.00676716969683424\n",
      "Epoch 103:\n",
      "Loss train 0.006717878859490156 valid 0.006767738337797928\n",
      "Epoch 104:\n",
      "Loss train 0.0067192343529313804 valid 0.006769985674670601\n",
      "Epoch 105:\n",
      "Loss train 0.006720807543024421 valid 0.006768106498239984\n",
      "Epoch 106:\n",
      "Loss train 0.006719260662794113 valid 0.006769039866552164\n",
      "Epoch 107:\n",
      "Loss train 0.006719846185296774 valid 0.006770105553233108\n",
      "Epoch 108:\n",
      "Loss train 0.006720574945211411 valid 0.006770038820936988\n",
      "Epoch 109:\n",
      "Loss train 0.006720160692930221 valid 0.006773913374488536\n",
      "Epoch 110:\n",
      "Loss train 0.006726409494876862 valid 0.006771463840281105\n",
      "Epoch 111:\n",
      "Loss train 0.00673515647649765 valid 0.006794560547744673\n",
      "Epoch 112:\n",
      "Loss train 0.00672768303193152 valid 0.0067668915126949435\n",
      "Epoch 113:\n",
      "Loss train 0.006720013823360205 valid 0.00677283096329336\n",
      "Epoch 114:\n",
      "Loss train 0.006720980629324913 valid 0.0067684133348663205\n",
      "Epoch 115:\n",
      "Loss train 0.006718393042683601 valid 0.006768231208521703\n",
      "Epoch 116:\n",
      "Loss train 0.006718426896259189 valid 0.006768073533020981\n",
      "Epoch 117:\n",
      "Loss train 0.006718863034620881 valid 0.006767876843026639\n",
      "Epoch 118:\n",
      "Loss train 0.006723551778122783 valid 0.006774985383953002\n",
      "Epoch 119:\n",
      "Loss train 0.006722140870988369 valid 0.006766390428315173\n",
      "Epoch 120:\n",
      "Loss train 0.006719139544293285 valid 0.006767390918001716\n",
      "Epoch 121:\n",
      "Loss train 0.006717995135113597 valid 0.006767444788348893\n",
      "Epoch 122:\n",
      "Loss train 0.006719470070675015 valid 0.006768222474521002\n",
      "Epoch 123:\n",
      "Loss train 0.006718117790296674 valid 0.006767860815314387\n",
      "Epoch 124:\n",
      "Loss train 0.006720547145232558 valid 0.006769663412561416\n",
      "Epoch 125:\n",
      "Loss train 0.006726182671263814 valid 0.006782766941588728\n",
      "Epoch 126:\n",
      "Loss train 0.006729321973398328 valid 0.006785521573223191\n",
      "Epoch 127:\n",
      "Loss train 0.006727612903341651 valid 0.0067740508516113745\n",
      "Epoch 128:\n",
      "Loss train 0.0067243322264403105 valid 0.006774943108547433\n",
      "Epoch 129:\n",
      "Loss train 0.006728142499923706 valid 0.006788924274412163\n",
      "Epoch 130:\n",
      "Loss train 0.00672909296117723 valid 0.006770193348480519\n",
      "Epoch 131:\n",
      "Loss train 0.006740717217326164 valid 0.006776017824134643\n",
      "Epoch 132:\n",
      "Loss train 0.00674527958035469 valid 0.0067708153029927476\n",
      "Epoch 133:\n",
      "Loss train 0.006734183337539434 valid 0.006768716479451899\n",
      "Epoch 134:\n",
      "Loss train 0.006723020412027836 valid 0.006769937259519094\n",
      "Epoch 135:\n",
      "Loss train 0.006719052325934172 valid 0.00676810346230469\n",
      "Epoch 136:\n",
      "Loss train 0.006719485810026527 valid 0.006769140771256591\n",
      "Epoch 137:\n",
      "Loss train 0.006720484234392643 valid 0.006770135774336586\n",
      "Epoch 138:\n",
      "Loss train 0.006728337099775672 valid 0.0067698385580567885\n",
      "Epoch 139:\n",
      "Loss train 0.0067215207498520614 valid 0.006769976081810939\n",
      "Epoch 140:\n",
      "Loss train 0.006722562480717897 valid 0.006783055136990462\n",
      "Epoch 141:\n",
      "Loss train 0.006724539585411549 valid 0.006765698728659654\n",
      "Epoch 142:\n",
      "Loss train 0.006721228547394276 valid 0.006774075135116166\n",
      "Epoch 143:\n",
      "Loss train 0.006722630420699716 valid 0.00676863555375322\n",
      "Epoch 144:\n",
      "Loss train 0.006730119837448001 valid 0.006768700580862421\n",
      "Epoch 145:\n",
      "Loss train 0.0067204245366156105 valid 0.006767654885264422\n",
      "Epoch 146:\n",
      "Loss train 0.006724730925634503 valid 0.006769306008723993\n",
      "Epoch 147:\n",
      "Loss train 0.006719711236655712 valid 0.006783067710856162\n",
      "Epoch 148:\n",
      "Loss train 0.006736129987984896 valid 0.006767055175300222\n",
      "Epoch 149:\n",
      "Loss train 0.006728508044034243 valid 0.0067948331252340646\n",
      "Epoch 150:\n",
      "Loss train 0.006731756031513214 valid 0.006786270431430215\n",
      "Epoch 151:\n",
      "Loss train 0.0067243012599647045 valid 0.006770912832410038\n",
      "Epoch 152:\n",
      "Loss train 0.0067259037867188455 valid 0.00678633893636744\n",
      "Epoch 153:\n",
      "Loss train 0.006729271775111556 valid 0.006778907324723002\n",
      "Epoch 154:\n",
      "Loss train 0.0067226949613541365 valid 0.006778882205696952\n",
      "Epoch 155:\n",
      "Loss train 0.006724321097135544 valid 0.006772567109915939\n",
      "Epoch 156:\n",
      "Loss train 0.006724167568609119 valid 0.006769122131931473\n",
      "Epoch 157:\n",
      "Loss train 0.006720351008698344 valid 0.00676800024190056\n",
      "Epoch 158:\n",
      "Loss train 0.006718666246160865 valid 0.006771871104384161\n",
      "Epoch 159:\n",
      "Loss train 0.006722199590876698 valid 0.006769407015689283\n",
      "Epoch 160:\n",
      "Loss train 0.006725698616355658 valid 0.006771746395311102\n",
      "Epoch 161:\n",
      "Loss train 0.006725408183410764 valid 0.006797289198094463\n",
      "Epoch 162:\n",
      "Loss train 0.006737926136702299 valid 0.006768514797101289\n",
      "Epoch 163:\n",
      "Loss train 0.0067178643308579925 valid 0.006766944885891876\n",
      "Epoch 164:\n",
      "Loss train 0.006724027497693896 valid 0.00679450714167988\n",
      "Epoch 165:\n",
      "Loss train 0.0067414741963148115 valid 0.006777389734051801\n",
      "Epoch 166:\n",
      "Loss train 0.0067262871656566855 valid 0.00676897215016384\n",
      "Epoch 167:\n",
      "Loss train 0.006740191206336022 valid 0.0068037074446834055\n",
      "Epoch 168:\n",
      "Loss train 0.006732506677508354 valid 0.006771230167016658\n",
      "Epoch 169:\n",
      "Loss train 0.006728700455278158 valid 0.006777046657006087\n",
      "Epoch 170:\n",
      "Loss train 0.006729056686162948 valid 0.006817321260797775\n",
      "Epoch 171:\n",
      "Loss train 0.006732233287766576 valid 0.006776139472168388\n",
      "Epoch 172:\n",
      "Loss train 0.00672045792452991 valid 0.006768076545999916\n",
      "Epoch 173:\n",
      "Loss train 0.00672959852963686 valid 0.00676957746323267\n",
      "Epoch 174:\n",
      "Loss train 0.006717069074511528 valid 0.006769213200917242\n",
      "Epoch 175:\n",
      "Loss train 0.006720578158274293 valid 0.006766963637005249\n",
      "Epoch 176:\n",
      "Loss train 0.006722505623474717 valid 0.006768807807215862\n",
      "Epoch 177:\n",
      "Loss train 0.006724752439185977 valid 0.0067862361109738605\n",
      "Epoch 178:\n",
      "Loss train 0.0067235281690955166 valid 0.006775832309207866\n",
      "Epoch 179:\n",
      "Loss train 0.006720518367365003 valid 0.006768112489528197\n",
      "Epoch 180:\n",
      "Loss train 0.006719193747267127 valid 0.006768856342546283\n",
      "Epoch 181:\n",
      "Loss train 0.006717970361933112 valid 0.006765292798275237\n",
      "Epoch 182:\n",
      "Loss train 0.00671676667407155 valid 0.006765818868007013\n",
      "Epoch 183:\n",
      "Loss train 0.0067180337384343146 valid 0.006769103283050073\n",
      "Epoch 184:\n",
      "Loss train 0.006731253489851952 valid 0.006814841644967591\n",
      "Epoch 185:\n",
      "Loss train 0.006756299268454313 valid 0.006770289187126276\n",
      "Epoch 186:\n",
      "Loss train 0.006738639529794454 valid 0.006771271805712677\n",
      "Epoch 187:\n",
      "Loss train 0.006729628704488277 valid 0.006773814873302679\n",
      "Epoch 188:\n",
      "Loss train 0.006727920193225145 valid 0.006781250939771521\n",
      "Epoch 189:\n",
      "Loss train 0.006725145969539881 valid 0.006774933668426729\n",
      "Epoch 190:\n",
      "Loss train 0.006725184805691242 valid 0.006774125943463526\n",
      "Epoch 191:\n",
      "Loss train 0.006721697840839624 valid 0.0067776717850263485\n",
      "Epoch 192:\n",
      "Loss train 0.006725030252709985 valid 0.006777133413152604\n",
      "Epoch 193:\n",
      "Loss train 0.0067239351570606235 valid 0.00677420571783938\n",
      "Epoch 194:\n",
      "Loss train 0.006726493313908577 valid 0.006777072413211831\n",
      "Epoch 195:\n",
      "Loss train 0.0067247476428747175 valid 0.00677112187533794\n",
      "Epoch 196:\n",
      "Loss train 0.0067261117976158856 valid 0.0067788214831484995\n",
      "Epoch 197:\n",
      "Loss train 0.006723435176536441 valid 0.006775369617256561\n",
      "Epoch 198:\n",
      "Loss train 0.006734803318977356 valid 0.006774755462404786\n",
      "Epoch 199:\n",
      "Loss train 0.0067321038339287044 valid 0.006775186725141842\n",
      "Epoch 200:\n",
      "Loss train 0.00672363112680614 valid 0.0067839000313704104\n",
      "Epoch 201:\n",
      "Loss train 0.006733621470630169 valid 0.006770328968469709\n",
      "Epoch 202:\n",
      "Loss train 0.0067293438594788315 valid 0.006794942203163186\n",
      "Epoch 203:\n",
      "Loss train 0.0067279072478413585 valid 0.006769421634681593\n",
      "Epoch 204:\n",
      "Loss train 0.006719050277024507 valid 0.0067695474830655475\n",
      "Epoch 205:\n",
      "Loss train 0.006721718050539493 valid 0.0067710529865952504\n",
      "Epoch 206:\n",
      "Loss train 0.006719076633453369 valid 0.006774274731888779\n",
      "Epoch 207:\n",
      "Loss train 0.00672124563716352 valid 0.006765865649399289\n",
      "Epoch 208:\n",
      "Loss train 0.00672139241360128 valid 0.006769616131596547\n",
      "Epoch 209:\n",
      "Loss train 0.006720326840877533 valid 0.006780158406485914\n",
      "Epoch 210:\n",
      "Loss train 0.006730004632845521 valid 0.006766184956794149\n",
      "Epoch 211:\n",
      "Loss train 0.006720507144927978 valid 0.006779099387298472\n",
      "Epoch 212:\n",
      "Loss train 0.0067232700530439615 valid 0.006766931770428362\n",
      "Epoch 213:\n",
      "Loss train 0.006718166870996356 valid 0.006774850733014231\n",
      "Epoch 214:\n",
      "Loss train 0.006720255455002188 valid 0.006764774457059953\n",
      "Epoch 215:\n",
      "Loss train 0.006726778764277697 valid 0.006767155863734273\n",
      "Epoch 216:\n",
      "Loss train 0.0067176176235079765 valid 0.00676701283086254\n",
      "Epoch 217:\n",
      "Loss train 0.006719882739707828 valid 0.006766944476807697\n",
      "Epoch 218:\n",
      "Loss train 0.006735922815278172 valid 0.006827925025664793\n",
      "Epoch 219:\n",
      "Loss train 0.00674638319760561 valid 0.006770389660855174\n",
      "Epoch 220:\n",
      "Loss train 0.006735124671831727 valid 0.006771721869826456\n",
      "Epoch 221:\n",
      "Loss train 0.006727050431072712 valid 0.006776947802186369\n",
      "Epoch 222:\n",
      "Loss train 0.0067352756392210725 valid 0.006779445636304224\n",
      "Epoch 223:\n",
      "Loss train 0.006718767294660211 valid 0.006766745181743306\n",
      "Epoch 224:\n",
      "Loss train 0.006719446182250977 valid 0.006809507873253259\n",
      "Epoch 225:\n",
      "Loss train 0.006740271346643567 valid 0.0067666225823850195\n",
      "Epoch 226:\n",
      "Loss train 0.006719980901107192 valid 0.006789005642107193\n",
      "Epoch 227:\n",
      "Loss train 0.006733800563961267 valid 0.006771834735655533\n",
      "Epoch 228:\n",
      "Loss train 0.006724828761070967 valid 0.006768965048505122\n",
      "Epoch 229:\n",
      "Loss train 0.006722375936806202 valid 0.006773937401790758\n",
      "Epoch 230:\n",
      "Loss train 0.006719520129263401 valid 0.006772311985188264\n",
      "Epoch 231:\n",
      "Loss train 0.006721599120646715 valid 0.006776044184301522\n",
      "Epoch 232:\n",
      "Loss train 0.006720934947952628 valid 0.006777981966318951\n",
      "Epoch 233:\n",
      "Loss train 0.006738542672246694 valid 0.006819943967857045\n",
      "Epoch 234:\n",
      "Loss train 0.006733891880139708 valid 0.006790727177343906\n",
      "Epoch 235:\n",
      "Loss train 0.006726043485105038 valid 0.0067698288594103045\n",
      "Epoch 236:\n",
      "Loss train 0.00672831223346293 valid 0.006790637760893473\n",
      "Epoch 237:\n",
      "Loss train 0.006729036197066307 valid 0.006797806541543921\n",
      "Epoch 238:\n",
      "Loss train 0.006730276485905051 valid 0.006771751019984566\n",
      "Epoch 239:\n",
      "Loss train 0.006721372529864311 valid 0.006780064365593296\n",
      "Epoch 240:\n",
      "Loss train 0.006721026916056872 valid 0.0067671474435259515\n",
      "Epoch 241:\n",
      "Loss train 0.006722640246152878 valid 0.0067668769914860346\n",
      "Epoch 242:\n",
      "Loss train 0.006718713091686368 valid 0.006777466144334019\n",
      "Epoch 243:\n",
      "Loss train 0.006722482247278094 valid 0.006789510198711961\n",
      "Epoch 244:\n",
      "Loss train 0.006754835462197661 valid 0.006774590349013974\n",
      "Epoch 245:\n",
      "Loss train 0.006737078214064241 valid 0.006765699931417025\n",
      "Epoch 246:\n",
      "Loss train 0.006719763949513435 valid 0.006765963095789308\n",
      "Epoch 247:\n",
      "Loss train 0.006724016461521387 valid 0.006796909493767465\n",
      "Epoch 248:\n",
      "Loss train 0.006731266621500254 valid 0.006783867941577121\n",
      "Epoch 249:\n",
      "Loss train 0.006742124166339636 valid 0.006778074314662377\n",
      "Epoch 250:\n",
      "Loss train 0.006728926673531532 valid 0.00677923634236809\n",
      "Epoch 251:\n",
      "Loss train 0.0067317764274775985 valid 0.006767742967172646\n",
      "Epoch 252:\n",
      "Loss train 0.0067184618674218655 valid 0.006778240467997048\n",
      "Epoch 253:\n",
      "Loss train 0.006721584731712938 valid 0.006768764736559109\n",
      "Epoch 254:\n",
      "Loss train 0.006724398396909237 valid 0.0067661035887509775\n",
      "Epoch 255:\n",
      "Loss train 0.006716560153290629 valid 0.0067639941378132735\n",
      "Epoch 256:\n",
      "Loss train 0.006719452561810613 valid 0.006767077196373461\n",
      "Epoch 257:\n",
      "Loss train 0.006731667928397656 valid 0.006767320482381803\n",
      "Epoch 258:\n",
      "Loss train 0.006730099581182003 valid 0.006771245682947375\n",
      "Epoch 259:\n",
      "Loss train 0.006726585002616048 valid 0.006773883420362737\n",
      "Epoch 260:\n",
      "Loss train 0.006718732183799148 valid 0.0067679705961550115\n",
      "Epoch 261:\n",
      "Loss train 0.006718666572123766 valid 0.006772611190231078\n",
      "Epoch 262:\n",
      "Loss train 0.006723589170724154 valid 0.0067756207760289525\n",
      "Epoch 263:\n",
      "Loss train 0.006719794496893883 valid 0.006770494006027976\n",
      "Epoch 264:\n",
      "Loss train 0.0067212424706667665 valid 0.006764678146304764\n",
      "Epoch 265:\n",
      "Loss train 0.006716918898746371 valid 0.00677188245864951\n",
      "Epoch 266:\n",
      "Loss train 0.006717539113014937 valid 0.006764265525422559\n",
      "Epoch 267:\n",
      "Loss train 0.006722321501001716 valid 0.0067702789994000066\n",
      "Epoch 268:\n",
      "Loss train 0.006717534409835935 valid 0.006766262162550354\n",
      "Epoch 269:\n",
      "Loss train 0.006720442930236459 valid 0.006777611100004027\n",
      "Epoch 270:\n",
      "Loss train 0.006728533282876014 valid 0.006788958357659939\n",
      "Epoch 271:\n",
      "Loss train 0.006729705259203911 valid 0.006768797896867934\n",
      "Epoch 272:\n",
      "Loss train 0.0067251857835799456 valid 0.006766942556680233\n",
      "Epoch 273:\n",
      "Loss train 0.006725353561341763 valid 0.00677866742819261\n",
      "Epoch 274:\n",
      "Loss train 0.006719258707016706 valid 0.006783990621012453\n",
      "Epoch 275:\n",
      "Loss train 0.006730198906734586 valid 0.006771330974280821\n",
      "Epoch 276:\n",
      "Loss train 0.006721375603228807 valid 0.0067792686190690996\n",
      "Epoch 277:\n",
      "Loss train 0.00671984632499516 valid 0.006767427754510863\n",
      "Epoch 278:\n",
      "Loss train 0.006719754636287689 valid 0.0067683005717354\n",
      "Epoch 279:\n",
      "Loss train 0.006719579547643661 valid 0.006769172511934226\n",
      "Epoch 280:\n",
      "Loss train 0.006730173621326685 valid 0.006794394749150566\n",
      "Epoch 281:\n",
      "Loss train 0.00673982510343194 valid 0.006772742503781461\n",
      "Epoch 282:\n",
      "Loss train 0.006734979385510087 valid 0.006780911051358633\n",
      "Epoch 283:\n",
      "Loss train 0.006719277333468198 valid 0.006767004330091235\n",
      "Epoch 284:\n",
      "Loss train 0.006721373833715916 valid 0.006763966868373219\n",
      "Epoch 285:\n",
      "Loss train 0.0067216069903224705 valid 0.006797289076458131\n",
      "Epoch 286:\n",
      "Loss train 0.0067311468534171585 valid 0.006774403287447394\n",
      "Epoch 287:\n",
      "Loss train 0.006734831724315882 valid 0.006774264301984772\n",
      "Epoch 288:\n",
      "Loss train 0.0067178255412727594 valid 0.0067663962824549995\n",
      "Epoch 289:\n",
      "Loss train 0.006717340275645256 valid 0.006765428111864292\n",
      "Epoch 290:\n",
      "Loss train 0.006717745028436184 valid 0.0067719283229719885\n",
      "Epoch 291:\n",
      "Loss train 0.006728770025074482 valid 0.006785183861507031\n",
      "Epoch 292:\n",
      "Loss train 0.006735381344333291 valid 0.006794797576158116\n",
      "Epoch 293:\n",
      "Loss train 0.0067278995178639885 valid 0.006766547594592063\n",
      "Epoch 294:\n",
      "Loss train 0.006717683700844646 valid 0.006766848576737905\n",
      "Epoch 295:\n",
      "Loss train 0.006721142726019025 valid 0.006782681334871639\n",
      "Epoch 296:\n",
      "Loss train 0.006735652079805731 valid 0.006763467751421696\n",
      "Epoch 297:\n",
      "Loss train 0.006725421734154224 valid 0.006802095725305509\n",
      "Epoch 298:\n",
      "Loss train 0.00673008463345468 valid 0.006777683388286132\n",
      "Epoch 299:\n",
      "Loss train 0.006721892021596431 valid 0.0067788211310911665\n",
      "Epoch 300:\n",
      "Loss train 0.006721225939691067 valid 0.006768770594504192\n",
      "Epoch 301:\n",
      "Loss train 0.006726755481213331 valid 0.006781391176444642\n",
      "Epoch 302:\n",
      "Loss train 0.006728632934391498 valid 0.006773160451592294\n",
      "Epoch 303:\n",
      "Loss train 0.006721677212044597 valid 0.00676641106348972\n",
      "Epoch 304:\n",
      "Loss train 0.006716349860653281 valid 0.006776989210028312\n",
      "Epoch 305:\n",
      "Loss train 0.006720033381134272 valid 0.006769066097741972\n",
      "Epoch 306:\n",
      "Loss train 0.006716714985668659 valid 0.006765355468892163\n",
      "Epoch 307:\n",
      "Loss train 0.0067149588838219644 valid 0.006776954754321433\n",
      "Epoch 308:\n",
      "Loss train 0.006723956018686294 valid 0.006775716413814847\n",
      "Epoch 309:\n",
      "Loss train 0.006721827341243625 valid 0.006764483882045913\n",
      "Epoch 310:\n",
      "Loss train 0.00672147455625236 valid 0.0067698803918309946\n",
      "Epoch 311:\n",
      "Loss train 0.006726983189582825 valid 0.006806200020414072\n",
      "Epoch 312:\n",
      "Loss train 0.006741856876760721 valid 0.006768414804156482\n",
      "Epoch 313:\n",
      "Loss train 0.006715920614078641 valid 0.006767594996068952\n",
      "Epoch 314:\n",
      "Loss train 0.006716601364314556 valid 0.006773433687545679\n",
      "Epoch 315:\n",
      "Loss train 0.006725164782255888 valid 0.0067831038178070005\n",
      "Epoch 316:\n",
      "Loss train 0.006729323277249932 valid 0.006808083602650835\n",
      "Epoch 317:\n",
      "Loss train 0.006727867992594838 valid 0.006775318131621638\n",
      "Epoch 318:\n",
      "Loss train 0.006725235097110272 valid 0.0067795041376368485\n",
      "Epoch 319:\n",
      "Loss train 0.006718838401138782 valid 0.0067705973252220825\n",
      "Epoch 320:\n",
      "Loss train 0.00671798107214272 valid 0.006766040444397314\n",
      "Epoch 321:\n",
      "Loss train 0.006714982492849231 valid 0.0067635829580510905\n",
      "Epoch 322:\n",
      "Loss train 0.006719809025526047 valid 0.006762624988965214\n",
      "Epoch 323:\n",
      "Loss train 0.006730700889602303 valid 0.006791369024589339\n",
      "Epoch 324:\n",
      "Loss train 0.006725890608504414 valid 0.006767218037494374\n",
      "Epoch 325:\n",
      "Loss train 0.006722565880045295 valid 0.0067739762225613445\n",
      "Epoch 326:\n",
      "Loss train 0.006725185504183173 valid 0.006763908183476027\n",
      "Epoch 327:\n",
      "Loss train 0.006720918323844671 valid 0.006782148859598146\n",
      "Epoch 328:\n",
      "Loss train 0.00672878478653729 valid 0.0067785793265189844\n",
      "Epoch 329:\n",
      "Loss train 0.006723212637007236 valid 0.006766475631894552\n",
      "Epoch 330:\n",
      "Loss train 0.006718072900548577 valid 0.006804102495680614\n",
      "Epoch 331:\n",
      "Loss train 0.0067407856695353985 valid 0.00677242212425406\n",
      "Epoch 332:\n",
      "Loss train 0.006714718695729971 valid 0.0067643122437728905\n",
      "Epoch 333:\n",
      "Loss train 0.0067214935552328825 valid 0.0067943629521141444\n",
      "Epoch 334:\n",
      "Loss train 0.006725617730990052 valid 0.006773054260419984\n",
      "Epoch 335:\n",
      "Loss train 0.006723138550296426 valid 0.006782643602353233\n",
      "Epoch 336:\n",
      "Loss train 0.006732649682089686 valid 0.006781147580639579\n",
      "Epoch 337:\n",
      "Loss train 0.006737296190112829 valid 0.006782013740692158\n",
      "Epoch 338:\n",
      "Loss train 0.006725026248022914 valid 0.006789416881738388\n",
      "Epoch 339:\n",
      "Loss train 0.006742685614153743 valid 0.0067802741105326405\n",
      "Epoch 340:\n",
      "Loss train 0.006721661612391472 valid 0.006773434381767582\n",
      "Epoch 341:\n",
      "Loss train 0.0067151592578738924 valid 0.006763542518319964\n",
      "Epoch 342:\n",
      "Loss train 0.0067149893380701545 valid 0.006764579418041947\n",
      "Epoch 343:\n",
      "Loss train 0.006715777842327952 valid 0.0067685782782016864\n",
      "Epoch 344:\n",
      "Loss train 0.00671870382502675 valid 0.006768427196633101\n",
      "Epoch 345:\n",
      "Loss train 0.006729172775521874 valid 0.006801010217316868\n",
      "Epoch 346:\n",
      "Loss train 0.0067250078544020655 valid 0.006776854326572596\n",
      "Epoch 347:\n",
      "Loss train 0.006729535385966301 valid 0.006766457954841108\n",
      "Epoch 348:\n",
      "Loss train 0.00672949030995369 valid 0.006795578540703915\n",
      "Epoch 349:\n",
      "Loss train 0.006724805431440472 valid 0.006768698487163739\n",
      "Epoch 350:\n",
      "Loss train 0.006717001367360353 valid 0.0067632617111876795\n",
      "Epoch 351:\n",
      "Loss train 0.006719151325523853 valid 0.0067664146669907665\n",
      "Epoch 352:\n",
      "Loss train 0.006717878766357899 valid 0.006767521817880974\n",
      "Epoch 353:\n",
      "Loss train 0.006720136804506183 valid 0.006778428453337939\n",
      "Epoch 354:\n",
      "Loss train 0.006727408897131681 valid 0.00678287315286\n",
      "Epoch 355:\n",
      "Loss train 0.006727770157158375 valid 0.006776429948952583\n",
      "Epoch 356:\n",
      "Loss train 0.006719893263652921 valid 0.006771550539139675\n",
      "Epoch 357:\n",
      "Loss train 0.0067214987240731714 valid 0.006773901611659819\n",
      "Epoch 358:\n",
      "Loss train 0.006715963827446103 valid 0.006766232324904111\n",
      "Epoch 359:\n",
      "Loss train 0.006714381324127316 valid 0.006765231124711384\n",
      "Epoch 360:\n",
      "Loss train 0.006729783676564694 valid 0.006804462603673428\n",
      "Epoch 361:\n",
      "Loss train 0.006725551281124354 valid 0.006766646395570504\n",
      "Epoch 362:\n",
      "Loss train 0.006722164247184992 valid 0.00676601364457597\n",
      "Epoch 363:\n",
      "Loss train 0.0067188393790274855 valid 0.006769392664622866\n",
      "Epoch 364:\n",
      "Loss train 0.0067236792296171185 valid 0.006772496616937703\n",
      "Epoch 365:\n",
      "Loss train 0.006721309246495366 valid 0.006767350631084682\n",
      "Epoch 366:\n",
      "Loss train 0.006727631622925401 valid 0.006786106444747502\n",
      "Epoch 367:\n",
      "Loss train 0.006726213172078133 valid 0.006767622134163292\n",
      "Epoch 368:\n",
      "Loss train 0.0067198343575000765 valid 0.006766324958288309\n",
      "Epoch 369:\n",
      "Loss train 0.006722757732495666 valid 0.006769771575105196\n",
      "Epoch 370:\n",
      "Loss train 0.00671651498414576 valid 0.006765115180731905\n",
      "Epoch 371:\n",
      "Loss train 0.0067156117875128984 valid 0.006767009559056532\n",
      "Epoch 372:\n",
      "Loss train 0.006733163632452488 valid 0.006778335170360303\n",
      "Epoch 373:\n",
      "Loss train 0.006716996198520064 valid 0.006772348074903727\n",
      "Epoch 374:\n",
      "Loss train 0.006717103719711304 valid 0.006781136977775561\n",
      "Epoch 375:\n",
      "Loss train 0.006715854210779071 valid 0.006765696899544469\n",
      "Epoch 376:\n",
      "Loss train 0.0067165999207645655 valid 0.006784096233432212\n",
      "Epoch 377:\n",
      "Loss train 0.006730984523892402 valid 0.006773320928026548\n",
      "Epoch 378:\n",
      "Loss train 0.006733329873532057 valid 0.006773148221690097\n",
      "Epoch 379:\n",
      "Loss train 0.006730654509738088 valid 0.006764502669502974\n",
      "Epoch 380:\n",
      "Loss train 0.006724839797243476 valid 0.00676242889079294\n",
      "Epoch 381:\n",
      "Loss train 0.006725095491856336 valid 0.006765301860381081\n",
      "Epoch 382:\n",
      "Loss train 0.0067172385286539795 valid 0.006763074084587053\n",
      "Epoch 383:\n",
      "Loss train 0.00672571798786521 valid 0.006769975341626074\n",
      "Epoch 384:\n",
      "Loss train 0.006730686267837882 valid 0.006839500503571738\n",
      "Epoch 385:\n",
      "Loss train 0.0067451541312038895 valid 0.00682817789581491\n",
      "Epoch 386:\n",
      "Loss train 0.00675899819470942 valid 0.006770568474847841\n",
      "Epoch 387:\n",
      "Loss train 0.006742223212495446 valid 0.006779782504325243\n",
      "Epoch 388:\n",
      "Loss train 0.006726526515558362 valid 0.00676860828939509\n",
      "Epoch 389:\n",
      "Loss train 0.006720919068902731 valid 0.006773761096118537\n",
      "Epoch 390:\n",
      "Loss train 0.006716882763430476 valid 0.006765944978677507\n",
      "Epoch 391:\n",
      "Loss train 0.006714627891778946 valid 0.00676487022269294\n",
      "Epoch 392:\n",
      "Loss train 0.006720323767513037 valid 0.006768960510123045\n",
      "Epoch 393:\n",
      "Loss train 0.006724628712981939 valid 0.006767491399294997\n",
      "Epoch 394:\n",
      "Loss train 0.006716765277087688 valid 0.00676151727647291\n",
      "Epoch 395:\n",
      "Loss train 0.006712027173489332 valid 0.006761211789092738\n",
      "Epoch 396:\n",
      "Loss train 0.006712437374517322 valid 0.006763321096389041\n",
      "Epoch 397:\n",
      "Loss train 0.006713147321715951 valid 0.006766807190244061\n",
      "Epoch 398:\n",
      "Loss train 0.006715823337435723 valid 0.006764128214609905\n",
      "Epoch 399:\n",
      "Loss train 0.006714424584060907 valid 0.0067610223858928\n",
      "Epoch 400:\n",
      "Loss train 0.0067129773553460835 valid 0.00676837940032072\n",
      "Epoch 401:\n",
      "Loss train 0.0067145336419343945 valid 0.006772671501162926\n",
      "Epoch 402:\n",
      "Loss train 0.006715968577191233 valid 0.006767043210655718\n",
      "Epoch 403:\n",
      "Loss train 0.006714570010080934 valid 0.0067658442995193364\n",
      "Epoch 404:\n",
      "Loss train 0.006716335797682405 valid 0.006761841856459677\n",
      "Epoch 405:\n",
      "Loss train 0.006722354050725699 valid 0.006778004323887284\n",
      "Epoch 406:\n",
      "Loss train 0.0067247120197862385 valid 0.0067644483640407855\n",
      "Epoch 407:\n",
      "Loss train 0.006715041166171431 valid 0.0067638761537462625\n",
      "Epoch 408:\n",
      "Loss train 0.0067174391821026806 valid 0.006763430266633317\n",
      "Epoch 409:\n",
      "Loss train 0.006718350667506456 valid 0.006772294400833558\n",
      "Epoch 410:\n",
      "Loss train 0.006722877593711018 valid 0.006769535464283372\n",
      "Epoch 411:\n",
      "Loss train 0.0067188053391873835 valid 0.006771569867185112\n",
      "Epoch 412:\n",
      "Loss train 0.006732193194329739 valid 0.006764074747183156\n",
      "Epoch 413:\n",
      "Loss train 0.006714368239045143 valid 0.0067611068338572075\n",
      "Epoch 414:\n",
      "Loss train 0.006714759115129709 valid 0.006769369939308684\n",
      "Epoch 415:\n",
      "Loss train 0.006715781847015023 valid 0.006771678287678321\n",
      "Epoch 416:\n",
      "Loss train 0.006714440463110804 valid 0.0067620477671946275\n",
      "Epoch 417:\n",
      "Loss train 0.00671471836976707 valid 0.0067664536474895025\n",
      "Epoch 418:\n",
      "Loss train 0.00672147530131042 valid 0.006766285282823122\n",
      "Epoch 419:\n",
      "Loss train 0.00671568401157856 valid 0.006766795617908108\n",
      "Epoch 420:\n",
      "Loss train 0.00672670123167336 valid 0.00679882441979342\n",
      "Epoch 421:\n",
      "Loss train 0.006738077336922288 valid 0.006773055399139424\n",
      "Epoch 422:\n",
      "Loss train 0.00672374302521348 valid 0.006774630372361107\n",
      "Epoch 423:\n",
      "Loss train 0.0067203464452177284 valid 0.006765396680716492\n",
      "Epoch 424:\n",
      "Loss train 0.006715587200596928 valid 0.006762102300377455\n",
      "Epoch 425:\n",
      "Loss train 0.00671375123783946 valid 0.006766029074827987\n",
      "Epoch 426:\n",
      "Loss train 0.006713264994323254 valid 0.00676104640083248\n",
      "Epoch 427:\n",
      "Loss train 0.0067128284834325315 valid 0.0067753781186038285\n",
      "Epoch 428:\n",
      "Loss train 0.00673020831309259 valid 0.006781690683242868\n",
      "Epoch 429:\n",
      "Loss train 0.0067335595842450855 valid 0.006770647500866646\n",
      "Epoch 430:\n",
      "Loss train 0.006728311395272612 valid 0.006762364013653723\n",
      "Epoch 431:\n",
      "Loss train 0.006727882660925388 valid 0.006775407429996509\n",
      "Epoch 432:\n",
      "Loss train 0.006740590184926986 valid 0.006796855751153614\n",
      "Epoch 433:\n",
      "Loss train 0.006730065681040287 valid 0.006782921413872642\n",
      "Epoch 434:\n",
      "Loss train 0.0067280365619808435 valid 0.006763296437110332\n",
      "Epoch 435:\n",
      "Loss train 0.006716642109677195 valid 0.006765001586551112\n",
      "Epoch 436:\n",
      "Loss train 0.006718337675556541 valid 0.006764957110059112\n",
      "Epoch 437:\n",
      "Loss train 0.006718129152432084 valid 0.006761927361651928\n",
      "Epoch 438:\n",
      "Loss train 0.006721773184835911 valid 0.00677904609339214\n",
      "Epoch 439:\n",
      "Loss train 0.0067222054116427895 valid 0.006771414934095537\n",
      "Epoch 440:\n",
      "Loss train 0.006721084238961339 valid 0.0067608695493757325\n",
      "Epoch 441:\n",
      "Loss train 0.006717494875192642 valid 0.006786447826207017\n",
      "Epoch 442:\n",
      "Loss train 0.006731082545593381 valid 0.006763118180632742\n",
      "Epoch 443:\n",
      "Loss train 0.006716310186311603 valid 0.00676361823305867\n",
      "Epoch 444:\n",
      "Loss train 0.006715591764077544 valid 0.00675962890622235\n",
      "Epoch 445:\n",
      "Loss train 0.006714142486453056 valid 0.006763729201414846\n",
      "Epoch 446:\n",
      "Loss train 0.006716315122321248 valid 0.006762760512508896\n",
      "Epoch 447:\n",
      "Loss train 0.006713342806324363 valid 0.006760065104020916\n",
      "Epoch 448:\n",
      "Loss train 0.006717527657747269 valid 0.006777773307879254\n",
      "Epoch 449:\n",
      "Loss train 0.006725589977577329 valid 0.006766671728195828\n",
      "Epoch 450:\n",
      "Loss train 0.0067181071732193235 valid 0.006762656287662246\n",
      "Epoch 451:\n",
      "Loss train 0.0067203827667981384 valid 0.006762736705006095\n",
      "Epoch 452:\n",
      "Loss train 0.006715662078931928 valid 0.006761900111591478\n",
      "Epoch 453:\n",
      "Loss train 0.0067231884691864256 valid 0.006803889129587416\n",
      "Epoch 454:\n",
      "Loss train 0.006722727324813604 valid 0.006759738278260653\n",
      "Epoch 455:\n",
      "Loss train 0.006726833852007985 valid 0.006764368629011362\n",
      "Epoch 456:\n",
      "Loss train 0.0067142007872462274 valid 0.006764021151215091\n",
      "Epoch 457:\n",
      "Loss train 0.006713350350037217 valid 0.00677528691432276\n",
      "Epoch 458:\n",
      "Loss train 0.006730662798509002 valid 0.006772123265194195\n",
      "Epoch 459:\n",
      "Loss train 0.00672162463888526 valid 0.006783805648185143\n",
      "Epoch 460:\n",
      "Loss train 0.006720535503700375 valid 0.00676683008137586\n",
      "Epoch 461:\n",
      "Loss train 0.0067131735384464266 valid 0.006780851973707347\n",
      "Epoch 462:\n",
      "Loss train 0.00672467085532844 valid 0.006762736243725789\n",
      "Epoch 463:\n",
      "Loss train 0.0067220829427242276 valid 0.0067826771559441005\n",
      "Epoch 464:\n",
      "Loss train 0.006718351365998387 valid 0.006772871812163619\n",
      "Epoch 465:\n",
      "Loss train 0.00671646841801703 valid 0.006775879819019183\n",
      "Epoch 466:\n",
      "Loss train 0.006721876421943307 valid 0.006774603512891828\n",
      "Epoch 467:\n",
      "Loss train 0.006713933870196342 valid 0.006762789693043526\n",
      "Epoch 468:\n",
      "Loss train 0.00672368872910738 valid 0.006768010188184825\n",
      "Epoch 469:\n",
      "Loss train 0.006717969290912151 valid 0.006763381950582608\n",
      "Epoch 470:\n",
      "Loss train 0.006715464824810624 valid 0.006767119661884158\n",
      "Epoch 471:\n",
      "Loss train 0.006716610305011273 valid 0.006774953038603283\n",
      "Epoch 472:\n",
      "Loss train 0.006733182305470109 valid 0.0067672344016764035\n",
      "Epoch 473:\n",
      "Loss train 0.00672438875772059 valid 0.006769095033562572\n",
      "Epoch 474:\n",
      "Loss train 0.006715987203642726 valid 0.006761227391428117\n",
      "Epoch 475:\n",
      "Loss train 0.006722233863547445 valid 0.00676803840820939\n",
      "Epoch 476:\n",
      "Loss train 0.006718750903382897 valid 0.006782412914841413\n",
      "Epoch 477:\n",
      "Loss train 0.006729988660663366 valid 0.006764777478736003\n",
      "Epoch 478:\n",
      "Loss train 0.006712637422606349 valid 0.006760485550901605\n",
      "Epoch 479:\n",
      "Loss train 0.006713107414543629 valid 0.006776125265720331\n",
      "Epoch 480:\n",
      "Loss train 0.006722148833796382 valid 0.006760857099546323\n",
      "Epoch 481:\n",
      "Loss train 0.006715224683284759 valid 0.006781857212449348\n",
      "Epoch 482:\n",
      "Loss train 0.006732885539531708 valid 0.006762682441404761\n",
      "Epoch 483:\n",
      "Loss train 0.006733982730656863 valid 0.006787486645237223\n",
      "Epoch 484:\n",
      "Loss train 0.006738312542438507 valid 0.006771545767893855\n",
      "Epoch 485:\n",
      "Loss train 0.006719094840809703 valid 0.006771081600213306\n",
      "Epoch 486:\n",
      "Loss train 0.006736587919294834 valid 0.006773689238778579\n",
      "Epoch 487:\n",
      "Loss train 0.0067251533269882206 valid 0.00678289960317188\n",
      "Epoch 488:\n",
      "Loss train 0.0067244048696011305 valid 0.006779619188063927\n",
      "Epoch 489:\n",
      "Loss train 0.006720854714512825 valid 0.006763738659883836\n",
      "Epoch 490:\n",
      "Loss train 0.006715170480310917 valid 0.006759137952741023\n",
      "Epoch 491:\n",
      "Loss train 0.006711908709257841 valid 0.006761512698590292\n",
      "Epoch 492:\n",
      "Loss train 0.0067111246287822725 valid 0.006763140897758961\n",
      "Epoch 493:\n",
      "Loss train 0.006722244247794151 valid 0.006762752271600611\n",
      "Epoch 494:\n",
      "Loss train 0.006715905293822289 valid 0.006777609429442502\n",
      "Epoch 495:\n",
      "Loss train 0.006718021212145686 valid 0.006764692068146207\n",
      "Epoch 496:\n",
      "Loss train 0.006721759960055351 valid 0.006797524522549633\n",
      "Epoch 497:\n",
      "Loss train 0.006736174831166864 valid 0.006784369581040784\n",
      "Epoch 498:\n",
      "Loss train 0.006753550190478563 valid 0.00675734614845591\n",
      "Epoch 499:\n",
      "Loss train 0.006725537916645407 valid 0.006765886791805583\n",
      "Epoch 500:\n",
      "Loss train 0.006721301004290581 valid 0.006759821433269118\n",
      "Epoch 501:\n",
      "Loss train 0.006715013086795807 valid 0.006767733160748879\n",
      "Epoch 502:\n",
      "Loss train 0.006717377295717597 valid 0.006769595421704907\n",
      "Epoch 503:\n",
      "Loss train 0.006720852851867676 valid 0.006767633384394039\n",
      "Epoch 504:\n",
      "Loss train 0.006712374603375792 valid 0.006762926852733474\n",
      "Epoch 505:\n",
      "Loss train 0.0067123508546501395 valid 0.00676099743598749\n",
      "Epoch 506:\n",
      "Loss train 0.00671157524921 valid 0.0067617616367322405\n",
      "Epoch 507:\n",
      "Loss train 0.00671043349429965 valid 0.00675944000260422\n",
      "Epoch 508:\n",
      "Loss train 0.006716053560376167 valid 0.006765966021722404\n",
      "Epoch 509:\n",
      "Loss train 0.006716958107426762 valid 0.0067678523625297055\n",
      "Epoch 510:\n",
      "Loss train 0.006722543714568019 valid 0.006763682301472617\n",
      "Epoch 511:\n",
      "Loss train 0.006713958643376827 valid 0.006762898257605636\n",
      "Epoch 512:\n",
      "Loss train 0.006712075648829341 valid 0.00676366120431764\n",
      "Epoch 513:\n",
      "Loss train 0.006711569568142295 valid 0.006767972209952134\n",
      "Epoch 514:\n",
      "Loss train 0.006711503397673369 valid 0.006761915977861598\n",
      "Epoch 515:\n",
      "Loss train 0.006714406004175544 valid 0.00676333874549984\n",
      "Epoch 516:\n",
      "Loss train 0.006716148927807808 valid 0.006783151920132992\n",
      "Epoch 517:\n",
      "Loss train 0.006716252723708749 valid 0.0067612438437844044\n",
      "Epoch 518:\n",
      "Loss train 0.006712753186002373 valid 0.006761525832359237\n",
      "Epoch 519:\n",
      "Loss train 0.006715463381260634 valid 0.006759612236224957\n",
      "Epoch 520:\n",
      "Loss train 0.0067121510859578844 valid 0.006766547874125486\n",
      "Epoch 521:\n",
      "Loss train 0.006730841472744942 valid 0.006794970890056518\n",
      "Epoch 522:\n",
      "Loss train 0.006722179614007473 valid 0.006766253098417594\n",
      "Epoch 523:\n",
      "Loss train 0.006725997803732753 valid 0.006792629422559159\n",
      "Epoch 524:\n",
      "Loss train 0.006732231890782714 valid 0.006775322447553464\n",
      "Epoch 525:\n",
      "Loss train 0.006725897220894695 valid 0.006762658425438976\n",
      "Epoch 526:\n",
      "Loss train 0.006717011984437704 valid 0.006758681491890267\n",
      "Epoch 527:\n",
      "Loss train 0.006713219964876771 valid 0.0067600303939760985\n",
      "Epoch 528:\n",
      "Loss train 0.006719259126111865 valid 0.006762320635043829\n",
      "Epoch 529:\n",
      "Loss train 0.0067136961501091715 valid 0.006768712580907502\n",
      "Epoch 530:\n",
      "Loss train 0.0067135653458535675 valid 0.006759087852868722\n",
      "Epoch 531:\n",
      "Loss train 0.006717368774116039 valid 0.006782929962630221\n",
      "Epoch 532:\n",
      "Loss train 0.006728570722043514 valid 0.006760306206506751\n",
      "Epoch 533:\n",
      "Loss train 0.0067191147711127995 valid 0.0067672188920542935\n",
      "Epoch 534:\n",
      "Loss train 0.006715416209772229 valid 0.006759722254696531\n",
      "Epoch 535:\n",
      "Loss train 0.006711323838680982 valid 0.006761966382046952\n",
      "Epoch 536:\n",
      "Loss train 0.006715603685006499 valid 0.006773347529574532\n",
      "Epoch 537:\n",
      "Loss train 0.006719645671546459 valid 0.006762835182211598\n",
      "Epoch 538:\n",
      "Loss train 0.006717545352876187 valid 0.00676192919059538\n",
      "Epoch 539:\n",
      "Loss train 0.006714056478813291 valid 0.006759357102365671\n",
      "Epoch 540:\n",
      "Loss train 0.006715000933036208 valid 0.006769126241905927\n",
      "Epoch 541:\n",
      "Loss train 0.006713645625859499 valid 0.006765684355917589\n",
      "Epoch 542:\n",
      "Loss train 0.006731528649106621 valid 0.006802710352781093\n",
      "Epoch 543:\n",
      "Loss train 0.006728618126362562 valid 0.006760365835877977\n",
      "Epoch 544:\n",
      "Loss train 0.006726553943008184 valid 0.006770549704974293\n",
      "Epoch 545:\n",
      "Loss train 0.0067294308915734295 valid 0.00676226741968522\n",
      "Epoch 546:\n",
      "Loss train 0.006721954606473446 valid 0.006761090753175975\n",
      "Epoch 547:\n",
      "Loss train 0.006711310427635908 valid 0.006776851240629583\n",
      "Epoch 548:\n",
      "Loss train 0.0067146413028240206 valid 0.006762674721011764\n",
      "Epoch 549:\n",
      "Loss train 0.006710861530154944 valid 0.006768599995990433\n",
      "Epoch 550:\n",
      "Loss train 0.006712657306343317 valid 0.0067618867936264145\n",
      "Epoch 551:\n",
      "Loss train 0.006715821987017989 valid 0.006761640771238254\n",
      "Epoch 552:\n",
      "Loss train 0.006717447284609079 valid 0.0067672262488005066\n",
      "Epoch 553:\n",
      "Loss train 0.006729152658954263 valid 0.006784957336341033\n",
      "Epoch 554:\n",
      "Loss train 0.0067228048108518125 valid 0.006770099525698608\n",
      "Epoch 555:\n",
      "Loss train 0.0067124517634511 valid 0.006757897658339027\n",
      "Epoch 556:\n",
      "Loss train 0.006709744920954108 valid 0.0067736200798752715\n",
      "Epoch 557:\n",
      "Loss train 0.006714594503864646 valid 0.006760807714480297\n",
      "Epoch 558:\n",
      "Loss train 0.006712193181738257 valid 0.006761932624836312\n",
      "Epoch 559:\n",
      "Loss train 0.006711126770824194 valid 0.006762113546384301\n",
      "Epoch 560:\n",
      "Loss train 0.00671150591224432 valid 0.006764579543509004\n",
      "Epoch 561:\n",
      "Loss train 0.0067175788339227435 valid 0.006757222410188768\n",
      "Epoch 562:\n",
      "Loss train 0.006716311210766435 valid 0.006782425405265512\n",
      "Epoch 563:\n",
      "Loss train 0.00672949873842299 valid 0.006763244113076343\n",
      "Epoch 564:\n",
      "Loss train 0.006712248874828219 valid 0.006765658390425956\n",
      "Epoch 565:\n",
      "Loss train 0.006723284488543868 valid 0.006775271601399458\n",
      "Epoch 566:\n",
      "Loss train 0.006726989196613431 valid 0.006775190910030581\n",
      "Epoch 567:\n",
      "Loss train 0.006726311612874269 valid 0.006788195508468689\n",
      "Epoch 568:\n",
      "Loss train 0.006720950547605753 valid 0.006766022187220234\n",
      "Epoch 569:\n",
      "Loss train 0.006714599113911391 valid 0.006762577151637105\n",
      "Epoch 570:\n",
      "Loss train 0.006725396448746324 valid 0.006800375638201505\n",
      "Epoch 571:\n",
      "Loss train 0.006719508627429605 valid 0.00675971722657525\n",
      "Epoch 572:\n",
      "Loss train 0.00671043349429965 valid 0.00676803997772941\n",
      "Epoch 573:\n",
      "Loss train 0.006714905705302953 valid 0.006756782621780973\n",
      "Epoch 574:\n",
      "Loss train 0.006709660729393363 valid 0.0067639387176260946\n",
      "Epoch 575:\n",
      "Loss train 0.006710999831557274 valid 0.006756923281413683\n",
      "Epoch 576:\n",
      "Loss train 0.006712958449497819 valid 0.006765130409380155\n",
      "Epoch 577:\n",
      "Loss train 0.006712037371471524 valid 0.006765933383566639\n",
      "Epoch 578:\n",
      "Loss train 0.006718128174543381 valid 0.006770244407956083\n",
      "Epoch 579:\n",
      "Loss train 0.006719696894288063 valid 0.006765889524621039\n",
      "Epoch 580:\n",
      "Loss train 0.006733933556824922 valid 0.006800603254978096\n",
      "Epoch 581:\n",
      "Loss train 0.0067291093524545435 valid 0.00676531730352531\n",
      "Epoch 582:\n",
      "Loss train 0.006722742598503828 valid 0.006775630079942193\n",
      "Epoch 583:\n",
      "Loss train 0.006711412454023957 valid 0.006755842984141527\n",
      "Epoch 584:\n",
      "Loss train 0.006709179934114217 valid 0.006760643654563031\n",
      "Epoch 585:\n",
      "Loss train 0.006711890641599894 valid 0.00676254546658739\n",
      "Epoch 586:\n",
      "Loss train 0.0067163239233195785 valid 0.0067675039087427445\n",
      "Epoch 587:\n",
      "Loss train 0.006732177827507258 valid 0.00676821975821623\n",
      "Epoch 588:\n",
      "Loss train 0.006724721100181341 valid 0.006761001020212214\n",
      "Epoch 589:\n",
      "Loss train 0.00672691878862679 valid 0.0068205152045446825\n",
      "Epoch 590:\n",
      "Loss train 0.006736734649166465 valid 0.006769940190073788\n",
      "Epoch 591:\n",
      "Loss train 0.006717418087646365 valid 0.006759216048153638\n",
      "Epoch 592:\n",
      "Loss train 0.006719250790774822 valid 0.006795452360873114\n",
      "Epoch 593:\n",
      "Loss train 0.006728534493595362 valid 0.006762055901197836\n",
      "Epoch 594:\n",
      "Loss train 0.006720025185495615 valid 0.006775444322992151\n",
      "Epoch 595:\n",
      "Loss train 0.006712729064747691 valid 0.0067564376866912365\n",
      "Epoch 596:\n",
      "Loss train 0.0067103102337569 valid 0.006760940225975136\n",
      "Epoch 597:\n",
      "Loss train 0.006707842368632555 valid 0.00675570338466026\n",
      "Epoch 598:\n",
      "Loss train 0.006720539089292288 valid 0.0067897575784119125\n",
      "Epoch 599:\n",
      "Loss train 0.006720413360744715 valid 0.006762827125042442\n",
      "Epoch 600:\n",
      "Loss train 0.006726062390953302 valid 0.00676808087126631\n",
      "Epoch 601:\n",
      "Loss train 0.006714517716318369 valid 0.006765863926273236\n",
      "Epoch 602:\n",
      "Loss train 0.006709997495636344 valid 0.006755771527798961\n",
      "Epoch 603:\n",
      "Loss train 0.006709383009001613 valid 0.006759080887284064\n",
      "Epoch 604:\n",
      "Loss train 0.00670919856056571 valid 0.006758320276089547\n",
      "Epoch 605:\n",
      "Loss train 0.006709836795926094 valid 0.006762897331669154\n",
      "Epoch 606:\n",
      "Loss train 0.0067149218637496235 valid 0.0067624244691817\n",
      "Epoch 607:\n",
      "Loss train 0.006717089284211397 valid 0.006775563194264613\n",
      "Epoch 608:\n",
      "Loss train 0.00671605309471488 valid 0.006766563815677009\n",
      "Epoch 609:\n",
      "Loss train 0.006726474454626441 valid 0.0067855964662157835\n",
      "Epoch 610:\n",
      "Loss train 0.0067235709633678194 valid 0.006775284962104444\n",
      "Epoch 611:\n",
      "Loss train 0.006717656878754497 valid 0.006764639905064702\n",
      "Epoch 612:\n",
      "Loss train 0.006709043681621551 valid 0.006758826369870425\n",
      "Epoch 613:\n",
      "Loss train 0.006715538492426276 valid 0.006792104735839733\n",
      "Epoch 614:\n",
      "Loss train 0.00672231768257916 valid 0.006771875784632393\n",
      "Epoch 615:\n",
      "Loss train 0.006725784670561552 valid 0.006761821273020935\n",
      "Epoch 616:\n",
      "Loss train 0.0067351113073527815 valid 0.006771238895510585\n",
      "Epoch 617:\n",
      "Loss train 0.006727268174290657 valid 0.006768832680661249\n",
      "Epoch 618:\n",
      "Loss train 0.006716266181319952 valid 0.006768880566904488\n",
      "Epoch 619:\n",
      "Loss train 0.006716668326407671 valid 0.006771969797539659\n",
      "Epoch 620:\n",
      "Loss train 0.00671860221773386 valid 0.006759093861361189\n",
      "Epoch 621:\n",
      "Loss train 0.006715437537059188 valid 0.006757595237118783\n",
      "Epoch 622:\n",
      "Loss train 0.006716210674494505 valid 0.006763110050083321\n",
      "Epoch 623:\n",
      "Loss train 0.006720643490552902 valid 0.006757680353593592\n",
      "Epoch 624:\n",
      "Loss train 0.006710074888542294 valid 0.006759635402807945\n",
      "Epoch 625:\n",
      "Loss train 0.006716420175507665 valid 0.006767346339534195\n",
      "Epoch 626:\n",
      "Loss train 0.006716364994645119 valid 0.006760088663595303\n",
      "Epoch 627:\n",
      "Loss train 0.006720027467235923 valid 0.00680760384485699\n",
      "Epoch 628:\n",
      "Loss train 0.006721367873251438 valid 0.006758700553351902\n",
      "Epoch 629:\n",
      "Loss train 0.0067166043911129234 valid 0.006776137398155041\n",
      "Epoch 630:\n",
      "Loss train 0.006716999644413591 valid 0.006773744026656942\n",
      "Epoch 631:\n",
      "Loss train 0.006717763002961874 valid 0.006758645991599682\n",
      "Epoch 632:\n",
      "Loss train 0.006710152234882116 valid 0.006759241598409697\n",
      "Epoch 633:\n",
      "Loss train 0.006723101856186986 valid 0.006785633167861225\n",
      "Epoch 634:\n",
      "Loss train 0.006733494298532605 valid 0.006788071194072665\n",
      "Epoch 635:\n",
      "Loss train 0.006719439337030053 valid 0.006765802830942548\n",
      "Epoch 636:\n",
      "Loss train 0.0067114535253494974 valid 0.006757927743010353\n",
      "Epoch 637:\n",
      "Loss train 0.006712536886334419 valid 0.006765908561544947\n",
      "Epoch 638:\n",
      "Loss train 0.006710880016908049 valid 0.006759147474939005\n",
      "Epoch 639:\n",
      "Loss train 0.006710809469223022 valid 0.0067614807528416435\n",
      "Epoch 640:\n",
      "Loss train 0.006713447812944651 valid 0.006761989915091963\n",
      "Epoch 641:\n",
      "Loss train 0.006714584911242128 valid 0.006757936949898547\n",
      "Epoch 642:\n",
      "Loss train 0.006731482222676277 valid 0.006801226216449155\n",
      "Epoch 643:\n",
      "Loss train 0.0067369922995567325 valid 0.006810632527293674\n",
      "Epoch 644:\n",
      "Loss train 0.0067216843832284216 valid 0.006761325800086882\n",
      "Epoch 645:\n",
      "Loss train 0.006710870051756501 valid 0.006759625193173879\n",
      "Epoch 646:\n",
      "Loss train 0.006712047616019845 valid 0.006757904156995014\n",
      "Epoch 647:\n",
      "Loss train 0.006710090581327677 valid 0.006774564163063738\n",
      "Epoch 648:\n",
      "Loss train 0.006713390024378896 valid 0.006761594255851952\n",
      "Epoch 649:\n",
      "Loss train 0.006709376070648432 valid 0.006759221115369203\n",
      "Epoch 650:\n",
      "Loss train 0.006723542604595423 valid 0.006773085738238969\n",
      "Epoch 651:\n",
      "Loss train 0.006725711608305573 valid 0.006811357129690337\n",
      "Epoch 652:\n",
      "Loss train 0.006725012324750423 valid 0.006779164303192271\n",
      "Epoch 653:\n",
      "Loss train 0.006715604662895202 valid 0.006757454076505343\n",
      "Epoch 654:\n",
      "Loss train 0.006716832052916288 valid 0.006757087717015287\n",
      "Epoch 655:\n",
      "Loss train 0.006713194400072097 valid 0.006756685273120615\n",
      "Epoch 656:\n",
      "Loss train 0.006722026318311691 valid 0.006772329595149727\n",
      "Epoch 657:\n",
      "Loss train 0.0067206971347332 valid 0.006763296912663872\n",
      "Epoch 658:\n",
      "Loss train 0.006713170139119029 valid 0.006759800688558185\n",
      "Epoch 659:\n",
      "Loss train 0.00671014622785151 valid 0.006767766938675163\n",
      "Epoch 660:\n",
      "Loss train 0.006712285289540887 valid 0.00676679111953191\n",
      "Epoch 661:\n",
      "Loss train 0.006714770337566733 valid 0.006763924292020661\n",
      "Epoch 662:\n",
      "Loss train 0.0067086209077388045 valid 0.006756707534434359\n",
      "Epoch 663:\n",
      "Loss train 0.0067113492172211405 valid 0.0067629856452177985\n",
      "Epoch 664:\n",
      "Loss train 0.006710905302315951 valid 0.006762130291297342\n",
      "Epoch 665:\n",
      "Loss train 0.006713834125548601 valid 0.006768659528911428\n",
      "Epoch 666:\n",
      "Loss train 0.0067131076473742725 valid 0.006757032669785921\n",
      "Epoch 667:\n",
      "Loss train 0.006708439951762557 valid 0.006757847142610159\n",
      "Epoch 668:\n",
      "Loss train 0.00670760846696794 valid 0.006757161660047314\n",
      "Epoch 669:\n",
      "Loss train 0.006708655459806323 valid 0.0067580859504287796\n",
      "Epoch 670:\n",
      "Loss train 0.006716235168278217 valid 0.006766051466994509\n",
      "Epoch 671:\n",
      "Loss train 0.006713459081947803 valid 0.006756391373676273\n",
      "Epoch 672:\n",
      "Loss train 0.0067106957081705335 valid 0.0067552221528808295\n",
      "Epoch 673:\n",
      "Loss train 0.006714158365502954 valid 0.006763279596839011\n",
      "Epoch 674:\n",
      "Loss train 0.006713856477290392 valid 0.006773100909289674\n",
      "Epoch 675:\n",
      "Loss train 0.006725917989388108 valid 0.00677050601602196\n",
      "Epoch 676:\n",
      "Loss train 0.0067217824049294 valid 0.006767656890341756\n",
      "Epoch 677:\n",
      "Loss train 0.006726583931595087 valid 0.006758646898608796\n",
      "Epoch 678:\n",
      "Loss train 0.006712367525324225 valid 0.006755791236501372\n",
      "Epoch 679:\n",
      "Loss train 0.006713725486770272 valid 0.006783504500417691\n",
      "Epoch 680:\n",
      "Loss train 0.006714475527405739 valid 0.006761020974051059\n",
      "Epoch 681:\n",
      "Loss train 0.006710121920332312 valid 0.006756395139870854\n",
      "Epoch 682:\n",
      "Loss train 0.00670753107406199 valid 0.00675429035388275\n",
      "Epoch 683:\n",
      "Loss train 0.006707189884036779 valid 0.006764992342587214\n",
      "Epoch 684:\n",
      "Loss train 0.006710167601704597 valid 0.006756588540905608\n",
      "Epoch 685:\n",
      "Loss train 0.006716000661253929 valid 0.0067567796365421255\n",
      "Epoch 686:\n",
      "Loss train 0.0067166212480515245 valid 0.006761217511130248\n",
      "Epoch 687:\n",
      "Loss train 0.006708660861477256 valid 0.006779340279274929\n",
      "Epoch 688:\n",
      "Loss train 0.006728434795513749 valid 0.006758442604955355\n",
      "Epoch 689:\n",
      "Loss train 0.006734346924349666 valid 0.006805410155499044\n",
      "Epoch 690:\n",
      "Loss train 0.006725138984620571 valid 0.006758669568611052\n",
      "Epoch 691:\n",
      "Loss train 0.006707025645300746 valid 0.006756859195347417\n",
      "Epoch 692:\n",
      "Loss train 0.006711281975731254 valid 0.00676429578513492\n",
      "Epoch 693:\n",
      "Loss train 0.006710753031075001 valid 0.006760579832823402\n",
      "Epoch 694:\n",
      "Loss train 0.006715497002005577 valid 0.006769300845658561\n",
      "Epoch 695:\n",
      "Loss train 0.006712378794327378 valid 0.0067572260962744475\n",
      "Epoch 696:\n",
      "Loss train 0.006709207082167268 valid 0.006772881968699648\n",
      "Epoch 697:\n",
      "Loss train 0.006712581403553486 valid 0.006755679419070762\n",
      "Epoch 698:\n",
      "Loss train 0.006716044247150421 valid 0.006787119628057087\n",
      "Epoch 699:\n",
      "Loss train 0.006736798444762826 valid 0.006765639415116956\n",
      "Epoch 700:\n",
      "Loss train 0.006744445580989122 valid 0.006763370070390099\n",
      "Epoch 701:\n",
      "Loss train 0.006734526390209794 valid 0.006773560726750981\n",
      "Epoch 702:\n",
      "Loss train 0.006719064293429255 valid 0.00676362296772515\n",
      "Epoch 703:\n",
      "Loss train 0.006716150743886829 valid 0.006770743854151518\n",
      "Epoch 704:\n",
      "Loss train 0.006714989431202412 valid 0.006755800751753999\n",
      "Epoch 705:\n",
      "Loss train 0.006706701777875424 valid 0.006759035273464237\n",
      "Epoch 706:\n",
      "Loss train 0.00671135657466948 valid 0.006761812766978444\n",
      "Epoch 707:\n",
      "Loss train 0.006709874048829079 valid 0.006770373496545451\n",
      "Epoch 708:\n",
      "Loss train 0.006719605531543493 valid 0.006780387708721224\n",
      "Epoch 709:\n",
      "Loss train 0.00671540847979486 valid 0.006766286953212596\n",
      "Epoch 710:\n",
      "Loss train 0.006713324412703514 valid 0.006754088991362707\n",
      "Epoch 711:\n",
      "Loss train 0.006706804130226373 valid 0.006754198485960737\n",
      "Epoch 712:\n",
      "Loss train 0.006710220174863934 valid 0.006772627221333098\n",
      "Epoch 713:\n",
      "Loss train 0.0067281803581863645 valid 0.0067686552397912166\n",
      "Epoch 714:\n",
      "Loss train 0.006733187939971685 valid 0.006759049157129873\n",
      "Epoch 715:\n",
      "Loss train 0.006720291078090667 valid 0.006784076718224804\n",
      "Epoch 716:\n",
      "Loss train 0.0067167406436055895 valid 0.006765462521481177\n",
      "Epoch 717:\n",
      "Loss train 0.0067128957714885475 valid 0.006756667288065064\n",
      "Epoch 718:\n",
      "Loss train 0.006707225507125258 valid 0.006758975585489397\n",
      "Epoch 719:\n",
      "Loss train 0.006705811293795705 valid 0.006755829299055582\n",
      "Epoch 720:\n",
      "Loss train 0.006706560263410211 valid 0.006754347119403727\n",
      "Epoch 721:\n",
      "Loss train 0.006707597756758332 valid 0.006767575996920269\n",
      "Epoch 722:\n",
      "Loss train 0.006713160406798124 valid 0.006759026110662226\n",
      "Epoch 723:\n",
      "Loss train 0.006705676252022385 valid 0.006761692106217609\n",
      "Epoch 724:\n",
      "Loss train 0.00670702331699431 valid 0.006753730124468757\n",
      "Epoch 725:\n",
      "Loss train 0.006703763315454125 valid 0.006753358245787382\n",
      "Epoch 726:\n",
      "Loss train 0.006705014081671834 valid 0.0067609182535303255\n",
      "Epoch 727:\n",
      "Loss train 0.006712765991687774 valid 0.0067685365247098506\n",
      "Epoch 728:\n",
      "Loss train 0.006712769810110331 valid 0.006775710665864129\n",
      "Epoch 729:\n",
      "Loss train 0.0067298456095159056 valid 0.0067558015775275706\n",
      "Epoch 730:\n",
      "Loss train 0.006709002610296011 valid 0.006752832181179614\n",
      "Epoch 731:\n",
      "Loss train 0.006704986607655883 valid 0.00675546903640946\n",
      "Epoch 732:\n",
      "Loss train 0.006708498485386372 valid 0.006776047767372753\n",
      "Epoch 733:\n",
      "Loss train 0.006711401138454675 valid 0.0067718224384302065\n",
      "Epoch 734:\n",
      "Loss train 0.006710609141737223 valid 0.006755266335626342\n",
      "Epoch 735:\n",
      "Loss train 0.006709605176001787 valid 0.006754004047201056\n",
      "Epoch 736:\n",
      "Loss train 0.006708157062530517 valid 0.006755818283027888\n",
      "Epoch 737:\n",
      "Loss train 0.006706549832597375 valid 0.006753813249162058\n",
      "Epoch 738:\n",
      "Loss train 0.006710715126246214 valid 0.0067572924581364505\n",
      "Epoch 739:\n",
      "Loss train 0.006716636335477233 valid 0.006776122629817951\n",
      "Epoch 740:\n",
      "Loss train 0.006726045068353415 valid 0.006754250302168881\n",
      "Epoch 741:\n",
      "Loss train 0.006716297660022974 valid 0.006779497766417707\n",
      "Epoch 742:\n",
      "Loss train 0.006712173437699675 valid 0.006754278683807843\n",
      "Epoch 743:\n",
      "Loss train 0.00670647150836885 valid 0.006753723157049586\n",
      "Epoch 744:\n",
      "Loss train 0.006712123891338706 valid 0.006752605835538782\n",
      "Epoch 745:\n",
      "Loss train 0.0067084401845932005 valid 0.006767284343836907\n",
      "Epoch 746:\n",
      "Loss train 0.006709186732769013 valid 0.006757008946245403\n",
      "Epoch 747:\n",
      "Loss train 0.0067087824922055 valid 0.006754916089689013\n",
      "Epoch 748:\n",
      "Loss train 0.0067086461000144485 valid 0.006763782633860088\n",
      "Epoch 749:\n",
      "Loss train 0.006715008290484548 valid 0.006762174772394494\n",
      "Epoch 750:\n",
      "Loss train 0.0067078159656375645 valid 0.00676248979113241\n",
      "Epoch 751:\n",
      "Loss train 0.0067127496935427185 valid 0.006762526139272953\n",
      "Epoch 752:\n",
      "Loss train 0.006719811400398612 valid 0.006780177335500433\n",
      "Epoch 753:\n",
      "Loss train 0.006713771028444171 valid 0.006769964826094134\n",
      "Epoch 754:\n",
      "Loss train 0.00672316811978817 valid 0.006771915144703455\n",
      "Epoch 755:\n",
      "Loss train 0.006719389976933598 valid 0.006774466017798819\n",
      "Epoch 756:\n",
      "Loss train 0.006710218312218785 valid 0.006756891380442484\n",
      "Epoch 757:\n",
      "Loss train 0.00670569078065455 valid 0.0067619429243426015\n",
      "Epoch 758:\n",
      "Loss train 0.006712940242141485 valid 0.006758819956677955\n",
      "Epoch 759:\n",
      "Loss train 0.006709803035482765 valid 0.006785763208680257\n",
      "Epoch 760:\n",
      "Loss train 0.006724034715443849 valid 0.006770276288330277\n",
      "Epoch 761:\n",
      "Loss train 0.006711725704371929 valid 0.00676628216405694\n",
      "Epoch 762:\n",
      "Loss train 0.0067157675977796314 valid 0.006764432355477055\n",
      "Epoch 763:\n",
      "Loss train 0.006750197894871235 valid 0.006759802835864453\n",
      "Epoch 764:\n",
      "Loss train 0.006729790288954973 valid 0.006773652586790347\n",
      "Epoch 765:\n",
      "Loss train 0.006713921204209328 valid 0.006760713150490355\n",
      "Epoch 766:\n",
      "Loss train 0.006717357272282243 valid 0.0067614728018405785\n",
      "Epoch 767:\n",
      "Loss train 0.006706475093960762 valid 0.006754942235762007\n",
      "Epoch 768:\n",
      "Loss train 0.006709864642471075 valid 0.0067722832076957985\n",
      "Epoch 769:\n",
      "Loss train 0.006708040926605463 valid 0.006758950794467674\n",
      "Epoch 770:\n",
      "Loss train 0.006709670322015882 valid 0.006759160107109941\n",
      "Epoch 771:\n",
      "Loss train 0.0067059805151075125 valid 0.006751505684083809\n",
      "Epoch 772:\n",
      "Loss train 0.006720481393858791 valid 0.006797421942840569\n",
      "Epoch 773:\n",
      "Loss train 0.0067193212918937205 valid 0.006756830673024071\n",
      "Epoch 774:\n",
      "Loss train 0.006725290697067976 valid 0.0067999730308717955\n",
      "Epoch 775:\n",
      "Loss train 0.006714382953941822 valid 0.006771775870630853\n",
      "Epoch 776:\n",
      "Loss train 0.006709366850554943 valid 0.00675712729322634\n",
      "Epoch 777:\n",
      "Loss train 0.006707096425816417 valid 0.0067593622483358325\n",
      "Epoch 778:\n",
      "Loss train 0.006705480581149459 valid 0.006753151903939842\n",
      "Epoch 779:\n",
      "Loss train 0.006710322294384241 valid 0.006760919488847552\n",
      "Epoch 780:\n",
      "Loss train 0.006708176201209426 valid 0.006758845436237967\n",
      "Epoch 781:\n",
      "Loss train 0.006707159569486976 valid 0.006761664890569729\n",
      "Epoch 782:\n",
      "Loss train 0.006708538578823209 valid 0.006758479035328937\n",
      "Epoch 783:\n",
      "Loss train 0.006713538710027933 valid 0.006788187598618522\n",
      "Epoch 784:\n",
      "Loss train 0.006723633734509349 valid 0.0067595147627116554\n",
      "Epoch 785:\n",
      "Loss train 0.006712620565667749 valid 0.006755080172442263\n",
      "Epoch 786:\n",
      "Loss train 0.006707269325852394 valid 0.006755431566394327\n",
      "Epoch 787:\n",
      "Loss train 0.006714200461283326 valid 0.006797382826226026\n",
      "Epoch 788:\n",
      "Loss train 0.006720094289630651 valid 0.006762098479392136\n",
      "Epoch 789:\n",
      "Loss train 0.006707381969317794 valid 0.006755263374080774\n",
      "Epoch 790:\n",
      "Loss train 0.00671606594696641 valid 0.0067620963764964724\n",
      "Epoch 791:\n",
      "Loss train 0.006708899512887001 valid 0.006778971817796476\n",
      "Epoch 792:\n",
      "Loss train 0.006717751966789365 valid 0.006752100343048215\n",
      "Epoch 793:\n",
      "Loss train 0.006722393725067377 valid 0.006776655950956616\n",
      "Epoch 794:\n",
      "Loss train 0.006732835853472352 valid 0.006809630912659559\n",
      "Epoch 795:\n",
      "Loss train 0.006745505053550005 valid 0.0067552879321745914\n",
      "Epoch 796:\n",
      "Loss train 0.006713025318458676 valid 0.006752701096605438\n",
      "Epoch 797:\n",
      "Loss train 0.006707013072445989 valid 0.006751012311640586\n",
      "Epoch 798:\n",
      "Loss train 0.006709083402529359 valid 0.006758051115280537\n",
      "Epoch 799:\n",
      "Loss train 0.006709029758349061 valid 0.006753091343028102\n",
      "Epoch 800:\n",
      "Loss train 0.006711533665657044 valid 0.006769220186513813\n",
      "Epoch 801:\n",
      "Loss train 0.006707673473283648 valid 0.006770010935439679\n",
      "Epoch 802:\n",
      "Loss train 0.0067309562116861345 valid 0.006757598513322154\n",
      "Epoch 803:\n",
      "Loss train 0.00672114915214479 valid 0.006794059545007066\n",
      "Epoch 804:\n",
      "Loss train 0.00671517513692379 valid 0.0067554658799144855\n",
      "Epoch 805:\n",
      "Loss train 0.006706634489819408 valid 0.00675781624172852\n",
      "Epoch 806:\n",
      "Loss train 0.006706344708800316 valid 0.00676406590352177\n",
      "Epoch 807:\n",
      "Loss train 0.006709320191293955 valid 0.006760517340685182\n",
      "Epoch 808:\n",
      "Loss train 0.006708445679396391 valid 0.006762066746362004\n",
      "Epoch 809:\n",
      "Loss train 0.006720840698108077 valid 0.0067770075588890555\n",
      "Epoch 810:\n",
      "Loss train 0.006712429597973824 valid 0.006772512381364937\n",
      "Epoch 811:\n",
      "Loss train 0.006719657545909285 valid 0.0067584160207581935\n",
      "Epoch 812:\n",
      "Loss train 0.006715427804738283 valid 0.0067609546829332234\n",
      "Epoch 813:\n",
      "Loss train 0.006712730694562197 valid 0.00677238362893167\n",
      "Epoch 814:\n",
      "Loss train 0.00671495646238327 valid 0.006752480744006985\n",
      "Epoch 815:\n",
      "Loss train 0.006721882103011012 valid 0.006772586398155888\n",
      "Epoch 816:\n",
      "Loss train 0.006710382737219333 valid 0.006769753775637802\n",
      "Epoch 817:\n",
      "Loss train 0.006707179406657815 valid 0.006751294651633367\n",
      "Epoch 818:\n",
      "Loss train 0.006711739813908935 valid 0.00682088086441039\n",
      "Epoch 819:\n",
      "Loss train 0.006720914924517274 valid 0.006755958705806996\n",
      "Epoch 820:\n",
      "Loss train 0.006708549009636045 valid 0.006751942082576759\n",
      "Epoch 821:\n",
      "Loss train 0.006707465322688222 valid 0.006780519079812797\n",
      "Epoch 822:\n",
      "Loss train 0.006724919797852636 valid 0.0067574894693080345\n",
      "Epoch 823:\n",
      "Loss train 0.006713637057691813 valid 0.006754753057294232\n",
      "Epoch 824:\n",
      "Loss train 0.006711245514452457 valid 0.006759580317224185\n",
      "Epoch 825:\n",
      "Loss train 0.006708887312561274 valid 0.006755939509992744\n",
      "Epoch 826:\n",
      "Loss train 0.0067028900608420375 valid 0.006754052743937414\n",
      "Epoch 827:\n",
      "Loss train 0.006703665386885405 valid 0.006756370485295979\n",
      "Epoch 828:\n",
      "Loss train 0.006704578828066588 valid 0.0067611736176280916\n",
      "Epoch 829:\n",
      "Loss train 0.006708033848553896 valid 0.006760233750079091\n",
      "Epoch 830:\n",
      "Loss train 0.006707607069984078 valid 0.006768298111961813\n",
      "Epoch 831:\n",
      "Loss train 0.00670851762406528 valid 0.006753932163101011\n",
      "Epoch 832:\n",
      "Loss train 0.006704092910513282 valid 0.0067555788356617845\n",
      "Epoch 833:\n",
      "Loss train 0.0067052277736365795 valid 0.006750893357352975\n",
      "Epoch 834:\n",
      "Loss train 0.006705144187435508 valid 0.006750848539697122\n",
      "Epoch 835:\n",
      "Loss train 0.0067056408151984215 valid 0.006756421258317507\n",
      "Epoch 836:\n",
      "Loss train 0.006717150146141648 valid 0.00679430862480669\n",
      "Epoch 837:\n",
      "Loss train 0.006724755000323057 valid 0.006765894518116143\n",
      "Epoch 838:\n",
      "Loss train 0.006707414053380489 valid 0.006769102162252536\n",
      "Epoch 839:\n",
      "Loss train 0.006716516148298979 valid 0.006754192950098087\n",
      "Epoch 840:\n",
      "Loss train 0.006708798371255398 valid 0.006753151483017331\n",
      "Epoch 841:\n",
      "Loss train 0.006704499432817102 valid 0.006763832164830412\n",
      "Epoch 842:\n",
      "Loss train 0.006709253136068583 valid 0.0067643143420617\n",
      "Epoch 843:\n",
      "Loss train 0.006717870011925697 valid 0.006754787947079777\n",
      "Epoch 844:\n",
      "Loss train 0.006706727156415582 valid 0.006777018115329182\n",
      "Epoch 845:\n",
      "Loss train 0.006728780642151833 valid 0.006753579307911985\n",
      "Epoch 846:\n",
      "Loss train 0.006726424768567085 valid 0.006784884916462926\n",
      "Epoch 847:\n",
      "Loss train 0.006712831696495414 valid 0.006759293708484452\n",
      "Epoch 848:\n",
      "Loss train 0.00670507294125855 valid 0.006760398091448143\n",
      "Epoch 849:\n",
      "Loss train 0.006709794467315078 valid 0.006783883660203477\n",
      "Epoch 850:\n",
      "Loss train 0.006713056471198797 valid 0.006753734393591454\n",
      "Epoch 851:\n",
      "Loss train 0.006709747761487961 valid 0.006764589705778499\n",
      "Epoch 852:\n",
      "Loss train 0.006711025210097432 valid 0.006774755061380109\n",
      "Epoch 853:\n",
      "Loss train 0.0067218535579741 valid 0.00678018795396598\n",
      "Epoch 854:\n",
      "Loss train 0.006717897718772292 valid 0.006774563363151742\n",
      "Epoch 855:\n",
      "Loss train 0.006733202887699008 valid 0.006750839364650822\n",
      "Epoch 856:\n",
      "Loss train 0.0067141834180802105 valid 0.006758732361329221\n",
      "Epoch 857:\n",
      "Loss train 0.00670811883173883 valid 0.006753509523626544\n",
      "Epoch 858:\n",
      "Loss train 0.0067077621817588804 valid 0.006755508275378463\n",
      "Epoch 859:\n",
      "Loss train 0.006708937510848045 valid 0.00675813670702648\n",
      "Epoch 860:\n",
      "Loss train 0.006704630004242063 valid 0.006751900193114959\n",
      "Epoch 861:\n",
      "Loss train 0.006703322008252144 valid 0.006758834667828258\n",
      "Epoch 862:\n",
      "Loss train 0.006707589467987418 valid 0.006764147883623992\n",
      "Epoch 863:\n",
      "Loss train 0.006713708909228444 valid 0.006761183163352203\n",
      "Epoch 864:\n",
      "Loss train 0.00670388424769044 valid 0.006760377942682013\n",
      "Epoch 865:\n",
      "Loss train 0.0067151368129998446 valid 0.006753997522964955\n",
      "Epoch 866:\n",
      "Loss train 0.006710934033617377 valid 0.006758078636583025\n",
      "Epoch 867:\n",
      "Loss train 0.006705172220245003 valid 0.006761310443435885\n",
      "Epoch 868:\n",
      "Loss train 0.006714404793456197 valid 0.00675678302129404\n",
      "Epoch 869:\n",
      "Loss train 0.006708533084020018 valid 0.0067528235633187335\n",
      "Epoch 870:\n",
      "Loss train 0.0067029708530753854 valid 0.0067556624152903645\n",
      "Epoch 871:\n",
      "Loss train 0.006706019630655646 valid 0.00675861837788277\n",
      "Epoch 872:\n",
      "Loss train 0.006709853280335664 valid 0.006750490728280986\n",
      "Epoch 873:\n",
      "Loss train 0.006708051962777972 valid 0.00678755668795327\n",
      "Epoch 874:\n",
      "Loss train 0.006712439795956015 valid 0.006750084328482299\n",
      "Epoch 875:\n",
      "Loss train 0.006709435721859336 valid 0.006768474025529907\n",
      "Epoch 876:\n",
      "Loss train 0.0067087011877447365 valid 0.006757194096915082\n",
      "Epoch 877:\n",
      "Loss train 0.006707692798227072 valid 0.006749646613175926\n",
      "Epoch 878:\n",
      "Loss train 0.006700256187468767 valid 0.006756290986856903\n",
      "Epoch 879:\n",
      "Loss train 0.006705005327239633 valid 0.006769415392672115\n",
      "Epoch 880:\n",
      "Loss train 0.006720854016020894 valid 0.006802094064029802\n",
      "Epoch 881:\n",
      "Loss train 0.006729056686162948 valid 0.006753862121104037\n",
      "Epoch 882:\n",
      "Loss train 0.006721890391781926 valid 0.006769203680046379\n",
      "Epoch 883:\n",
      "Loss train 0.006709614163264632 valid 0.006769634642888084\n",
      "Epoch 884:\n",
      "Loss train 0.006709496676921845 valid 0.006757277948975044\n",
      "Epoch 885:\n",
      "Loss train 0.0067058475222438576 valid 0.006751671273979101\n",
      "Epoch 886:\n",
      "Loss train 0.006701255217194557 valid 0.0067497068711606534\n",
      "Epoch 887:\n",
      "Loss train 0.006708315620198846 valid 0.006752952796728612\n",
      "Epoch 888:\n",
      "Loss train 0.006703547714278102 valid 0.006757061252032583\n",
      "Epoch 889:\n",
      "Loss train 0.006705174408853054 valid 0.006774152527449793\n",
      "Epoch 890:\n",
      "Loss train 0.006725258287042379 valid 0.006765311600755199\n",
      "Epoch 891:\n",
      "Loss train 0.006708112498745322 valid 0.006752656380302045\n",
      "Epoch 892:\n",
      "Loss train 0.006718751229345799 valid 0.0067757697683272335\n",
      "Epoch 893:\n",
      "Loss train 0.006717707822099328 valid 0.006801028845100949\n",
      "Epoch 894:\n",
      "Loss train 0.00673378687351942 valid 0.006764955031433467\n",
      "Epoch 895:\n",
      "Loss train 0.006708108307793737 valid 0.006761211968454819\n",
      "Epoch 896:\n",
      "Loss train 0.006707921577617526 valid 0.006758255911452577\n",
      "Epoch 897:\n",
      "Loss train 0.006706920312717557 valid 0.00675064415830263\n",
      "Epoch 898:\n",
      "Loss train 0.0067021258175373076 valid 0.0067517281591332355\n",
      "Epoch 899:\n",
      "Loss train 0.006704728212207555 valid 0.006764566997703943\n",
      "Epoch 900:\n",
      "Loss train 0.006706730462610722 valid 0.0067533984528960435\n",
      "Epoch 901:\n",
      "Loss train 0.0067071018274873495 valid 0.006762864238522361\n",
      "Epoch 902:\n",
      "Loss train 0.006713890749961138 valid 0.006776840674650848\n",
      "Epoch 903:\n",
      "Loss train 0.006722723506391049 valid 0.0067539028645992975\n",
      "Epoch 904:\n",
      "Loss train 0.006703505991026759 valid 0.006763036548636977\n",
      "Epoch 905:\n",
      "Loss train 0.006708549009636045 valid 0.0067552957270679784\n",
      "Epoch 906:\n",
      "Loss train 0.006717254081740976 valid 0.0067583514065135955\n",
      "Epoch 907:\n",
      "Loss train 0.0067203807644546035 valid 0.006821570583679504\n",
      "Epoch 908:\n",
      "Loss train 0.0067248891573399305 valid 0.0067657292774484625\n",
      "Epoch 909:\n",
      "Loss train 0.0067079339642077684 valid 0.006756583401895541\n",
      "Epoch 910:\n",
      "Loss train 0.0067051809281110765 valid 0.006757393173877308\n",
      "Epoch 911:\n",
      "Loss train 0.0067076170817017555 valid 0.006752710936493122\n",
      "Epoch 912:\n",
      "Loss train 0.00670243427157402 valid 0.006749961440399067\n",
      "Epoch 913:\n",
      "Loss train 0.006705877836793661 valid 0.006752276023906645\n",
      "Epoch 914:\n",
      "Loss train 0.006709276139736176 valid 0.0067604928566414434\n",
      "Epoch 915:\n",
      "Loss train 0.006710801087319851 valid 0.006750971421665239\n",
      "Epoch 916:\n",
      "Loss train 0.006713527580723166 valid 0.006772702150082891\n",
      "Epoch 917:\n",
      "Loss train 0.006706847436726093 valid 0.0067473657008189965\n",
      "Epoch 918:\n",
      "Loss train 0.006705676252022385 valid 0.006751112126445384\n",
      "Epoch 919:\n",
      "Loss train 0.006701136054471135 valid 0.006755519199671822\n",
      "Epoch 920:\n",
      "Loss train 0.006703524477779865 valid 0.0067521018246713815\n",
      "Epoch 921:\n",
      "Loss train 0.006710403319448233 valid 0.0067557251800478895\n",
      "Epoch 922:\n",
      "Loss train 0.006706882873550058 valid 0.006768324710471249\n",
      "Epoch 923:\n",
      "Loss train 0.00670424485579133 valid 0.006757258947477569\n",
      "Epoch 924:\n",
      "Loss train 0.00670607821084559 valid 0.006760464186561296\n",
      "Epoch 925:\n",
      "Loss train 0.006714131869375706 valid 0.006770918574752786\n",
      "Epoch 926:\n",
      "Loss train 0.006714673293754458 valid 0.006775798154227423\n",
      "Epoch 927:\n",
      "Loss train 0.006711467262357473 valid 0.006754406015338745\n",
      "Epoch 928:\n",
      "Loss train 0.0067286522593349215 valid 0.00678338867534179\n",
      "Epoch 929:\n",
      "Loss train 0.006710647093132138 valid 0.006749607945847757\n",
      "Epoch 930:\n",
      "Loss train 0.0067023338284343485 valid 0.006765399081981891\n",
      "Epoch 931:\n",
      "Loss train 0.006713746720924974 valid 0.006758525091329169\n",
      "Epoch 932:\n",
      "Loss train 0.006735940324142575 valid 0.0068186627816041985\n",
      "Epoch 933:\n",
      "Loss train 0.006719238357618451 valid 0.006758990854805439\n",
      "Epoch 934:\n",
      "Loss train 0.006708360742777586 valid 0.0067556504861310835\n",
      "Epoch 935:\n",
      "Loss train 0.006720171635970473 valid 0.006750130218069273\n",
      "Epoch 936:\n",
      "Loss train 0.006712085288017988 valid 0.006752788023597753\n",
      "Epoch 937:\n",
      "Loss train 0.006702615320682526 valid 0.0067490895913989496\n",
      "Epoch 938:\n",
      "Loss train 0.006702637718990445 valid 0.006752454820705509\n",
      "Epoch 939:\n",
      "Loss train 0.006706822849810124 valid 0.006751376308187201\n",
      "Epoch 940:\n",
      "Loss train 0.006716028274968267 valid 0.0067910600485954166\n",
      "Epoch 941:\n",
      "Loss train 0.006709521124139428 valid 0.006753716057004861\n",
      "Epoch 942:\n",
      "Loss train 0.006704553030431271 valid 0.006752164852127871\n",
      "Epoch 943:\n",
      "Loss train 0.006701407767832279 valid 0.006753950327239372\n",
      "Epoch 944:\n",
      "Loss train 0.006702038086950779 valid 0.006751097123443909\n",
      "Epoch 945:\n",
      "Loss train 0.006702318694442511 valid 0.006755853219344672\n",
      "Epoch 946:\n",
      "Loss train 0.006704807281494141 valid 0.006749557520559099\n",
      "Epoch 947:\n",
      "Loss train 0.0067118471022695305 valid 0.006760387622640971\n",
      "Epoch 948:\n",
      "Loss train 0.006708665983751416 valid 0.006753741550208465\n",
      "Epoch 949:\n",
      "Loss train 0.00670340727083385 valid 0.006754593016859517\n",
      "Epoch 950:\n",
      "Loss train 0.0067031379789113995 valid 0.006748887968844499\n",
      "Epoch 951:\n",
      "Loss train 0.006705245980992913 valid 0.006749722233466261\n",
      "Epoch 952:\n",
      "Loss train 0.006703534862026572 valid 0.006753710569394179\n",
      "Epoch 953:\n",
      "Loss train 0.006707038637250662 valid 0.0067820132282480775\n",
      "Epoch 954:\n",
      "Loss train 0.00673397840000689 valid 0.006768550279164083\n",
      "Epoch 955:\n",
      "Loss train 0.006718977866694331 valid 0.006773695484683739\n",
      "Epoch 956:\n",
      "Loss train 0.006707851774990559 valid 0.006761514354049102\n",
      "Epoch 957:\n",
      "Loss train 0.006718788435682655 valid 0.006749514745438378\n",
      "Epoch 958:\n",
      "Loss train 0.006709528574720025 valid 0.006763467689188607\n",
      "Epoch 959:\n",
      "Loss train 0.006710708001628518 valid 0.006767430151539648\n",
      "Epoch 960:\n",
      "Loss train 0.006712520914152265 valid 0.00674927342016312\n",
      "Epoch 961:\n",
      "Loss train 0.006699001183733344 valid 0.006749033397067421\n",
      "Epoch 962:\n",
      "Loss train 0.0066994654946029185 valid 0.00674951331629489\n",
      "Epoch 963:\n",
      "Loss train 0.0066997077316045765 valid 0.006750036981517083\n",
      "Epoch 964:\n",
      "Loss train 0.006701404880732298 valid 0.006754739930216022\n",
      "Epoch 965:\n",
      "Loss train 0.0067049288656562565 valid 0.006755537286421642\n",
      "Epoch 966:\n",
      "Loss train 0.006703065149486065 valid 0.006750132106558332\n",
      "Epoch 967:\n",
      "Loss train 0.006706540286540985 valid 0.006749494485753693\n",
      "Epoch 968:\n",
      "Loss train 0.006707840831950307 valid 0.0067602317634178\n",
      "Epoch 969:\n",
      "Loss train 0.006701852940022945 valid 0.006753302385308917\n",
      "Epoch 970:\n",
      "Loss train 0.0067107378970831634 valid 0.0067674557553751855\n",
      "Epoch 971:\n",
      "Loss train 0.006712875049561262 valid 0.006748916786196612\n",
      "Epoch 972:\n",
      "Loss train 0.006706087663769722 valid 0.0067748465150863175\n",
      "Epoch 973:\n",
      "Loss train 0.0067326132208108905 valid 0.0068262754360716\n",
      "Epoch 974:\n",
      "Loss train 0.006721619144082069 valid 0.006747731333095137\n",
      "Epoch 975:\n",
      "Loss train 0.006699786055833101 valid 0.006750839040872473\n",
      "Epoch 976:\n",
      "Loss train 0.00671040234155953 valid 0.006759491664825496\n",
      "Epoch 977:\n",
      "Loss train 0.006722230883315206 valid 0.006764905686981893\n",
      "Epoch 978:\n",
      "Loss train 0.006724619166925549 valid 0.00674971575208326\n",
      "Epoch 979:\n",
      "Loss train 0.0067046754993498325 valid 0.0067591680774246946\n",
      "Epoch 980:\n",
      "Loss train 0.006710172770544887 valid 0.006751252024438184\n",
      "Epoch 981:\n",
      "Loss train 0.006712132785469294 valid 0.006777156328442716\n",
      "Epoch 982:\n",
      "Loss train 0.006705567333847284 valid 0.006749805521101184\n",
      "Epoch 983:\n",
      "Loss train 0.006700625922530889 valid 0.006755039793175123\n",
      "Epoch 984:\n",
      "Loss train 0.006702080415561795 valid 0.006759320651328462\n",
      "Epoch 985:\n",
      "Loss train 0.006707893637940288 valid 0.006778117013527135\n",
      "Epoch 986:\n",
      "Loss train 0.006712568597868085 valid 0.00674821675437204\n",
      "Epoch 987:\n",
      "Loss train 0.006710403226315975 valid 0.006787665004032136\n",
      "Epoch 988:\n",
      "Loss train 0.006729768589138985 valid 0.006761718871233922\n",
      "Epoch 989:\n",
      "Loss train 0.006724903080612421 valid 0.006749804541437449\n",
      "Epoch 990:\n",
      "Loss train 0.0067039575893431905 valid 0.006761128397734841\n",
      "Epoch 991:\n",
      "Loss train 0.006705536879599095 valid 0.006761323025307974\n",
      "Epoch 992:\n",
      "Loss train 0.006706858891993761 valid 0.006756524175154619\n",
      "Epoch 993:\n",
      "Loss train 0.006708479905501008 valid 0.0067607798953187625\n",
      "Epoch 994:\n",
      "Loss train 0.006703469716012478 valid 0.006750378209912381\n",
      "Epoch 995:\n",
      "Loss train 0.00670347367413342 valid 0.006779851695891551\n",
      "Epoch 996:\n",
      "Loss train 0.006714793015271425 valid 0.006747461524931393\n",
      "Epoch 997:\n",
      "Loss train 0.006702965451404452 valid 0.00675352304908133\n",
      "Epoch 998:\n",
      "Loss train 0.006705151498317718 valid 0.0067484700541149654\n",
      "Epoch 999:\n",
      "Loss train 0.0067074066959321495 valid 0.006772937478019692\n",
      "Epoch 1000:\n",
      "Loss train 0.0067085892427712675 valid 0.006745871722707204\n"
     ]
    }
   ],
   "source": [
    "batch_size = 500_000\n",
    "learning_rate = 1e-5\n",
    "n_epochs = 1000\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {}:'.format(epoch + 1))\n",
    "\n",
    "    model.train()\n",
    "    avg_loss = train_one_epoch()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_outputs = torch.squeeze(model(torch.from_numpy(valid_features_normalized.astype(\"float32\"))))\n",
    "        valid_loss = loss_function(valid_outputs, torch.from_numpy(valid_fco2).to(torch.device(\"cuda\"))).detach().cpu().item()\n",
    "\n",
    "    print('Loss train {} valid {}'.format(avg_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77338c01-b87a-4913-81ba-44d9ea2ebe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (for consistency check):  0.0067458717227072045\n",
      "RMSE:  0.08213325588765616\n",
      "Maximum absolute deviation:  3.452851286877376\n",
      "99.9th percentile of absolute deviation (1000 val's larger):  0.4504538492971377\n",
      "99.99th percentile of absolute deviation (100 val's larger):  0.9481441796927209\n",
      "99.999th percentile of absolute deviation (10 val's larger):  1.7622041263873702\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE (for consistency check): \", MSE(valid_fco2, model_output_after_training))\n",
    "print(\"RMSE: \", np.sqrt(MSE(valid_fco2, model_output_after_training)))\n",
    "print(\"Maximum absolute deviation: \", np.max(np.abs(model_output_after_training-valid_fco2)))\n",
    "print(\"99.9th percentile of absolute deviation (1000 val's larger): \", np.percentile(np.abs(model_output_after_training-valid_fco2), q=99.9))\n",
    "print(\"99.99th percentile of absolute deviation (100 val's larger): \", np.percentile(np.abs(model_output_after_training-valid_fco2), q=99.99))\n",
    "print(\"99.999th percentile of absolute deviation (10 val's larger): \", np.percentile(np.abs(model_output_after_training-valid_fco2), q=99.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d52e1f8-ae5e-4ee2-8f09-f5239a4c5edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some tests of model performance\n",
      "Index:  2486469\n",
      "42.382211071672096\n",
      "tensor([42.3677], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  889830\n",
      "1006.2861357301771\n",
      "tensor([1006.1254], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  1523244\n",
      "503.3432186052197\n",
      "tensor([503.4655], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  4919512\n",
      "784.3194463700629\n",
      "tensor([784.3297], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  772012\n",
      "321.1514859455472\n",
      "tensor([321.2200], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  42812\n",
      "1363.8978478486451\n",
      "tensor([1363.9291], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  4554809\n",
      "349.59341067823567\n",
      "tensor([349.5726], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  4297866\n",
      "1577.4074128192042\n",
      "tensor([1577.2814], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  1591483\n",
      "45.524201800380936\n",
      "tensor([45.5781], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n",
      "Index:  3986147\n",
      "908.4829140524911\n",
      "tensor([908.4073], device='cuda:0', grad_fn=<EluBackward0>)\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "print(\"Some tests of model performance\")\n",
    "random_indices = np.random.randint(ntrain, size=10, dtype=int)\n",
    "for ind in random_indices:\n",
    "    print(\"Index: \", ind)\n",
    "    print(train_fco2[ind])\n",
    "    print(model(torch.from_numpy(train_features_normalized[ind, :].astype(\"float32\"))))\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83c7e13a-a436-48b5-9ed2-0c1698c4f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"64x3_elu_1000epo_lr3_bs1000_1000epo_lr4_bs5000_1000epo_lr4_bs50000_1000epo_lr5_bs500000.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00d7f317-0078-44f2-806a-80d1b09f3a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyse where model is still wrong:\n",
      "Minimum pCO2 in valid data:  7.831486289595464\n",
      "Maximum pCO2 in valid data:  4158.790981271339\n",
      "Number of values between 100 and 1000 muatm:  640217\n",
      "Maximum absolute deviation in this range:  1.6519389985968473\n"
     ]
    }
   ],
   "source": [
    "print(\"Analyse where model is still wrong:\")\n",
    "print(\"Minimum pCO2 in valid data: \", np.min(valid_fco2))\n",
    "print(\"Maximum pCO2 in valid data: \", np.max(valid_fco2))\n",
    "range_mask = (valid_fco2 < 1000) * (valid_fco2 > 100)\n",
    "print(\"Number of values between 100 and 1000 muatm: \", np.sum(range_mask))\n",
    "print(\"Maximum absolute deviation in this range: \", np.max(np.abs(model_output_after_training-valid_fco2) * range_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742d4a99-ecc4-4c22-9f8f-6cac127e8e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
